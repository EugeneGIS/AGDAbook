[["index.html", "Advanced Geospatial Data Analysis in R: Environmental Applications Preface Author Information Prerequisites Acknowledgements", " Advanced Geospatial Data Analysis in R: Environmental Applications Marj Tonini, Haokun Liu 2024-04-22 Preface Earth surface environmental processes exhibit distinctive characteristics, encompassing both spatial and temporal dimensions, along with various attributes and predictive variables. Furthermore, in the era of Data Science, the wealth of available data and the rapid development of analytical models have emerged as distinctive aspects in the realm of Geospatial Data Analysis (GDA). Coupled with uncertainty and complexity issues, all this contribute to making this field of research highly challenging. The domain of GDA encompasses data exploration, manipulation, and modelling, from the acquisition phase up to the visualization and interpretation of the results. Mapping where events are located and how they relate to each other provides a better understanding of the process being studied. Finally, as an increasing volume of geo-environmental data becomes more and more accessible, the demand for experts in this domain is growing rapidly, both in public research institutions as well as in private companies. Defined for the first time by Naur as “The science of dealing with data”, the term Data Science evolved over time around the original concept of “…converting data into information and knowledge” (IASC, 1977). In the disciplines like Environmental and Earth Sciences, Physical Geography, Humanities and Social Sciences, the use of Data Science procedures is emerging only recently, proving to be extremely efficient to deal with the complexity of the investigated process and the heterogeneity of the underlying data sources. This leads to a cultural shift, moving scientists away from individual working within their own research domain. Indeed, disciplinary boundaries are more and more permeable, pushing scientists to be more open to collaborate among them and with decision makers on the investigation and understanding of real-world problems. Modern earth and environmental scientists need to interact with other disciplines, apparently far from their domain. This openness is increasingly important as society struggles to respond to the implication of anthropogenic pressures on different issues, such as natural hazards and climate change, or the harmful impacts of human activities on biodiversity, water and air quality, human health. The target audience of this eBook are master and graduate (PhD) students in Earth/Environmental Sciences, Biology and Spatial Ecology, Physical Geography, and equivalent disciplines. We strive to empower students by guiding them through both theoretical knowledge and hands-on practical applications, enabling them to cultivate effective problem-solving skills. The primary focus here delves into applying Data Science methodologies to understand and analyze Earth’s surface environmental processes. The scientific approaches related to this emerging discipline, ranging from statistics, mathematics, geomatics and computer science, are often hard to be acquired. While maintaining a strong emphasis on rigorous mathematical and statistical formalism in the methods presented, this eBook primarily accentuates their practical applications within the realm of Geosciences The book is addressed to intermediate to advanced R users with some experience on geospatial data and a great interest in geocomputing. If you have some very basic knowledge in these fields, we encourage you to explore links provided in each chapter, which redirect you to further useful documentation. This eBook seeks to provide the audience with: A good understanding of main practical concepts and applied aspects in GDA; Advanced tools designed to proficiently navigate various techniques for analysing spatial datasets in geosciences. Specifically, the methodologies outlined will equip readers with expertise in various algorithms tailored for analyzing complex geo-environmental datasets. This knowledge will empower them to extract valuable insights and translate them into actionable decisions. Explored algorithms include: Geographically Weighted Summary Statistics for exploratory data analyses and visualization of geographical variations in the statistical data distribution; Ripley’s K-function, Kernel Density Estimator, DBSCAN for cluster detection and mapping; Self-Organizing Maps as an example of unsupervised machine learning approach to data clustering and visualization; Random Forest as an example of a supervised machine learning algorithm, applied here for classification. Author Information Marj Tonini is a spatial data scientist with a strong interest on geospatial modelling for risk assessment, mainly related to wildfires and landslides. She gained her PhD in 2002 at Sant’Anna School of advanced studies (Pisa, Italy), defending a thesis on agro-environmental modelling. She started working at the University of Lausanne in 2004, as post-doc and, in 2008, she was appointed senior research manager at the Institute of Earth Surface Dynamics (current position). Her research focuses on the development of innovative approaches allowing to enable efficient learning from complex environmental datasets. She seeks to set up a methodological framework to understand the spatio-temporal dynamic of environmental processes and to assess the influence of predictor variables. Her current research targets are on land use/land cover change analyses and on the elaboration of predictive scenario and susceptibility / risk assessment for natural hazard. Haokun Liu is a Ph.D. student at the Group of Cities and dynamics of networks (CITADYNE Group), University of Lausanne. Meanwhile, he is also a student assistant at Swiss Geocomputing Center (SGC). Benefiting from the strict and effective training in China and Switzerland, his research interests and experience include, but are not limited to: 1) Urban analytics, 2) Health geography, 3) Spatial data science. With the goal of bridging the gap between theories and practical applications, he am focusing on the intersections among urban health, complex perspective, activity space, and the Spatial-social-semantic framework. Prerequisites Knowledge of basic statistics: methods of descriptive statistics (measures of central tendency and dispersion); how to assess relationships between variables; concepts of correlation and regression. Basic knowledge in geomatics (GIS): basic operations with raster and vector datasets. R programming basics and RStudio. Acknowledgements The case study presented in each chapter came from different projects carried out in collaboration with several colleagues, including master and PhD students. All the produced scientific papers are duly cited in the bibliography. We would like to thank the following collaborators: Mário Gonzalez Pereira and Joana Parente, for the extensive and fruitful collaboration we have had in investigating the spatio-temporal distribution of wildfires in Portugal; one of our studies seeking to investigate the evolution of forest fires, from spatio-temporal point events to smoothed density maps, is an integral part of Chapters 2 and 3. Stuart Lane and Natan Micheletti, for introducing me to the fascinating world of rock glacier research; notably, the 3D-points clouds dataset analysed in Chapter 4 was acquired and processed by Natan during his PhD studies. Axelle Bersier, for the meticulous work she did in acquiring and pre-processing the Swiss national population census dataset, which have been used for the exercise about unsupervised learning in Chapter 5. Julien Riese, for producing the input dataset and collaborating with me in developing the code allowing to assess the landslides susceptibility in canton Vaud, that is the main topic of Chapter 6. Both Julien and Axelle serve as a shining example of a highly successful master’s students whom I had the pleasure of supervising. "],["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 R Language 1.2 R Markdown 1.3 Data type in computational analysis", " Chapter 1 Introduction to R 1.1 R Language R is a complete programming language and software environment for statistical computing and graphical representation. As part of the GNU Project (free software, mass collaboration project), the source code is free available. Its functionalities can be expanded by importing packages. For more details on R see https://www.r-project.org/. 1.1.1 R Packages A package is a file generally composed of R scripts (e.g., functions). On all operation systems the function “install.packages()” can be used to download and install a package automatically. Otherwise, a package already installed in R can be loaded in a session by using the command . In R, the directories where the packages are stored are called “libraries”. The terms “package” and “library” are sometimes used synonymously. For example, to check the list of the installed packeges, the function can be used. When you open an R Markdown document (.Rmd) the program propose you automatically to install the libraries listed there. 1.1.2 Some tips R is case sensitive! Previously used command can be recalled in the console by using the up arrow on the keyboard. The working directory by default is “C:/user/…/Documents”. It can be found using the command It can be changed using the command line In R Markdown: the working directory when evaluating R code chunks is the directory of the input document by default. To access to a specific file in a sub-folder use “. /subfolder/file.ext” To access to a specific file in a up-folder use “. . /upfolder/file.ext” 1.1.3 R Commands (online resources) Many table resuming the main R commands can be found online. Here some useful links: A short list of the most useful R commands Table of Useful R commands Basic Commands to Get Started with R 1.2 R Markdown This is an R Markdown document :-) Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is a simple and easy to use plain text language allowing to combine R code, results from data analysis (including plots and tables), and comments into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or web pages). Code lines are organized into code blocks, seeking to solve specified tasks, and referred to as “code chunk”. For more details on using R Markdown see http://rmarkdown.rstudio.com. All what you have to do during the computing labs is to read each explanatory paragraph before running each individual R code chunk, one by one, and to interpret the results. Finally, to create a personal document (usually a PDF) from rmarkdown, you need to Knit the document. Knitting a document simply means taking all the text and code and creating a nicely formatted document. 1.3 Data type in computational analysis 1.3.1 Variables Variables are used to store values in a computer program. Values can be numbers (real and complex), words (string), matrices, and even tables. The fundamental or atomic data in R Programming can be: integer: number without decimals numeric: number with decimals (float or double depending on the precision) character: string, label factors: a label with a limited number of categories logical: true/false Figure 1.1: Data Types in R 1.3.2 Data structure in R R’s base data structures can be organised by their dimensionality (1d, 2d, or nd) and whether they are homogeneous (all contents must be of the same type) or heterogeneous (the contents can be of different types). This gives rise to the four data structures most often used in data analysis: Figure 1.2: Data structures in R A Vector is a one-dimensional structure winch can contain object of one type only: numerical (integer and double), character, and logical. # Investigate vector&#39;s types: v1 &lt;- c(0.5, 0.7); v1; typeof(v1) ## [1] 0.5 0.7 ## [1] &quot;double&quot; v2 &lt;-c(1:10); v2; typeof(v2) ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] &quot;integer&quot; v3 &lt;- c(TRUE, FALSE); v3; typeof(v3) ## [1] TRUE FALSE ## [1] &quot;logical&quot; v4 &lt;- c(&quot;Swiss&quot;, &quot;Itay&quot;, &quot;France&quot;, &quot;Germany&quot;); v4; typeof(v4) ## [1] &quot;Swiss&quot; &quot;Itay&quot; &quot;France&quot; &quot;Germany&quot; ## [1] &quot;character&quot; #Create a sequence from 0 to 5 with a step of 0.5: v5 &lt;- seq(1, 5, by=0.5); v5; typeof(v5) ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 ## [1] &quot;double&quot; length(v5) ## [1] 9 summary(v5) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 3 4 5 #Extract the third element of the vector v5[3] ## [1] 2 #Exclude the third element from the vector and save as new vector v5[-3] ## [1] 1.0 1.5 2.5 3.0 3.5 4.0 4.5 5.0 w5&lt;-v5[-3]; w5 ## [1] 1.0 1.5 2.5 3.0 3.5 4.0 4.5 5.0 A Matrix is a two-dimensional structure winch can contain object of one type only. The function can be used to construct matrices with specific dimensions. # Matrix of elements equal to &quot;zero&quot; and dimension 2x5 m1&lt;-matrix(0,2,5); m1 #(two rows by five columns) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 # Matrix of integer elements (1 to 12, 3x4) m2&lt;-matrix(1:12, 3,4); m2 ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 # Extract the second row m2[2, ] ## [1] 2 5 8 11 # Extract the third column m2[,3] ## [1] 7 8 9 # Extract the the second element of the third column m2[2,3] ## [1] 8 1.3.3 Data Frame A data frame allows to collect data of different type. All elements must have the same length. A list is a more flexible structure since it can contain variables of different types and lengths. Nevertheless, the preferred structure for statistical analyses and computation is the data frame. It is a good practice to explore the data frame before performing further computation on the data. This can be simply accomplished by using the commands to explore the structure of the data and to display the summary statistics and quickly summarize the data. For numerical vectors the command can be used to plot the basic histogram of the given values. # Create the vectors with the variables cities &lt;- c(&quot;Berlin&quot;, &quot;New York&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;) area &lt;- c(892, 1214, 105, 2188) population &lt;- c(3.4, 8.1, 2.1, 12.9) continent &lt;- c(&quot;Europe&quot;, &quot;Norh America&quot;, &quot;Europe&quot;, &quot;Asia&quot;) # Concatenate the vectors into a new data frame df1 &lt;- data.frame(cities, area, population, continent) df1 ## cities area population continent ## 1 Berlin 892 3.4 Europe ## 2 New York 1214 8.1 Norh America ## 3 Paris 105 2.1 Europe ## 4 Tokyo 2188 12.9 Asia #Add a column (e.g., language spoken) using the command &quot;cbind&quot; df2 &lt;- cbind (df1, &quot;Language&quot; = c (&quot;German&quot;, &quot;English&quot;, &quot;Freanch&quot;, &quot;Japanese&quot;)) df2 ## cities area population continent Language ## 1 Berlin 892 3.4 Europe German ## 2 New York 1214 8.1 Norh America English ## 3 Paris 105 2.1 Europe Freanch ## 4 Tokyo 2188 12.9 Asia Japanese #Explore the data frame str(df2) # see the structure ## &#39;data.frame&#39;: 4 obs. of 5 variables: ## $ cities : chr &quot;Berlin&quot; &quot;New York&quot; &quot;Paris&quot; &quot;Tokyo&quot; ## $ area : num 892 1214 105 2188 ## $ population: num 3.4 8.1 2.1 12.9 ## $ continent : chr &quot;Europe&quot; &quot;Norh America&quot; &quot;Europe&quot; &quot;Asia&quot; ## $ Language : chr &quot;German&quot; &quot;English&quot; &quot;Freanch&quot; &quot;Japanese&quot; summary(df2) # compute basic statistics ## cities area population continent ## Length:4 Min. : 105.0 Min. : 2.100 Length:4 ## Class :character 1st Qu.: 695.2 1st Qu.: 3.075 Class :character ## Mode :character Median :1053.0 Median : 5.750 Mode :character ## Mean :1099.8 Mean : 6.625 ## 3rd Qu.:1457.5 3rd Qu.: 9.300 ## Max. :2188.0 Max. :12.900 ## Language ## Length:4 ## Class :character ## Mode :character ## ## ## # Use the symbol &quot;$&quot; to address a particular column pop&lt;-(df2$population) pop ## [1] 3.4 8.1 2.1 12.9 hist(pop) # plot the histogram "],["geographically-weighted-summary-statistics.html", "Chapter 2 Geographically Weighted Summary Statistics 2.1 Introduction 2.2 The overall methodology 2.3 Forest fires dataset 2.4 Compute the geographically whited statistics 2.5 Conclusions and further analyses", " Chapter 2 Geographically Weighted Summary Statistics 2.1 Introduction In fire management, it is crucial to investigate where fires occurred more frequently and to distinguish between small and large fires. This is key information to understand the ignition factors and planning strategies to reduce forest fires, control and manage ignition sources, and identify areas at risk. Despite the availability of forest fires spatio-temporal inventories, it is not evident to extract information about their pattern distribution simply by looking at the original arrangement of the mapped burnt areas. To this end, Geographically Weighed Summary Statistics (GWSS) can be computed, under the assumption that burned areas generally follow a geographic trend. We compute here the GW local means, the GW local standard deviation and the GW localized skewness of burned areas in continental Portugal, registered in the period 1990-2013. This application is inspired by the work of1 2.2 The overall methodology Summary statistics include a number of measures that can be used to summarize a set of observations, the most important of which are measures of central tendency (arithmetic mean, median and mode) and measures of dispersion around the mean (variance and standard deviation). In addition, measures of skewness and kurtosis are descriptors of the shape of the probability distribution function, the former indicating the asymmetry and the latter the peakedness/tailedness of the curve. In the case of spatial data, these global statistical descriptors may vary from one region to another, as their values may be affected by local environmental and socio-economic factors. In this case, an appropriately localized calibration can provide a better description of the observed values. One way to achieve this goal is to weight the above statistical measures for a given quantitative variable based on their geographical location. We introduce here the method proposed by2 and implemented in the function GWSS presented in the .3 The evaluation of geographically weighted summary statistics is obtained by computing a summary for a small area around each geolocalized punctual observation, by using the kernel density estimation technique (KDE).4 KDE is estimated at each point, taking into account the influence of the points falling within an area, with increasing weight towards the center, corresponding to the point location. A surface summary statistic is thus obtained. 2.3 Forest fires dataset Forest fires inventories indicating the location, the starting date and other related variables, such as the cause of ignition and the size of the area burned, are broadly available with a different degree of accuracy in different countries. In the present study, we consider the Portuguese National Mapping Burnt Areas (NMBA 2016), freely available from the website of the Institute for the Conservation of Nature and Forests (ICNF). This is a long spatio-temporal dataset (from 1975) resulting from the processing of satellite images acquired once a year at the end of the summer season. Row data consist of records of observed fire scars. The burned areas were estimated by using image classification techniques, then compared with ground data to resolve the discrepancies. Polygons have been converted into point shapefile, where each point represent the centroid of the burned areas, while the size of the burned areas and the starting date of the fires events are given as attributes. In this work, for consistency reasons, we consider only fires occurred between 1990 and 2013 and with a burned area above 5 hectares. . Figure 2.1: Total annual number of forest fire events, expressed in thousands of square metres 2.3.1 Load the libraries First you have to load the following libraries: splancs: for display and analysis of spatial point pattern data GWmodel: techniques from a particular branch of spatial statistics, termed geographically-weighted (GW) models sf: support for simple features, a standardized way to encode spatial vector data ggplot2: a system for ‘declaratively’ creating graphics sp: classes and methods for spatial data library(splancs) library(GWmodel) library(sf) library(ggplot2) library(sp) (.packages()) ## [1] &quot;ggplot2&quot; &quot;sf&quot; &quot;GWmodel&quot; &quot;Rcpp&quot; &quot;robustbase&quot; ## [6] &quot;splancs&quot; &quot;sp&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [11] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 2.3.2 Import the Portuguese forest fire dataset In this section you will load the geodata representing the dataset of forest fires occurred in the continental Portuguese area in the period 1990-2013. You will also load the boundaries of the study area. You will start by exploring the datasets using mainly visual tools (plotting and histogram). # Import Portugal boundary Portugal &lt;- st_read(&quot;data/Lab01/Area_Portugal.shp&quot;) # Import the Portuguese forest fires dataset for the entire Portuguese area. FFPorto&lt;-st_read(&quot;data/Lab01/FF_Portugal.shp&quot;) You can explore the dataset by using different tools for exploratory data analyses. You will start by visualizing the databases. In the GIS environment, this correspond to the attribute table of a vector punctual feature. Than you can plot the histogram of events distribution based on the variable “Area_ha” (the size in hectares of the burned area). Since this is a power low distribution, for a better understanding it’s recommended to transform the data using a logarithmic scale. Using Log10 you can easily evaluate the frequency distribution of the burned areas. # Show the attribute table (first 10 rows) FFPorto # Open the attribute table in a new tab View(FFPorto) # Summary statistics of all of the attributes associated with this dataset summary(FFPorto$Area_ha) # Display the histogram of burned area distribution hist(FFPorto$Area_ha) hist(log10(FFPorto$Area_ha)) 2.3.3 Forest fires spatial distribution For a better understanding of the phenomenon, you can group the events according to the size of the burned area. Based on the frequency distribution of the burned areas, the following three classes can be defined: Small fires: less than 15 ha Medium fires: between 15 ha and 100 ha Large fires: bigger than 100 ha Plotting the forest fires events using different colors, based on the size of the burned areas, can simplify the understanding of their pattern distribution, knowing that fires of different size have normally different drivers. 2.4 Compute the geographically whited statistics From the exploratory data analysis performed above, it seems that a simple plotting of the forest fires events based on their spatial distribution, even if classified based on their size, can not really help to understand their behaviors. This is because we face to a huge number of events and the variable that we are using to characterize them (i.e., the size of the burned area) is very heterogeneous. To this aim, we can compute basic and robust GWSS and plot the data accordingly. GWSS includes geographically weighted means, standard deviations and the skweness. As you can see from the R Documentation (command: , same data manipulations are necessary to transform the forest fires dataset in a compatible data frame format. GWSS parameters: We summarize the data based on the size of the burned area (vars). We use here an adaptive kernel where the bandwidth (bw) corresponds to the number (100 in this case) of nearest neighbors (i.e. adaptive distance). We keep the default values for the other parameters. # Transform inputs data into a spatial points data frame FFdf&lt;-data.frame(X=FFPorto$X, Y=FFPorto$Y, Area=FFPorto$Area_ha) FFspdf&lt;-SpatialPointsDataFrame(FFdf[,1:2], FFdf) str(FFspdf) # Run gwss: this operation can take several minutes...be patient! # While waiting, you can look at gwss R Documentation help(gwss) FFgwss &lt;- gwss(FFspdf,vars=(&quot;Area&quot;),adaptive=TRUE, bw=100) ## RUN IT ONLY IF YOU FAIL TO RUN THE CODE ABOVE ## If it&#39;s too slow, you can stop the job ## and load the result directly #load (&quot;FFgwss&quot;) 2.4.1 Look at the results The resulting object (FFgwss) has a number of components. The most important one is the spatial data frame containing the results of local summary statistics for each data point location, stored in FFgwss$SDF (that is a spatial DataFrame). # display the first 6 rows head(FFgwss$SDF) # Inspect the resulting object: FFgwss 2.4.2 GWSS maps To produce a map of the local geographically weighted summary statistic of your choice, firstly we need to enter a small R function definition. This is just a short R program to draw a map: you can think of it as a command that tells R how to draw a map (see Geographically Weighted Summary Statistics (https://rpubs.com/chrisbrunsdon/99667) for more details). The advantage of defining a function is that the entire map can now be drawn using a single command for each variable, rather than having to repeat those steps each time. To define the intervals for the classification we used Jenks natural breaks classification method (textcolor{red}{style=“fisher”}). Finally the function is called by entering: quick.map(gwss.object,variable.name,legend.title,main.map.title) library(RColorBrewer) #a useful tool for designing map color palettes. library(classInt) #to define class intervals # The function definition to draw the map: quick.map &lt;- function(spdf,var,legend.title,main.title) { x &lt;- spdf@data[,var] int &lt;- classIntervals(x, n=5, style=&quot;fisher&quot;) cut.vals&lt;-int$brks x.cut &lt;- cut(x,cut.vals) cut.levels &lt;- levels(x.cut) cut.band &lt;- match(x.cut,cut.levels) colors &lt;- rev(brewer.pal(length(cut.levels), &quot;RdYlGn&quot;)) par(mar=c(1,1,1,1)) plot(Portugal$geometry,col=&#39;lightgrey&#39;) title(main.title) plot(spdf,add=TRUE,col=colors[cut.band],pch=16, cex=0.5) legend(&quot;bottomright&quot;,cut.levels,col=colors,pch=16,bty=&quot;n&quot;,title=legend.title) } # Call the function to display the maps of the Local Mean (LM), Local Standard Deviation(LSD), and Local Skweness (LSKe) par(mfrow=c(1,3)) quick.map(FFgwss$SDF, &quot;Area_LM&quot;, &quot;Area (ha)&quot;, &quot;GWL Means&quot;) ## Warning in classIntervals(x, n = 5, style = &quot;fisher&quot;): N is large, and some ## styles will run very slowly; sampling imposed quick.map(FFgwss$SDF, &quot;Area_LSD&quot;, &quot;Area (ha)&quot;, &quot;GWL Standard Deviation&quot;) ## Warning in classIntervals(x, n = 5, style = &quot;fisher&quot;): N is large, and some ## styles will run very slowly; sampling imposed quick.map(FFgwss$SDF, &quot;Area_LSKe&quot;, &quot;Area (ha)&quot;, &quot;GWL Skewness&quot;) ## Warning in classIntervals(x, n = 5, style = &quot;fisher&quot;): N is large, and some ## styles will run very slowly; sampling imposed 2.5 Conclusions and further analyses This practical computer lab allowed you to familiarize with GWSS, by the proposed application about geographically weighted summary statistics. This method allowed us to explore how the average burned area vary locally through Continental Portugal in the period 1990-2013. The global Geographically Weighted (GW) means informs you about the local average value of the burned area, based of the neighboring events occurred in a given period. Similarly, you may compute a GW standard deviation to see the extent to which the size of the burned area spread around this mean. Finally you can compute the GW skewness to measure the symmetry of distribution: a positively skewed distribution means that there is a higher number of data points having low values, with mean value lower that the median; and the contrary for a negatively skewed distribution. To be sure that everything is perfectly clear for you, we propose you to answer the following questions and to discuss your answers with the other participants to the course or directly with the teacher. What is the pattern distribution of the GW-means for burned area in Portugal during the investigated periods? Does the GW-standard deviation follows the same pattern? How can you interpret the two pattern in terms of burned area and their characterization? GW-skewness has positive values everywhere: what does it means? What do these values suggest to be the distribution of the burned areas, in terms of their size, around the local means? Which can be other applications of GWSS for geo-environmental data? In other words, can you imagine other geo-environmental dataset that can be analysed using GWSS? You can finally play with the code and try to run it using a different numbers of nearest neighbors (bw=x) and comparing the results. NB: You have to rename the original pdf to avoid overwriting it. In addition, if a pdf with the same name saved in the same destination folder is open, you will receive an error message, so close it before Knitting. References "],["kernel-density-estimator.html", "Chapter 3 Kernel Density Estimator 3.1 Introduction 3.2 Method 3.3 Load the libraries 3.4 Import the Portuguese forest fire dataset 3.5 The space-time K-function 3.6 Conclusions and further analyses", " Chapter 3 Kernel Density Estimator 3.1 Introduction The configuration of forest fires across space and time presents a complex pattern which significantly affects the forest environment and adjacent human developments. Statistical techniques designed for spatio-temporal random points can be utilized to identify a structure, recognize hot-spots and vulnerable areas, and address policy makers to prevention and forecasting measures. In this practical computer lab we consider the same case study as in the “Geographically Weighted Summary Statistics” lab. The main objective is to reveal if space and time act independently or whether neighboring events are also closer in time, interacting to generate spatio-temporal clusters. The attribute that we will consider to achieve this goal is the starting date of fires events. To account for the different geographical distribution of fires in Portugal, evets occurred in the Norther and Southern area will be modeled separately. For more details about the input dataset, please refer to the GWSS lab documentation. 3.2 Method To detect spatio-temporal clusters of forest fires, we will use the following statistical methods: The Ripley’s K- function, to test the space–time interaction and the spatial attraction/in dependency between fires of different size. The kernel density estimator, allowing elaborating smoothed density surfaces representing fires over-densities. We provide below a short description for both these methods. More details can be found in Tonini et al.5 3.2.1 Ripley’s K-function The Ripley’s K-function allows inferring about the spatial randomness of mapped punctual events. It is largely applied in environmental studies to analyse the pattern distribution of a spatial point process. The original spatial univariate K-function \\(K(s)\\) is defined as the ratio between the expected number \\(E\\) of point events falling at a certain distance \\(r\\) from an arbitrary event and the intensity \\(\\lambda\\) of the spatial point process, this last corresponding to the average number of points per unit area. Under complete spatial randomness, which assumes the independence among the events, \\(K(s)\\) is equal to the area of the circle around the target event for each distance’s value. It follows that events are spatially clustered within the range of distances at which \\(K(s)\\) assumes vales higher than this area, while they are spatially dispersed for lower values. The temporal K-function \\(K(t)\\) is defined in the same way as for the spatial case, with the time-based intensity and the time length replacing the spatial parameters. The spatio-temporal K-function, \\(K(s,t)\\) can be considered as a bivariate function where space and time represent the two variables of the equation. It is defined as the number of further events occurring within a distance \\(r\\) and time \\(t\\) from an arbitrary event. 3.2.1.1 Spatio-temporal interaction If there is no space–time interaction, \\(K(s,t)\\) is equal to the product of the purely spatial and purely temporal K-function. Inversely, if space and time interact generating clusters, the difference between these two values is positive [\\(D(s,t)=K(s,t)-K(s)*K(t)\\)]. Thus, we can use the perspective 3D-plot of the function \\(D(s,t)\\) to obtain a first diagnostic of space–time clustering: positive values indicate an interaction between these two variables at a well-detectable spatio-temporal scale. 3.2.2 Kernel density estimator The Kernel Density Estimator (KDE) is a non-parametric descriptor tool widely applied in GIS-science to elaborate smoothed density surfaces from spatial variables. A kernel function \\(K\\) allows weighing up the contribution of each event, based on the relative distance of neighbouring to the target. The parameter \\(h\\), called bandwidth, controls the smoothness of the estimated kernel density. Finally, the kernel density function \\(f_h(x)\\) is estimated by summing all the kernel functions \\(K\\) computed at each point location \\(x\\) and dividing the result by the total number of events (\\(n\\)): \\[f_h(x) = \\frac{1}{nh}\\sum_{i=j}^{n}K(\\frac{x-x_i}{h})\\] The time extension of the kernel density estimator (Tomoki Nakaya and Keiji Yano6) allows to compute the three-dimensional kernel density estimator which includes the spatio-temporal dimensions. In the present case study we apply a quadratic weighting kernel function, which is an approximation to the Gaussian kernel. Regarding the bandwidth’s value, we prose to consider the results of the spatio-temporal K-function as an indicator. Indeed, the distance values showing a maximum cluster behavior over the displayed perspective \\(D(s,t)\\) plot can be attributed to the \\(h\\)-value, minimizing the problem of under- or over-smoothing due to an arbitrary choice of the bandwidth. 3.3 Load the libraries Fist you have to load the following libraries: splancs: for display and analysis of spatial point pattern data. sf: Support for simple features, a standardized way to encode spatial vector data ggplot2: A system for ‘declaratively’ creating graphics sp: Classes and methods for spatial data spatstat: comprehensive open-source toolbox for analyzing Spatial Point Patterns library(splancs) library(spatstat) library(sf) library(ggplot2) library(sp) (.packages()) ## [1] &quot;ggplot2&quot; &quot;sf&quot; &quot;spatstat&quot; &quot;spatstat.linnet&quot; ## [5] &quot;spatstat.model&quot; &quot;rpart&quot; &quot;spatstat.explore&quot; &quot;nlme&quot; ## [9] &quot;spatstat.random&quot; &quot;spatstat.geom&quot; &quot;spatstat.data&quot; &quot;splancs&quot; ## [13] &quot;sp&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [17] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 3.4 Import the Portuguese forest fire dataset In this section you will load the geodata representing the dataset of the Forest Fires (FF) occurred in the continental Portuguese area in the period 1990-2013. You will also load the boundaries of the study area. You will start by exploring the datasets using mainly visual tools (plotting and histogram). # Import Portugal boundary Portugal &lt;- st_read(&quot;data/Lab02/Portugal.shp&quot;) # entire area PortN&lt;- st_read(&quot;data/Lab02/Porto_North.shp&quot;) # northern area PortS&lt;- st_read(&quot;data/Lab02/Porto_South.shp&quot;) # southern # Import the forest fires dataset FF&lt;-st_read(&quot;data/Lab02/ForestFires.shp&quot;) # entire area FFN&lt;-st_read(&quot;data/Lab02/FF_North.shp&quot;) #entire area FFS&lt;-st_read(&quot;data/Lab02/FF_South.shp&quot;) # Northern area # Import the shapefile of the Tagus river river&lt;-st_read(&quot;data/Lab02/Rio_Tajo.shp&quot;) summary(FF$Area_ha)# summary statistics hist(FF$Area_ha) hist(log10(FF$Area_ha)) # see the lab GWSS for more details For a better understanding of the phenomenon, we can group the events according to the size of the burnt area and to their incidence in the northern and in the southern part of continental Portugal. 3.4.1 FF-subsets based on the burned areas’ size Remember that fires of different size can have been induced by different drivers. Thus, in the following, we will investigate all the global cluster behavior of forest fires in Portugal considering the three subset separately. As you have seen in the lab GWSS, based on the frequency distribution of the burned areas, the following three classes can be defined: * Small fires: less than 15 ha * Medium fires: between 15 ha and 100 ha * Large fires: bigger than 100 ha SF=(subset(FF, Area_ha &lt;=15)) #create a sub-set including only small fires. summary(SF) ## Year Area_ha X Y ## Min. :1990 Min. : 5.000 Min. : 82761 Min. : 7772 ## 1st Qu.:1997 1st Qu.: 6.624 1st Qu.:182972 1st Qu.:403849 ## Median :2001 Median : 8.569 Median :213159 Median :472892 ## Mean :2002 Mean : 9.041 Mean :219789 Mean :440692 ## 3rd Qu.:2008 3rd Qu.:11.219 3rd Qu.:259054 3rd Qu.:518384 ## Max. :2013 Max. :15.000 Max. :361492 Max. :573192 ## geometry ## POINT :10902 ## epsg:NA : 0 ## +proj=tmer...: 0 ## ## ## # This is to save the plot pSF &lt;- ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=SF, size=0.5, col=&quot;yellow&quot;)+ ggtitle(&quot;Small fires&quot;) + coord_sf() MF=(subset(FF, Area_ha &gt;15 &amp; Area_ha &lt;=100)) #create a sub-set including only medium fires. summary(MF) ## Year Area_ha X Y ## Min. :1990 Min. :15.00 Min. : 82127 Min. : 7084 ## 1st Qu.:1997 1st Qu.:21.18 1st Qu.:186031 1st Qu.:403864 ## Median :2001 Median :30.89 Median :219459 Median :468503 ## Mean :2002 Mean :38.55 Mean :223915 Mean :439442 ## 3rd Qu.:2007 3rd Qu.:51.06 3rd Qu.:264349 3rd Qu.:518248 ## Max. :2013 Max. :99.98 Max. :359535 Max. :572488 ## geometry ## POINT :12070 ## epsg:NA : 0 ## +proj=tmer...: 0 ## ## ## # This is to save the plot pMF &lt;- ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=MF, size=0.5, col=&quot;orange&quot;)+ ggtitle(&quot;Midium fires&quot;) + coord_sf() LF=(subset(FF, Area_ha &gt;100)) #create a sub-set including only large fires. summary (LF) ## Year Area_ha X Y ## Min. :1990 Min. : 100.0 Min. : 83472 Min. : 8840 ## 1st Qu.:1996 1st Qu.: 139.6 1st Qu.:193831 1st Qu.:383164 ## Median :2002 Median : 220.7 Median :228709 Median :445272 ## Mean :2001 Mean : 548.3 Mean :229278 Mean :423441 ## 3rd Qu.:2006 3rd Qu.: 447.9 3rd Qu.:266146 3rd Qu.:500321 ## Max. :2013 Max. :66070.6 Max. :357933 Max. :571984 ## geometry ## POINT :4301 ## epsg:NA : 0 ## +proj=tmer...: 0 ## ## ## # This is to save the plot pLF &lt;- ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=LF, size=0.5, col=&quot;red&quot;)+ ggtitle(&quot;Large fires&quot;) + coord_sf() # Arrange the three spatial maps side by side install.packages(&#39;patchwork&#39;, repos = &quot;http://cran.us.r-project.org&quot;) ## ## The downloaded binary packages are in ## /var/folders/hf/q_2qq0tn1dngf4r3nzfhj3xh0000gn/T//RtmpzgnkHk/downloaded_packages library(patchwork) # Allow to combine separate ggplots into the same graphic pSF+pMF+pLF 3.4.2 FF-subsets based on their geographical distribution In continental Portugal, the northern half of the country (above the Tagus River) is characterized by the predominance of forest and semi-natural areas, and by the development of the main cities with their sub-urban ares intermingled with wild land, which the Northern area highly prone to forest fires. On the other hand, the southern half of the country is dominated by agricultural areas with mixed and broad-leaved forest concentrated near the south-west coast, which makes this are less affected by forest fires. For this reason we will consider these two areas separately. # Plot the map with all the objects ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=river, col=&quot;blue&quot;, size=2) + geom_sf(data=FFN, size=0.3, col=&quot;red&quot;) + geom_sf(data=FFS, size=0.3, col=&quot;orange&quot;) + ggtitle(&quot;Forest foirest in the northern and souther area&quot;) + theme(plot.title=element_text(hjust=0.5)) + coord_sf() 3.5 The space-time K-function 3.5.1 Extract Time and PTS object The function stkhat, included in the library spancs, allows to compute the space-time K-function. As you can see from the R Documentation (command: help(stkhat)), same data manipulations are necessary to transform the input data in a compatible data frame format. Namely the user needs to specify: pts: the input forest fires dataset, with the coordinates to geolocalize each event. times: a vector of times, defined by the starting date of ignition. poly: a polygon of class matrix enclosing the input dataset (pts) s and tm: a vector of spatial (s) and a vector of temporal (tm) distances for the analysis. # Extract &quot;pts&quot; (divided by 1000 to compute in Km) FFN_pts&lt;-as.points(FFN$X/1000,FFN$Y/1000) FFN_times&lt;-FFN$Year # extract &quot;times&quot; # Extract the coordinates (in Km): PTN_xy&lt;-st_coordinates(PortN$geometry/1000) # Define the matrix with the set of bounding points (&quot;poly&quot;) enclosing the input dataset: FFN_poly&lt;-PTN_xy[, -c(3,4)] 3.5.2 Compute the space-time K-function Compute the space-time K-function for forest fires in the northern area. Since the computation can take a long time (about 20 mints), we propose you to load directly the output R object provided (STK_North_10y). The general code is also provided, but preceded by the hashtag, so it is not treated as a command: you have to remove # if you wish to evaluate the code. We consider here a subset of forest fires event occurred in the period 2001-2010. The parameters s and t are defined here as follows: (s) each kilometer distance up to ten kilometers; (tm) each year up to five years. NB: If you wish to run the code, remove # to make it work # Open stkhat documentation help(stkhat) library(readr) STK_North_10y &lt;- readRDS(&quot;data/Lab02/STK_North_10y.RData&quot;) str(STK_North_10y) ## List of 5 ## $ s : num [1:11] 0 1 2 3 4 5 6 7 8 9 ... ## $ t : num [1:6] 0 1 2 3 4 5 ## $ ks : num [1:11] 1.61e-04 9.03 2.98e+01 6.18e+01 1.05e+02 ... ## $ kt : num [1:6] 0.814 2.29 3.596 4.827 6.085 ... ## $ kst: num [1:11, 1:6] 1.45e-03 1.06e+01 3.75e+01 7.47e+01 1.22e+02 ... 3.5.3 Assess the space-time clustering behavior In the following section you will explore and plot the values of the three components produced as outputs of the function stkhat: the spatial K-function (ks), the temporal K-function (kt); the space-time K-function (kst). Finally you will plot the perspective 3D-plot of \\(D(s,t)\\) to evaluate the space–time clustering behavior of forest fires in the present case study. The multifaceted shape of this function, along the spatial and the temporal dimension, can help to identify peaks of clustering. Finally, the corresponding values can be attributed to the bandwidth of the kernel density estimators allowing to elaborate smoothed density maps in the last step of this lab. 3.5.3.1 Plot the stkhat outputs Plot of the purely spatial and the purely temporal K function. # Plot of the purely spatial K function plot(STK_North_10y$s, STK_North_10y$ks, type=&quot;l&quot;, xlab=&quot;distance&quot;, ylab=&quot;Estimated Ks&quot;, main=&quot;Spatial K function&quot;) lines(STK_North_10y$s, pi*STK_North_10y$s^2, type=&quot;l&quot;, col=&quot;red&quot;) # Plot of the purely temporal K-function plot(STK_North_10y$t, STK_North_10y$kt, type=&quot;l&quot;, xlab=&quot;time&quot;, ylab=&quot;Estimated Kt&quot;, main=&quot;Temporal K function&quot;) lines(STK_North_10y$t, 2*STK_North_10y$t, type=&quot;l&quot;, col=&quot;red&quot;) Plot the space-time D-plot # Define the function: D(s,t)=K(s,t)-[K(s)*K(t)] Dplot &lt;- function (stkhat, Dzero = FALSE, main=TRUE) { oprod &lt;- outer(stkhat$ks, stkhat$kt) st.D &lt;- stkhat$kst - oprod persp(stkhat$s, stkhat$t, st.D, xlab = &quot;Distance (Km)&quot;, ylab = &quot;Time (years)&quot;, zlab = &quot;D(s,t)&quot;, expand = 0.5, ticktype = &quot;detailed&quot;, theta = -45, shade = 0.75, cex = 0.7, ltheta=120, col=&quot;cyan1&quot;, font.lab=2) } Dplot(STK_North_10y) title(&quot;Dplot Nothern Fires&quot;) 3.5.3.2 Run the space-time kernel density estimator The multifaceted shape the D-Plot identify peaks of clustering a time value of 3 year. In spece, events are clustered and every distance, so in this case we use the maximum values (10 km). these two values are attributed to the bandwidth of the kernel density estimators allowing to elaborate smoothed density maps. # Open the help to analyse the parameter of this kernel function: help(kernel3d) # Run the function KDE_FFN&lt;-kernel3d(FFN_pts, FFN_times, seq(80, 362, 1), seq(180, 580, 1), seq(1990,2013,1), 10, 3) summary(KDE_FFN$v) hist(log10(KDE_FFN$v)) min(KDE_FFN$v[KDE_FFN$v&gt;0]) #check the lower non-zero value ## [1] 2.893333e-12 # Create quantile clssification Q&lt;-quantile(KDE_FFN$v, seq(0.5,1,0.05)) Q ## 50% 55% 60% 65% 70% 75% ## 0.0002194741 0.0097073381 0.0263666531 0.0478856382 0.0748979448 0.1093260662 ## 80% 85% 90% 95% 100% ## 0.1560709189 0.2210177893 0.3232627322 0.5162620818 2.1627084390 # Create a blue/red palette pal&lt;-colorRampPalette(c(&quot;grey&quot;,&quot;blue&quot;,&quot;green&quot;, &quot;yellow&quot;,&quot;orange&quot;, &quot;red&quot; )) colsR&lt;-pal(length(Q)-1) # Display classes pie(Q, clockwise=TRUE, labels=round(Q, digits=2), border=&quot;white&quot;, col=colsR) # Plot KDE maps for selected years oldpar&lt;-par(mfrow=c(5,5), mar=c(1,1,1,1)) for (i in 1:24){ (image(seq(80, 362, 1), seq(180, 580, 1), KDE_FFN$v[,,i], asp=1, xlab=&quot;&quot;, ylab=&quot;&quot;, main=1989+i, breaks=Q, col=colsR)) } 3.6 Conclusions and further analyses This practical computer lab allowed you to familiarize with the assement of the global cluster behavior of hazardous events, achieved by using the Ripley’s k-function. In addition, we learned how smoothed density maps can be elaborated from punctual events, namely using the kernel density estimator. Both spatial and the temporal dimension have been considered in this case. This method allowed us to explore density distribution of forest fires events through Continental Portugal in the period 1990-2013. In the northern half of the country, hot spots are present almost on each investigated years, with a higher concentration in the northern areas. To be sure that everything is perfectly clear for you, we propose you to do this lab again, by using this time another subset chosen among the forest fires in the Southern area, the small, medium or the large forest fires dataset. For whatever dataset you are going to use, try to answer to the following questions. The reading of the paper Tonini et al.7, which inspired this lab, can help you in in this task. At which spatial and temporal distance you can observe a peak of clustering? Describe which is spatio-temporal density distribution of forest fires events through the study area. References "],["density-based-clustering-algorithm.html", "Chapter 4 Density-based Clustering Algorithm 4.1 Introduction 4.2 The overall methodology 4.3 DBSCAN: feature detection from points clouds 4.4 Conclusions and further analyses", " Chapter 4 Density-based Clustering Algorithm 4.1 Introduction Terrestrial laser scanning (TLS) has been one of the most successful methods for 3D data collection in the last few years. Sequential acquisition can be used to detect and quantify surface changes. The three main challenges arising from TLS data collection are: 1) the large number of points acquired is computationally intense and datasets have to be filtered depending on the aim of the investigation; 2) data collection methods suffer from perspective effects, which can lead to either zones of occlusion (shadow effect) or spatially-variable point densities; 3) 3D point clouds are normally interpolated to digital elevation models either as regular grids or triangulated irregular networks. Thus, for change detection proposes (i.e. determination of topographic change, including erosion and deposition), it is appropriate to develop methods based upon the direct analysis of the point clouds using semi-automatic approaches allowing to detect and extract individual features. In this practical computer lab we introduce a semi-automated method developed for isolating and identifying features of topographic change (i.e., apparent changes caused by surface displacements and indicating erosion or deposition) directly from point cloud data using a Density-Based Spatial Clustering of Applications with Noise (DBSCAN). This methodology was developed by Natan Micheletti, Marj Tonini, and Stuart N. Lane8 for a very active rock glacier front located in the Swiss Alps: the Tsarmine rock glacier. Figure 2.1: The Tsarmine rock glacier, Hérens Valley, in the Western Swiss Alps. Source: Micheletti et al, 2016 4.2 The overall methodology Stepwise analysis: Point clouds were generated using a TLS on a number of dates. Point clouds were co-registered using stable zones within the surveyed area. Using a threshold value, only the points where there may have been some topographical change were selected. The final dataset was treated with DBSCAN, aiming at grouping cluster-points into single features and filtering out noise points, found in low-density regions. The present lab deals with steps 2 and 3: feature detection using DBSCAN. The detected features are finally labeled as clusters and visualized in a 3D map. Material loss or gain can be analysed in a GIS environment according to the elevation assignment of the change and the volume of change computed for each cluster by a triangulation process (not performed in this lab). 4.2.1 DBSCAN: 3-D density based clustering algorithm DBSCAN allows identification of spatial clusters of arbitrary shape on the base of the local density of points. Points that are close together are grouped into the same cluster, while isolated points are labelled as noise. Two parameters are required to perform this classification: the minimum number of points necessary to form a cluster (\\(MinPts\\)), and the neighborhood size epsilon (\\(eps\\)). The algorithm explores each point in the dataset, counting the number of the neighboring points falling within a circle (for the 2D model) or a sphere (for the 3D model) of radius equal to \\(eps\\) (a). If this number is equal to or greater than \\(MinPts\\), points are labelled as belonging to the same cluster (b). If this number is lower than \\(MinPts\\), points are classified as noise. The central point of each cluster is called core-point. Since some points can be density-reachable by more than one core-point, they can belong to more than one cluster. In this case clusters are blended together to form a unique cluster feature of arbitrary shape (c). Figure 4.1: Parameters in DBSCAN to form a cluster 4.2.2 Field campaign An ultra-long range LiDAR RIEGL VZ-6000 scanner was employed to acquire sequential 3D datasets of the rock glacier front. TLS scans were performed on different dates over two consecutive summers: a first survey was carried out on the 23th of September 2014, and the last one on the 22th of September 2015. The instrument is equipped with on-board inclination sensors: this allows to use the inclination data to obtain (locally) georeferenced datasets where the Z dimension represents the elevation above the X-Y plane. After co-registration, a mask was used to restrict point clouds to the area of interest: the front of the rock glacier and the corridor below. DBSCAN requires as input a 3-dimensional dataset, which in our case consists on the points clouds (X,Y,Z) plus the displacement distance. For the two co-registered datasets (2014 and 2015), we set the first as the target and the more recent as the reference. For each point in the target cloud the corresponding nearest point in the reference cloud was identified and the distance between them evaluated using the software Cloud Compare. Co-registration errors, estimated as noise and not real material loss/gain signals, has to be removed from the analysis. Here, we noted from field observations that the size of displaced boulders is typically &gt; 0.30 m and we use this as a change criteria to remove the noise. 4.3 DBSCAN: feature detection from points clouds 4.3.1 Load the libraries Fist you have to load the following libraries: - dbscan: A fast implementation of several density-based algorithms of the DBSCAN family. - rgl: Provides medium to high level functions for 3D interactive graphics - plot3Drgl: Plot 3D graphs in rgl window. - classInt: Selected methods to choose class intervals for mapping puposes. - RColorBrewer: Provides color schemes for mapping. library(&quot;dbscan&quot;) library(&quot;rgl&quot;) library(&quot;plot3Drgl&quot;) library(&quot;classInt&quot;) library(&quot;RColorBrewer&quot;) (.packages()) ## [1] &quot;RColorBrewer&quot; &quot;classInt&quot; &quot;plot3Drgl&quot; &quot;plot3D&quot; &quot;rgl&quot; ## [6] &quot;dbscan&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; ## [11] &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 4.3.2 Import and visualise the point clouds dataset We provide the dataset corresponding to one-year displacement distance, masked over the the rock glacier front. The noise-points are removed using a threshold of 30 cm. Finally we will plot the 3D-points cloud filtered dataset using the displacement distance as attribute to display the map. # Import point cloud dataset (ptc) # 1 year displacement TLS campaigns; data masked over the active front only. ptc &lt;- read.table(&quot;data/Lab03/TsarmineRG_230914_frontonly_XYZ_dist_ref_220915.txt&quot;, header=FALSE, sep=&quot;&quot;) # Add names to columns: X,Y, Z coordinates and the displacement distance &quot;d&quot;. colnames(ptc)&lt;-c(&quot;Y&quot;,&quot;X&quot;,&quot;Z&quot;,&quot;d&quot;) # rename columns # Inspect the dataset: str(ptc) summary(ptc$d) hist(log10(ptc$d)) # Create a subset: removing noise-points (d&gt;30cm). ptc30 &lt;- subset(ptc, d&gt;=0.3) # Inspect the subset: str(ptc30) summary(ptc30$d) # Plot-3D: display 3D plot (X,Y,Z) with class intervals based on the distance d. # Create a class interval (int) based on natural breaks: int &lt;- classIntervals(ptc30$d, n=5, style=&quot;fisher&quot;) cut.vals&lt;-int$brks d.cut &lt;- cut(ptc30$d,cut.vals) cut.levels &lt;- levels(d.cut) cut.band &lt;- match(d.cut,cut.levels) cls &lt;- rev(brewer.pal(length(cut.levels), &quot;RdYlGn&quot;)) # Display the 3D-plot: scatter3D(ptc30$X, ptc30$Y, ptc30$Z, colvar =ptc30$d, breaks=cut.vals, cex=0.5) plotrgl() #animated 3D-plot 4.3.3 Run DBSCAN The two parameters \\(eps\\) and \\(MinPts\\) greatly affect the output cluster detection leading the the identification of a large number of small clusters (for small values) or a small number of large features of arbitrary shape (for large values). Once fixed the \\(MinPts\\) parameter, a suitable value for the \\(eps\\) neighborhood size can be estimated using a k-nearest neighbors (k-NN) distance graph, imposing k as equal to \\(MinPts\\) and plotting the distance to the nearest neighbors. The optimal \\(eps\\)-value should coincide with the stronger curvature of the curve. In the following, the minimum number of points allowing to form a cluster is fixed first, and then the plot of the k-nearest neighbor (k-NN) distance is used to find a suitable value for the \\(eps\\) neighborhood size. We can use this function to find the NN-distance (for a given k-value) corresponding to the stronger curvature. This values gives the best neighborhood size \\(eps\\) # Plot the k-NN distance graph (with k=10) kNNdistplot(ptc30[,-4], k = 10) Once the best \\(MinPts\\) adnd \\(eps\\) parameter have been selected, the DBSCAN function can be run. # Eps=1m (~3xsigma) ; MinPts=10 cl30&lt;-dbscan(ptc30, 1, minPts=10) # Inspect the results: str(cl30) summary(cl30$cluster) sum(cl30$cluster==0) # Plot only the detected clusters. # Join the data: cl30&lt;-cbind(ptc30, cluster=cl30$cluster) # Save as data frame and remove the noise (label==0) cl30_df&lt;-as.data.frame(subset(cl30, cl30$cluster&gt;=1)) # Simple plot: plot(cl30_df$X, cl30_df$Y, col=cl30_df$cluster, pch=20) # Animated 3D-plot: plot3d(cl30_df, col=cl30_df$cluster, pch=20) Finally you can export the result as a .txt file and import in a GIS system for further analyses, such as to determine the volumes of loss or gain of material. write.table(cl30_df, file=&quot;cls30_1_10.txt&quot;, sep=&quot;\\t&quot;) 4.4 Conclusions and further analyses The proposed method permitted detection of features of changes on a rock glacier front located in the Swiss Alps. Single cluster features of erosion and deposition/front movements were extracted directly from point clouds, detected by DBSCAN without the necessity of interpolate the 3-D original data. To be sure that everything is perfectly clear for you, we propose you to do this lab again, by changing the values of MinPts and eps and discuss the compare the resulting extracted features. References "],["self-organizing-map.html", "Chapter 5 Self-organizing Map 5.1 Introduction 5.2 Material and method 5.3 Variables inspection 5.4 Running SOM 5.5 SOM’s main outputs 5.6 Hierarchical clustering 5.7 Conclusions 5.8 Further analyses", " Chapter 5 Self-organizing Map 5.1 Introduction The increasing availability of multivariate datatets stimulates researches to develop new techniques that differ from those utilized in earlier scientific paradigms. In the field of social and economic science, geo-demographic segmentation defines a technique used to classify a population based on input data describing administrative units and people living there.9 Small areas can thus be classified into discrete categories using demographic input data, by means of data reduction techniques (such as the Principal component analysis). The main limitation of these approaches is the problem of communicating the multidimensional complexity characterizing the different output classes. Machine learning techniques can help in this context, as they are designed to extract useful formation and insight from the interaction of the variables describing the complex structure of a given phenomenon. In the present computing lab we introduce an unsupervised learning procedure based on Self-Organizing Map (SOM),10 allowing detecting clusters and characterizing the pattern of population dynamic in Switzerland in the recent period. This lab is inspired by the work of Marj Tonini et al.11 5.2 Material and method 5.2.1 The geo-demographic context Switzerland has the highest life expectancy in the world. It counts about 8,5 million inhabitants (official census data 2020), twice as much as at the beginning of the 20th century, mainly because of the high level of immigration. The number of foreigners that currently reside in the country corresponds to about one quarter of the total population. Most of the population (85%) lives in cities. Population aging increased over the course of the 20th century, with one in five person of retirement age today. In the present study, we developed a machine learning based approach to understand and describe the population patterns in different geographic areas. 5.2.2 SOM We use SOM, an unsupervised competitive learning neural network allowing representing a high-dimensional feature space (defined by the multivariate input dataset), as two-dimensional discretized pattern (the SOM grid of neurons). In SOM the proximity of the units in the output grid reflects the similarity of the corresponding input observations, which guarantees the preservation of the topological structure of the input data. Compared to other approaches, SOM is very efficient for data visualization. Indeed it provides additional outputs such as the heatmaps, representing the distribution of each input feature across the SOM grid, extremely useful to visually explore the relationship between the input variables. 5.2.3 Swiss census data Input data come from the national population census, provided by the Swiss Federal Statistical Office, and including information on the socio-economic status of the population and surveyed land use and land cover. This information has been aggregated to the municipality level for the purpose of the present investigation. Census data 2020 have been considered to analyse the current pattern, while the former census surveys can be considered to assess transitions and evaluate population dynamics. 5.3 Variables inspection 5.3.1 Import the data Fist, you have to import the Swiss census dataset for the year 2020, referred to the municipality administrative units. As you can see from the description of the selected variables , some of them can be discarded from the analysis. So, in the following step, we extract a subset of the most meaningful variables for the purpose of the present study. knitr::include_graphics(c(&quot;images/Variables1.jpg&quot;, &quot;images/Variables2.jpg&quot;)) Figure 5.1: Variables description Figure 5.2: Variables description 5.3.2 Inspect the data Plotting the histogram of the variables distribution allows to detect outliers, see if they have comparable range of values, and evaluate if a data transformation is necessary. # Plot variables&#39; frequency distribution for(i in 3:ncol(subset2020)) { hist((subset2020[ , i]), main=colnames(subset2020[i])) } 5.3.3 Data transformation Data transformation seeks the make the variables range comparable. This process ensure the data entries to look similar across all fields and records, making information easier to find, group and analyze. With Min-Max Scaling, the data values are scaled in a range between 0 and 1. As a consequence, the effect of outliers suppresses to a certain extent. Moreover, it helps us to have a smaller value of the standard deviation of the data scaled. # Custom function to implement min max scaling minMax &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) } # Apply max-min to the data dfnorm2020&lt;- as.data.frame(lapply((subset2020[ , -c(1,2)]), minMax)) summary(dfnorm2020) # Plot the histograms of the transformed data for(i in 1:ncol(dfnorm2020)) { hist((dfnorm2020[ , i]), main=colnames(dfnorm2020[i])) } 5.4 Running SOM 5.4.1 Load the libraries Fist you have to load the following libraries: kohonen: Supervised and Unsupervised Self-Organizing Maps (SOM) aweSOM: offers a set of tools to explore and analyze datasets with SOM ggplot2: create Elegant Data Visualizations Using the Grammar of Graphics colorpatch: rendering methods (ggplot extensions) library(kohonen) library(aweSOM) library(ggplot2) library(colorpatch) (.packages()) 5.4.2 Compute SOM The main idea with SOM is to map the input high-dimensional feature space in a lower-dimensional output space organized on a grid made up of regular units (i.e. the neurons). At the end of the process, each observation from the input space (\\(X_{k,i}\\)) will be associated (i.e., mapped) to a unit in the SOM grid. Depending on the size of the grid, one unit can include several input observations. The first step consists in defining the size and geometry of the SOM grid. In this case we define a rectangular grid of 18 by 13 units, allowing to allocate, on average, about 15 input-items per units, and whose geometry reproduces the shape of the study area. (NB: several parameters and configurations of the SOM grid can be implemented and compared, seeking to minimize the Quantization Error, QE) Finally, you can run the SOM model. Note that rlen indicates the number of times the complete data set will be presented to the network, while for the other parameters we will keep the default values. The overall computation include the following steps: 1) The data frame has to be transformed to a matrix mxM2020&lt;-as.matrix(dfnorm2020) # max-min 2) Create the SOM-grid Before running SOM, you have to create the grid of output units. # Gird size 18x13 units som_grid &lt;- somgrid(xdim = 18, ydim=13) 3) Run-SOM The general R function is used for creating simulations of random objects that can be reproduced. # Use max-min data transformation set.seed(123) # Run SOM SOM2020M&lt;- som(mxM2020, grid=som_grid, rlen=1000) print(SOM2020M) ## SOM of size 18x13 with a rectangular topology. ## Training data included. 4) SOM-model evaluation Finally, you can optimize the size of the grid by inspecting several quality measure and changing the parameters accordingly. In particular we will explore the following: Changes: shows the mean distance to the closest codebook vector during the training. Quantization error: average squared distance between the data points and the map’s codebook to which they are mapped. Lower is better. Percentage of explained variance: similar to other clustering methods, the share of total variance that is explained by the clustering (equal to 1 minus the ratio of quantization error to total variance). Higher is better. # Evaluate rlen plot(SOM2020M, type=&quot;changes&quot;) # Evaluate the results QEM&lt;-somQuality(SOM2020M, dfnorm2020) ## Quality measures: QEM$err.quant # Quantization error ## [1] 0.04534672 QEM$err.varratio # % explained variance ## [1] 82.06 5.5 SOM’s main outputs The main graphical outputs of SOM are the node counts, the neighborhood distances, and the heatmaps. Node counts map informs about the number of input vectors falling inside each output unit. Neighbourhood distance plot shows the distance between each unit and its neighborhoods. Heatmaps show the distribution of each input variable, associated to each input vectors, across the SOM grid. # Create a color palette coolBlueHotRed &lt;- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]} par(mfrow = c(1, 2)) # Plot node counts map plot(SOM2020M, type=&quot;count&quot;, main=&quot;Node Counts&quot;, palette.name=coolBlueHotRed) # Plot SOM neighbourhood distance plot(SOM2020M, type=&quot;dist.neighbours&quot;, main = &quot;SOM neighbour distances&quot;) Visualized side by side, heatmaps provide a useful tool to explore the correlation between the input variables . # Plot heatmaps for selected variables for (i in 1:18) { plot(SOM2020M, type = &quot;property&quot;, property = getCodes(SOM2020M)[,i], main=colnames(getCodes(SOM2020M))[i], palette.name=coolBlueHotRed) } knitr::include_graphics(&quot;images/Heatmaps.jpg&quot;) Figure 5.3: Heatmaps 5.6 Hierarchical clustering Hierarchical clustering can finally be performed to isolate groups of input vectors characterized by a similar distribution of the input variables. This second step takes the SOM-units (i.e., the CodeBook vectors) as input and group them into the desired number of clusters (we choose 7 in this case). This allows to characterize main typologies of geo-demographic groups. Apply hierarchical clustering: CB2020M &lt;- getCodes(SOM2020M) # Extract codebook vectors cls2020M &lt;- cutree(hclust(dist(CB2020M)), 7) Display the SOM-grid with different colors for each cluster: # Colour palette definition, will use later too. # If you change the number of hierarchical clustering, remember change the number of color. map_palette &lt;- c(&quot;steelblue3&quot;,&quot;darkgoldenrod4&quot;,&quot;darkolivegreen&quot;, &quot;springgreen3&quot;, &quot;darkorange&quot;, &quot;firebrick&quot;, &quot;darkolivegreen1&quot; ) plot(SOM2020M, type=&quot;mapping&quot;, pchs=&quot;&quot;, bgcol = map_palette[cls2020M], main = &quot;Clusters&quot;) Assign the cluster number (based on hierarchical clustering) to each unit: clasgn &lt;- cls2020M[SOM2020M$unit.classif] dfnorm2020$hc&lt;-clasgn clsnorm&lt;-cbind(dfnorm2020, subset2020$BFS_nummer) # Export the table with clusters write.table(clsnorm, file=&quot;hcM_7cls.csv&quot;, sep=&quot;,&quot;) Map the resulting clusters to the geographical space You can import the final table with the values of the population census variable in a GIS and join the values to the administrative limits at municipality levels in Switzerland. This allows you to elaborate a map as in . NB. You need only two columns: the code identifying each municipality (“BFS_nummer”) and the cluster number (hc). Open the table including the resulting clusters with a dedicated program (like Excel) to reorganize the header, as a new column for unique identifier has been created automatically (you can name it “ID”), and headers has to be shifted of one place on the right. Figure 5.4: Spatial pattern distribution of Swiss poputaion census If you want to try the primary and automatic process in Rstudio, you can conduct the below codes and see the map. library(st) library(sf) library(ggplot2) library(ggspatial) library(dplyr) # Load the shapefile data CH_outline &lt;- st_read(&quot;data/Lab04/Municiplities.shp&quot;) # Quick check the shapefile data ggplot() + geom_sf(data = CH_outline, size = 1.5, color = &quot;black&quot;, fill = &quot;cyan1&quot;) + ggtitle(&quot;Swiss municiplities&quot;) + coord_sf() # Rename the column colnames(clsnorm)[20] &lt;- &quot;BFS_NUMMER&quot; # Merge the dataframe and shapefile together by the column with the same name z = merge(CH_outline, clsnorm, by = &quot;BFS_NUMMER&quot;) # Rename the column with clustering results z$cluster &lt;- as.numeric(z$hc) # Mapping the clusters via ggplot like QGIS/ArcGIS Pro/Arcmap, similar package &#39;tmap&#39; CH_cluster &lt;- ggplot(data = z) + # Original data geom_sf(aes(fill = as.factor(cluster))) + # Mapping the clusters annotation_scale(location = &quot;bl&quot;, width_hint = 0.3, pad_x = unit(0.2, &quot;cm&quot;), pad_y = unit(0.1, &quot;cm&quot;)) + # Mapping scales, which are calculated by project coordinates annotation_north_arrow(location = &quot;tr&quot;, which_north = &quot;true&quot;, pad_x = unit(0.0, &quot;cm&quot;), pad_y = unit(0.1, &quot;cm&quot;), style = north_arrow_fancy_orienteering) + # Mapping north arrow scale_colour_manual(values = map_palette) + # Set up the our color palette theme_minimal() + # Background color, if use, it means without color labs(title = &quot;Spatial pattern distribution of Swiss poputaion census&quot;) + # Insert title of the map scale_fill_discrete(name = &quot;Clusters&quot;, labels = c(&quot;1: Unproductive&quot;, &quot;2: Forest/Peri-urban&quot;, &quot;3: Peri-urban/Rural&quot;, &quot;4: Rural&quot;,&quot;5: Suburban&quot;,&quot;6: Urban centre&quot;,&quot;7: Peri-urban fringe&quot;)) # Legend labels guides(fill=guide_legend(title=&quot;Clusters&quot;)) # Legend name # Save the map as jpg ggsave(&quot;CH_cluster.jpg&quot;) CH_cluster 5.6.1 Clusters characterization To interpret the final main clusters in therms of their geo-demographic characteristics, you can evaluate the distribution of each variable within the clusters by using box plots (also known as whisker plot). Box-plot is a standardized way to display a dataset based on the five-number summary statistics: the minimum, the maximum, the sample median, and the first and third quartiles (i.e., the median of the lower half (25%) and the median of the upper half (75%) of the dataset). # Creates an empty list object that will be filled by the loop cls20M &lt;- list() # Split the single clusters for(i in 1:7) { cls20M[[i]]&lt;-subset(clsnorm, clsnorm$hc==i) } clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(1:18)) # Box-plots for the single clusters for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), mar=c(8,3,3,1), cex.axis=0.5, las=2)} To better investigate the values assumed by each class of variables withing the different clusters, you can group them by category. # Box plot by categories: &quot;Physical space&quot; clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(1:5)) par(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7) for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), las=2)} Figure 5.5: Physical space # Box plot by categories: &quot;Demographics&quot; clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(6:13)) par(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7) for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), las=2)} Figure 5.6: Demographics # Box plot by categories: &quot;Socio-economics&quot; clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(14:18)) par(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7) for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), las=2)} Figure 5.7: Socio-economics 5.7 Conclusions Results of the present study reveal the main patterns of the population in Switzerland based on the surveyed land-use, socio-economic, and demographic indicators. To characterize the final main clusters, the distribution of each variable within them have been assessed by using box plots. We could thus identify main clusters including the most developed and active cities with higher income, the peri-urban areas mostly devoted to the agricultural activity, or the areas with higher levels of migration. SOM heatmaps allow to display the pattern distribution of each input variable over the SOM-grid and how values change in space. Visualized side by side, heatmaps show a picture of the different areas and their characteristics. In this way it is possible to explore the level of complementarity that links one or more variables among them. In conclusion, in the present study we proposed a performant data-driven approach based on unsupervised learning allowing to extract useful information from a huge volume of multivariate population census data. This approach led to represent and interpret the main patterns characterizing the dynamic of population in Switzerland in the recent period. 5.8 Further analyses To be sure that everything is perfectly clear for you, we propose you to answer the following questions and to discuss your answers with the other participants to the course or directly with the teacher. Change the size of the gridmap and check if you get better results for SOM. N.B. Better results are achieved when the quantisation error decreases, the explained variance increases, and there are no empty observations revealed by the Node Counts map. 2.1) Focusing on Cluster 6: which variable characterize it the most? Based on these variables, which class of land use can be associated to this cluster? 2.2) And in the case of Cluster 4? Describe the distribution of the clusters in the geographical space. In more details, describe to which kind of land use the different clusters can be referred and specify why. From the visual inspection of the heatmaps, describe the correlation you can observe between the following variables: a) “p_transport” and “p_infrastructure”; b) “p_agriculture” and “p_improductible”. References "],["random-forest.html", "Chapter 6 Random Forest 6.1 Introduction 6.2 Import the data 6.3 Create the input dataset 6.4 Run Random Forest 6.5 Susceptibility map 6.6 Variable importance plot 6.7 Conclusions and further analyses", " Chapter 6 Random Forest 6.1 Introduction In this application, we explore the capabilities of a stochastic approach based on a machine learning (ML) algorithm to elaborate landslides susceptibility mapping in Canton Vaud, Switzerland. Generally speaking, ML includes a class of algorithms for the analysis, modelling, and visualization of environmental data and it performs particularly well to model environmental hazards, which naturally have a complex and non-linear behavior. Here we use Random Forest, an ensemble ML algorithm based on decision trees. The research framework that inspired this computational lab refers to a pioneering study in susceptibility mapping for wildfire eventy by Marj Tonini et al.12 and further developed for the assessment of variable importance by Andrea Trucchia et al.13 6.1.1 The main objective Landslides are one of the major hazard occurring around the world. In Switzerland, landslides cause damages to infrastructures and sometimes threaten human lives. Shallow landslides are triggered by intense rainfalls. Such slope movements are generally very rapid and hardly predictable. Different quantitative approaches have been developed to assess the most susceptible areas. This project applies a data-driven methodology based on Random Forest (RF) (Leo Breiman14) to elaborate the landslides susceptibility map of canton of Vaud, in Switzerland. RF is applied to a set of independent variables (i.e., the predictors) and dependent variables (the inventoried landslides and an equal number of locations for absences). The overall methodology is described in the following graphic (). Figure 6.1: Basic elements of the generic methodology 6.1.2 Load the libraries To perform the analysis, you have first to install the following libraries: library(raster): The raster package provides classes and functions to manipulate geographic (spatial) data in ‘raster’ format. library(readr): The goal of ‘readr’ is to provide a fast and friendly way to read rectangular data (like ‘csv’, ‘tsv’, and ‘fwf’). library(randomForest): Classification and regression based on a forest of trees using random inputs, based on Breiman (2001). library(dplyr): It is the next iteration of plyr, focused on tools for working with data frames (hence the d in the name). library(pROC): Allowing to compute, analyze ROC curves, and library(plotROC) to display ROC curve (ggplot2): Is a system for declaratively creating graphics. library(sf): Support for simple features, a standardized way to encode spatial vector data. library(raster) library(readr) library(randomForest) library(dplyr) library(pROC) library(plotROC) library(ggplot2) library(sf) library(classInt) (.packages()) ## [1] &quot;classInt&quot; &quot;sf&quot; &quot;plotROC&quot; &quot;ggplot2&quot; &quot;pROC&quot; ## [6] &quot;dplyr&quot; &quot;randomForest&quot; &quot;readr&quot; &quot;raster&quot; &quot;sp&quot; ## [11] &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; ## [16] &quot;methods&quot; &quot;base&quot; List of the libraries 6.2 Import the data Import the landslides punctual dataset presences and pseudo-absences (LS_pa) and predictors (in raster format). This help the exploratory data analyses steps and to understand the input data structure. 6.2.1 Landslides dataset The landslide inventory has been provided by the environmental office of the canton of Vaud. Only shallow landslides are used for susceptibility modelling. One pixel per landslide-area (namely the one located at the highest elevation) has been extracted. Since the landslide scarp is located in the upper part of the polygon, it makes sense to consider the highest pixel to characterize each single event. Our model includes the implementation of the landslide pseudo-absences, which are the areas where the hazardous events did not took place (i.e. landslide location is known and the mapped footprint areas are available, but the non-landslide areas have to be defined). Indeed, to assure a good generalization of the model and to avoid the overestimation of the absence, pseudo-absences need to be generated in all the cases where they are not explicitly expressed. In this case study, an equal number of point as for presences has been randomly generated in the study area, except within landslides polygons, lakes and glaciers (that is what is called “validity domain”, where events could potentially occur). # Import the landslides dataset (dependent variable) LS_pa &lt;- read.csv(&quot;data/Lab05/LS_pa.csv&quot;) # Convert the numeric values (0/1) as factor ##(i.e. categorical value) LS_pa$LS&lt;-as.factor(LS_pa$LS) # Display the structure (str) and result summaries (summary) str(LS_pa) summary(LS_pa) # Plot the events plot(LS_pa$X,LS_pa$Y, col=LS_pa$LS, pch=20, cex=0.7) 6.2.2 Predictor variables Selecting predictor variables is a key stage on susceptibility ans risk modelling when using a data-driven approach. There is no consensus about the number of variables and which variables should be used for landslides assesment. In the present exercise we will use the following: DEM (digital elevation model): provided by the Swiss Federal Office of Topography. The elevation is not a direct conditioning factor for landslide; however, it can reflect differences in vegetation characteristics and soil. Slope: is one of the most explicating factor in landslide susceptibility modelling. It is computing as: \\[Slope = arctan(\\sqrt{(dz/dx)^2 + (dz/dy)^2)} * (\\pi/2)\\] Curvature: curvature is widely used in landslide susceptibility modelling. It allows assessing the water flow acceleration and sediment transport process (profile curvature) and the water flow propensity to converge and diverge (plan curvature). They were derived from DEM using the curvature tool in ArcGIS. TWI (topographical water index): topography plays a key role in the spatial distribution of soil hydrological conditions. Defining \\(\\alpha\\) as the upslope contributing area describing the propensity of a cell to receive water, and \\(\\beta\\) as the slope angle, TWI (compute by the formula below), reflects the propensity of a cell to evacuate water: \\[TWI=ln(\\alpha/tan(\\beta))\\] Distance to roads: roads build in mountainous areas often cut the slope, weakening the cohesion of the soil. Moreover, roads surfaces are highly impermeable. This raster has been elaborated using the euclidean distance tool in ArcGIS, from the swissTLMRegio map where roads are represented by lines. Land Cover: developed by the Swiss administration and based on aerial photographs and control points. It includes 27 categories distributed in the following 6 domains: human modified surfaces, herbaceous vegetation, shrubs vegetation, tree vegetation, surfaces without vegetation, water surfaces (glaciers included). Geology: The use of the lithology increase the performance of the susceptibility landslide models. We use here the map elaborated by the Canton Vaud, defining the geotypes and reclassified in 10 classes in order to differentiate sedimentary rocks. ## Import raster (independent variables) 25 meter resolution landCover&lt;-as.factor(raster(&quot;data/Lab05/landCover.tif&quot;)) geology&lt;-as.factor(raster(&quot;data/Lab05/geology.tif&quot;)) planCurv&lt;-raster(&quot;data/Lab05/plan_curvature.tif&quot;)/100 # this because ArcGIS multiply profCurv&lt;-raster(&quot;data/Lab05/profil_curvature.tif&quot;)/100 # ....curvature values by 100 TWI&lt;-raster(&quot;data/Lab05/TWI.tif&quot;) Slope&lt;-raster(&quot;data/Lab05/Slope.tif&quot;) dem&lt;-raster(&quot;data/Lab05/DEM.tif&quot;) dist&lt;-raster(&quot;data/Lab05/dist_roads.tif&quot;) Than the predictor variables (features) have to be aggregated. We use here the function stack to create a collection of RasterLayer objects with the same spatial extent and resolution. #create a Raster stack features&lt;-stack(dist, dem, TWI, planCurv, profCurv, Slope, geology, landCover) # Renames the variables names(features)&lt;-c(&quot;distRoad&quot;, &quot;DEM&quot;, &quot;TWI&quot;, &quot;planCurv&quot;, &quot;profCurv&quot;, &quot;Slope&quot;, &quot;Geology&quot;, &quot;LandCover&quot;) plot(features) 6.2.2.1 The use of categorical variables in Machine Learning The majority of ML algorithms (e.g., support vector machines, artificial neural network, deep learning) makes predictions on the base of the proximity between the values of the predictors, computed in terms of euclidean distance. This means that these algorithms can not handle directly categorical values (i.e., qualitative descriptors). Thus, in most of the cases, categorical variables need to be transformed into a numerical format. One of the advantage of using Random Forest (as implemented in R) is that it can handle directly categorical variables, since the algorithm operate by constructing a multitude of decision trees at training time and the best split is chosen just by counting the proportion of each class observation. To understand the characteristics of the categorical variables, you can plot the tow rasters Land Cover and Geology by using their original classes. To visualize the data, you can use R as a GIS, but you need to perform few data manipulations. # Create a random color palette library(RColorBrewer) n &lt;- 50 qual_col_pals = brewer.pal.info[brewer.pal.info$category == &#39;qual&#39;,] col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals))) # Load libraries for levelplot function, # allowing to plot raster based of categorical data. library(lattice) library(rasterVis) Raster geology: extract the attribute table and plot the map based on the geological classes. # ensure that the raster attributes are read as type &quot;factor&quot; geology &lt;- as.factor(raster(&quot;data/Lab05/geology.tif&quot;)) # Add a geological class column to the Raster Attribute Table rat &lt;- levels(geology)[[1]] rat$Geology_classes &lt;- c(&quot;glacial deposits&quot;, &quot;carbonates&quot;, &quot;marly-limestone&quot;, &quot;alluvial deposits&quot;, &quot;artificial materials&quot;, &quot;slope deposits&quot;, &quot;detrital rocks&quot;, &quot;metamorphic rocks&quot;, &quot;evaporitic rocks&quot;,&quot;magmatic rocks&quot;) levels(geology) &lt;- rat levels(geology) # Plot the raster based on the geological classes levelplot(geology, col.regions=col_vector) Raster land cover: extract the attribute table and plot the map based on the land cover classes. landCover &lt;- as.factor(raster(&quot;data/Lab05/landCover.tif&quot;)) ratLC &lt;- levels(landCover)[[1]] ratLC$LandCover_classes &lt;- c(&quot;impermeable man-made&quot;,&quot;permeable man-made&quot;, &quot;herbaceous vegetation&quot;,&quot;shrub vegetation&quot;, &quot;forest&quot;,&quot;no vegetation&quot;,&quot;glacier and water body&quot;) levels(landCover) &lt;- ratLC levels(landCover) # Plot the raster based on the land cover classes levelplot(landCover, col.regions=col_vector) 6.3 Create the input dataset In this step, you will extract the values of the predictors at each location in the landslides (presences and absences) dataset. # Shuffle the rows LS_sh &lt;- LS_pa [sample(nrow(LS_pa), nrow(LS_pa)), ] # Convert to spatial point dataframe LS_spdf&lt;-SpatialPointsDataFrame(LS_sh[,c(&quot;X&quot;, &quot;Y&quot;)], LS_sh, proj4string=crs(features)) LS_input &lt;- extract(features, LS_spdf, df=TRUE) # Add independent variable (LS) as additional column to the dataframe LS_input$LS &lt;- as.factor(LS_spdf$LS) # Convert Land cover and Geology classes as factor LS_input$LandCover &lt;- as.factor(LS_input$LandCover) LS_input$Geology &lt;- as.factor(LS_input$Geology) # Remove extra column (ID) LS_input &lt;- LS_input[,2:ncol(LS_input)] LS_input &lt;- na.omit(LS_input) # Explore the newly created input dataset. head(LS_input) str(LS_input) 6.3.1 Split the input dataset into training (80%) and testing (20%) A well-established procedure in ML is to split the input dataset into training, validation, and testing. The training dataset is needed to calibrate the parameters of the model, which will be used to get predictions on new data. The purpose of the validation dataset is to optimize the hyperparameter of the model (training phase). NB: in Random Forest this subset is represented by the Out-Of-Bag (OOB)! To provide an unbiased evaluation of the final model and to assess its performance, results are then predicted over unused observations (prediction phase), defined as the testing dataset. # Split the input dataset into training (80%) and testing (20%) n &lt;- nrow (LS_input) set.seed(123) n_train &lt;- round(0.80 * n) train_indices &lt;- sample(1:n, n_train) # Create indices LS_input_train &lt;- LS_input[train_indices, ] LS_input_test &lt;- LS_input[-train_indices, ] # Count the number of elements in the two subset: training and testing count(LS_input_train) count(LS_input_test) 6.4 Run Random Forest Computationally, a subset of the training dataset is generated by bootstrapping (i.e. random sampling with replacement). For each subset a decision tree is grown and, at each split, the algorithm randomly selects a number of variables (mtry) and it computes the Gini index to identify the best one. The process stops when each node contains less than a fixed number of data points. The fundamental hyperparameters that needs to be defined in RF are mtry and the total number of trees (ntrees). The prediction error on the training dataset is finally assessed by evaluating predictions on those observations that were not used in the subset, defined as “out-of-bag” (OOB). This values is used the optimize the values of the hyperparameters, by a trial and error process (that is, trying to minimize the OOB estimate of error rate). # Set the seed of R‘s random number generator, ## this is useful for creating simulations that can be reproduced. set.seed(123) # Run RF model RF_LS&lt;-randomForest(y=LS_input_train$LS, x=LS_input_train[1:8],data=LS_input_train, ntree=200, mtry=3,importance=TRUE, do.trace=TRUE) 6.4.1 RF main outputs Printing the results of RF allows you to gain insight into the outputs of the implemented model, namelly the following: - a summary of the model hyperparameters - the OOB estimate of error rate - the confusion matrix; in this case a 2x2 matrix used for evaluating the performance of the classification model (1-presence vs 0-absence). The plotting the of the error rate is useful to estimate the decreasing values on the OOB and on the predictions (presence (1) / absence (0)) over increasing number of treees. print(RF_LS) ## ## Call: ## randomForest(x = LS_input_train[1:8], y = LS_input_train$LS, ntree = 200, mtry = 3, importance = TRUE, do.trace = TRUE, data = LS_input_train) ## Type of random forest: classification ## Number of trees: 200 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 16.54% ## Confusion matrix: ## 0 1 class.error ## 0 1673 413 0.1979866 ## 1 273 1789 0.1323957 plot(RF_LS) legend(x=&quot;topright&quot;, legend=c(&quot;perd 0&quot;, &quot;pred 1&quot;, &quot;OOB error&quot;), col=c(&quot;red&quot;, &quot;green&quot;, &quot;black&quot;), lty=1:2, cex=0.8) # Show the predicted probability values RF.predict &lt;- predict(RF_LS,type=&quot;prob&quot;) head(RF.predict) # 0==abstece ; 1== presence ## 0 1 ## 2465 0.97333333 0.02666667 ## 2513 0.97674419 0.02325581 ## 2229 0.71250000 0.28750000 ## 527 0.52564103 0.47435897 ## 4293 0.05194805 0.94805195 ## 2988 0.04918033 0.95081967 6.4.2 Model evaluation The prediction capability of the implemented RF model can be evaluated by predicting the results over previously unseen data, that is the testing dataset. The Area Under the “Receiver Operating Characteristic (ROC)” Curve (AUC) represents the evaluation score used here as indicator of the goodness of the model in classifying areas more susceptible to landslides. ROC curve is a graphical technique based on the plot of the percentage of correct classification (the true positives rate) against the false positives rate (occurring when an outcome is incorrectly predicted as belonging to the class “1” when it actually belongs to the class “0”), evaluates for many thresholds. The AUC value lies between 0.5, denoting a bad classifier, and 1, denoting an excellent classifier. # Make predictions on the testing dataset RFpred_test &lt;- predict(object = RF_LS, newdata = LS_input_test, type=&quot;prob&quot;) roc &lt;- roc(LS_input_test$LS,RFpred_test[,2]) plot(1-roc$specificities,roc$sensitivities, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &quot;False positive rate&quot;, ylab = &quot;True positive rate&quot;) roc 6.5 Susceptibility map You have now all the elements to elaborate the final landslide susceptibility map. This will be achieved by making predictions (of presence only) based on the values of the predictors, which are stored into the RasterStack named *features*, created above. # Index=2 indicate the prediction of presence (1) Scp_pred &lt;- predict(features, RF_LS,type=&quot;prob&quot;, index=2) # Summary statistics summary(Scp_pred) hist(Scp_pred) # Export ratster writeRaster(Scp_pred,&quot;Scp_map25.tif&quot;, overwrite=TRUE) # Plot the output susceptibility map library(&quot;RColorBrewer&quot;) plot(Scp_pred, xlab = &quot;East [m]&quot;, ylab = &quot;North [m]&quot;, main = &quot;Landslides susceptibility map&quot;, col = terrain.colors(5)) 6.5.1 Class intervals for decision maker What can you say by looking at the map? Actually a risk heat map is a data visualization tool for communicating the level for a specific risk to occur. These maps helps authorities to identify and prioritize the risks associated with a given hazard. Normally an authority (i.e., a decision maker) prioritize its efforts based on the available resources it has. So, it can be more useful to detect the areas with the highest probability of burning based given intervals (i.e., breaks). The authority can thus concentrate its resources for preventive actions on a given percentage (such as 5%, 10%, or 20%) of the area with the highest probability of burning, instead of concentrate on the areas with a “stochastic” output probability value of 0.8 (for example). Breaks chosen based on the summary statics: these values corresponds to 0-25%-50%-75%-100% of the p-value distribution. brk&lt;-c(0, 0.03, 0.14, 0.4, 1) plot(Scp_pred, xlab = &quot;East [m]&quot;, ylab = &quot;North [m]&quot;, main = &quot;Landslides susceptibility map&quot;, col = rev(c(&quot;brown&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;grey&quot;)), breaks=brk) Breaks chosen based on well-estabished percentile classes # Output predicted values are transformed to a vector pred.vect &lt;- as.vector(Scp_pred@data@values) # The function &quot;quantile&quot; is used to fix classes qtl.pred &lt;- quantile(pred.vect,probs=c(0.25,0.5,0.75,0.85,0.95), na.rm = TRUE) # and then extract the corresponding values qtl.int&lt;- c(0,0.03,0.14,0.42,0.6,0.82,1) plot(Scp_pred, xlab = &quot;East [m]&quot;, ylab = &quot;North [m]&quot;, main = &quot;Landslides susceptibility map&quot;, col = rev(c(&quot;brown&quot;, &quot;red&quot;, &quot;orange&quot;,&quot;yellow&quot;, &quot;green&quot;, &quot;grey&quot;)), breaks=qtl.int) 6.6 Variable importance plot Although machine learning algorithms are often considered as a black box, RF provides two metrics allowing to assess the importance of each variables in the model: the mean decrease in accuracy (MDA), and the mean decrease in Gini index. The higher values indicate the most important variables. In addition, it is possible to plot a sample tree (selected randomly) to analyse its structure. library(&quot;party&quot;) x &lt;- ctree(LS~., data=LS_input_train) plot(x, type=&quot;simple&quot;) # Display the plot with the relative importance of each variable importance(RF_LS) varImpPlot(RF_LS) 6.6.1 Partial dependence plot In addition, the partial dependent plot allows to estimate, for each single variable, the relative probability of prediction success over different ranges of values. It gives a graphical depiction of the marginal effect of each variable on the class probability over different ranges of continuous or discrete values. Positive values are associated with the probability of occurrence of the phenomena (i.e., landslides presence), while negative vales indicate its absence. # Slope partialPlot(RF_LS, LS_input_train, x.var = Slope, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;Slope [Â°]&quot;, main = &quot;&quot;, ylab = &quot;Partial dependence&quot;) # Elevation partialPlot(RF_LS, LS_input_train, x.var = DEM, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;Elevation [m]&quot;, main = &quot;&quot;,ylab = &quot;Partial dependence&quot;) # Profile curvature partialPlot(RF_LS, LS_input_train, x.var = profCurv, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;Profile curvature [1/m]&quot;, main = &quot;&quot;, ylab = &quot;Partial dependence&quot;, xlim = c(-0.04,0.04)) # Plan Curvature partialPlot(RF_LS, LS_input_train, x.var = planCurv, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;Plan curvature [1/m]&quot;, main = &quot;&quot;, ylab = &quot;Partial dependence&quot;, xlim = c(-0.04,0.04)) # Distance to roard partialPlot(RF_LS, LS_input_train, x.var = distRoad, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;Distance to road [m]&quot;, main = &quot;&quot;, ylab = &quot;Partial dependence&quot;) # Topographic wetness index partialPlot(RF_LS, LS_input_train, x.var = TWI, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;TWI [-]&quot;, main = &quot;&quot;, ylab = &quot;Partial dependence&quot;) # Geology partialPlot(RF_LS, LS_input_train, x.var = Geology, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;Geology&quot;, main = &quot;&quot;, ylab = &quot;Partial dependence&quot;) # Land Cover partialPlot(RF_LS, LS_input_train, x.var = LandCover, rug = TRUE, which.class = RF_LS$classes[2], xlab= &quot;Land Cover&quot;, main = &quot;&quot;, ylab = &quot;Partial dependence&quot;) 6.7 Conclusions and further analyses This exercise allowed you to familiarize with Random Forest, by the proposed application about landslides susceptibility mapping and variables importance assessment. To be sure that everything is perfectly clear for you, we propose you to answer the following questions and to discuss your answers with the other participants to the course or directly with the teacher. Why is it important to implement the pseudo-absence, other that the presences (i.e., the observations) in a data-driven modelization? What is the difference between a numerical and a categorical variable? Give some examples of both types. Which is the values of the OOB estimate error rate of your model? Which parameters you can change to try to reduce it? Be brave and do it (i.e., change the values for ntree and mtry, than analyse which values for the AUC you obtain and which model perform better. Which are the three most important variables of your model (based on the MDA)? What is the slope value (or range of values) that gives the highest probability of landslides occurrence? And for the geology, which are the most important classes? In a model seeking to estimate the vulnerability to wildfire, which is the dependent variable and which can be the independent variables? The reading of the paper @tonini_evolution_2017, which inspired this lab, can help you in in this task. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
