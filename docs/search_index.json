[["index.html", "Advanced Geospatial Data Analysis in R - Environmental applications Preface Book overview Authors information My Journey of Learning: Integrating Geospatial Data Analysis with Data Science Acknowledgements", " Advanced Geospatial Data Analysis in R - Environmental applications Dr. Marj Tonini, Haokun Liu 05/09/2024 Preface Earth surface environmental processes exhibit distinctive characteristics, encompassing both spatial and temporal dimensions, along with various attributes and predictive variables. Coupled with uncertainty and complexity issues, all this contribute to make this field of research highly challenging. Furthermore, in the era of Data Science, the wealth of available data and the rapid development of analytical models have emerged as distinctive aspects in the realm of Advanced Geospatial Data Analysis (AGDA). The domain of AGDA encompasses data exploration, manipulation, and modelling, from the acquisition phase up to the visualization and interpretation of the results. Mapping where events are located and how they relate to each other provides a better understanding of the process being studied. Finally, as an increasing volume of geo-environmental data becomes more and more accessible, the demand for spatial data scientists is growing rapidly, both in public research institutions as well as in private companies. The term “Data Science” was defined for the first time by Peter Naur in 1974 as “the science of dealing with data, once they have been established, while the relation of the data to what they represent is delegated to other fields and sciences” (Naur, Peter, 1974). In essence, this definition frames Data Science as the technical and methodological handling of data, while the initial creation or gathering of data, as well as the interpretation of its meaning in the broader context, is not the primary focus of this discipline. In 1977, the International Association for Statistical Computing was estabilished and further refined the concept of data science, emphasizing its role in the process of transforming raw data into valuable information and actionable knowledge (IASC). The IASC articulated that data science involves not only the collection and processing of data but also the application of statistical methods and computational techniques to extract meaningful insights, which can then be used to support decision-making and drive innovation across various domains​. In disciplines like Environmental and Earth Sciences, Physical Geography, Humanities and Social Sciences, the use of Data Science procedures is emerging only recently, proving to be extremely efficient to deal with the complexity of the investigated process and the heterogeneity of the underlying data sources (amato_spatiotemporal_2022?). This leads to a cultural shift, moving scientists away from individual working within their own research domain. Indeed, disciplinary boundaries are more and more permeable, pushing scientists to be more open to collaborate among them and with decision makers on the investigation and understanding of real-world problems. Modern earth and environmental scientists need to interact with other disciplines, apparently far from their domain. This openness is increasingly important as society struggles to respond to the implication of anthropogenic pressures on different issues, such as natural hazards and climate change, or the harmful impacts of human activities on biodiversity, water and air quality, human health. Book overview The primary focus of this eBook is the application of Data Science methodologies to analyze and understand Earth’s surface environmental processes. The scientific approaches in this emerging field (spanning statistics, mathematics, geomatics, and computer science) are often challenging to master. Thus, while maintaining a rigorous emphasis on mathematical and statistical formalism, this eBook prioritizes practical applications in the domain of Geosciences. Emphasis will be placed on the application of advanced geospatial tools to natural and anthropogenic hazards, and to land use processes. Theoretical concepts will be supported by case studies achieved using spatial clustering techniques and supervised and unsupervised machine learning algorithms (e.g., stochastic point process models for spatio-temporal cluster analysis; application of machine learning based approaches for geodemographic segmentation, susceptibility assessment and risk mapping). Computing lab Computations are conducted using the R free software environment, which is specifically designed for data analysis and manipulation. To complete the applied computing labs presented in each chapter, readers can download the datasets from here: [AGDA Book Gihub]. The computing labs will cover the following geocomputational techniques: Exploratory Data Analysis and Visualization: Examining geographical variations in statistical data distributions, including Geographically Weighted Summary Statistics. Cluster Detection and Mapping: Techniques such as Ripley’s K-function, kernel density estimation, and DBSCAN for identifying and visualizing clusters. Self-Organizing Maps: As example of unsupervised machine learning method for data clustering/segmentation and visualization. Random Forest: As example of supervised machine learning algorithm. We present both the global and the local version, and we use Random Forest to introduce interpretability &amp; explainability in machine learning. Target audience The target audience includes master’s and PhD students in Earth and Environmental Sciences, Biology and Spatial Ecology, Physical Geography, and related disciplines. Our goal is to empower students by guiding them through both theoretical knowledge and hands-on practical applications, helping them develop strong problem-solving skills. This eBook aims to equip the audience with: A solid understanding of key practical concepts and applied aspects in AGDA. Advanced tools to effectively navigate and analyze spatial datasets in geosciences. This book is designed for intermediate to advanced R users with experience in geospatial data and a keen interest in geocomputing. If you have only a basic knowledge of these areas, we strongly encourage you to explore the essential references provided in each chapter. These resources offer valuable documentation and additional materials to help deepen your understanding. The main prerequisites are the following. Knowledge of basic statistics: methods of descriptive statistics (measures of central tendency and dispersion); how to assess relationships between variables; concepts of correlation and regression. Basic knowledge in geomatics (GIS): basic operations with raster and vector datasets. R programming basics and RStudio. Authors information Marj Tonini is a spatial data scientist with a profound expertise in geospatial modeling for risk assessment, particularly in relation to wildfires and landslides. She earned her Ph.D. in 2002 from the Sant’Anna School of Advanced Studies in Pisa, Italy, where she developed an agro-environmental modeling thesis that laid the foundation for her career. In 2004, she joined the University of Lausanne as a postdoctoral researcher in geospatial data analysis, and by 2008, she was appointed as Senior Research Manager at the Institute of Earth Surface Dynamics, where she continues to serve in her current role. Marj’s research is centered on the development of innovative methodologies that facilitate the efficient extraction of knowledge from complex environmental datasets. Her work is driven by the goal of creating a robust methodological framework to understand the spatio-temporal dynamics of environmental processes and to evaluate the influence of various predictor variables. Her current research focuses on the analysis of land use and land cover changes, alongside the development of predictive scenarios and the assessment of susceptibility and risks associated with natural hazards. Through her work, she seeks to advance the field of geospatial science by translating data-driven insights into practical solutions for managing and mitigating environmental risks. Marj has written the vast majority of this book. Her contributions include the development of the conceptual framework, writing the theoretical background chapters, developing case studies, designing methodologies, and conducting data collection and analysis, software/tool development. the Sections “My Journey of Learning” refers to her personal experience. Haokun Liu is a Ph.D. student at the Group of Cities and Dynamics of Networks (CITADYNE Group) at the University of Lausanne. In addition to his doctoral studies, he serves as a student assistant at the Swiss Geocomputing Center (SGC). Benefiting from rigorous and comprehensive training in both China and Switzerland, Haokun has developed a diverse research portfolio. His primary research interests and experience encompass a wide range of interdisciplinary fields, including urban analytics, health geography, and spatial data science. His work focuses on leveraging advanced computational techniques to address complex urban and environmental challenges, bridging the gap between data-driven insights and real-world applications. Haokun’s contributions were instrumental in establishing and managing the GitHub repository used to host and maintain this eBook. He also played a key role in data visualization and mapping, and provided valuable revisions to the applied computing labs. My Journey of Learning: Integrating Geospatial Data Analysis with Data Science In the early 2000s, as I (Marj) embarked on my PhD journey, a new frontier was beginning to unfold in the world of environmental research. Geographical information systems (GIS) were just starting to make waves, offering a fresh perspective on how we could investigate and understand the environment. It was through my thesis on the effects of spreading olive vegetation water on agricultural land that I first delved into the world of GIS. Each tool and technique I learned opened new doors, revealing the vast potential of spatial data in environmental analysis and management. This early exposure was more than just a technical skill; it was a revelation. I began to see the world through the lens of spatial data, understanding how it could illuminate patterns and connections that were otherwise hidden. This experience not only shaped my research but ignited a passion that would drive me to explore the depths of spatial analysis for years to come. After completing my PhD, my journey led me to a PostDoc position where I had the extraordinary opportunity to work alongside Professor Mikhail Kanevski. Mikhail was a true pioneer, a visionary in the fields of environmental data mining, geostatistics, and the burgeoning field of applying machine learning to spatial environmental data. Being part of his research group was like stepping into a realm where the boundaries of what was possible were constantly being pushed and redefined. In this research team, I found myself diving headfirst into the world of spatial statistics, driven by the desire to uncover valuable insights from the complex data that described our environment. The work was challenging, yet it was also exhilarating, as every new technique I learned brought me closer to understanding the intricate tapestry of environmental phenomena. This experience was pivotal, setting the course for my future research and solidifying my belief in the power of combining advanced techniques with a deep understanding of spatial data. As the years passed, the tools and resources available to researchers like me grew exponentially. The development and widespread adoption of free programming languages like Python and R revolutionized the way we approached statistical analysis and machine learning. What was once the domain of a select few became accessible to many, thanks to the wealth of pre-developed scripts and resources these languages offered. For me, and for countless others, this accessibility was a game-changer. It meant that advanced techniques were no longer out of reach, and it fostered a culture of greater reproducibility in research. Using standardized, widely adopted tools like Python and R, researchers could now more easily share their code and methodologies, allowing others to replicate and validate findings. This collaborative spirit not only strengthened the reliability and credibility of scientific research but also created a community where knowledge could be shared and built upon more effectively. Reflecting on this journey, I realize that each step—from my early days with GIS to my time in Professor Kanevski’s group, to my current role as an independent lead researcher in environmental spatial data analysis in the era of data science—has been a chapter in a larger story of discovery and innovation. It’s a story of how spatial data and advanced analytics have become indispensable tools in our quest to understand and protect the environment, and it’s a story that continues to unfold with each new challenge and breakthrough. Acknowledgements The case studies presented in each chapter came from different projects carried out in collaboration with several colleagues, including master and PhD students. All the produced scientific papers are duly cited in the bibliography. I would like to express my gratitude to Professor Mário Gonzalez Pereira and Dr. Joana Parente, for their extensive and fruitful collaboration in investigating the spatio-temporal distribution of wildfires in Portugal. One of our studies, which explores the evolution of forest fires from spatio-temporal point events to smoothed density maps, forms an integral part of Chapters 3 and 4. I also thank Professor Stuart Lane and Dr. Natan Micheletti for introducing me to the fascinating world of rock glacier research. Notably, the 3D point cloud dataset analyzed in Chapter 8 was acquired and processed by Natan during his PhD studies. My thanks also go to Axelle Bersier for her meticulous work in acquiring and pre-processing the Swiss national population census dataset, which is used for the exercise on unsupervised learning in Chapter 5. The same for Julien Riese, who produced the input dataset and collaborated with me in developing the code that assesses landslide susceptibility in Canton Vaud, the main focus of Chapter 6. Both Julien and Axelle exemplify the high caliber of master’s students I have had the pleasure of supervising. For transparency, I acknowledge the use of ChatGPT in assisting with the reformulation of certain sentences in this book and DALL-E to generate the image icon. References Naur, Peter. (1974). Concise survey of computer methods. New York : Petrocelli Books. https://archive.org/details/concisesurveyofc0000naur "],["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 R Language 1.2 R Markdown 1.3 Data type in computational analysis", " Chapter 1 Introduction to R 1.1 R Language R is a complete programming language and software environment for statistical computing and graphical representation. As part of the GNU Project - free software, mass collaboration project - (https://www.gnu.org/software/software.en.html), the source code is free available. For more details on R see https://www.r-project.org/. 1.1.1 R Packages Functionalities in R can be expanded by importing packages. A package is a collection of R functions, data and compiled code. The location where the packages are stored is called the library. If there is a particular functionality that you require, you can download the package from the appropriate site and it will be stored in your library. In all operation systems the function install.packages() can be used to download and install a package automatically. Otherwise, a package already installed in R can be loaded in a session by using the command library(package_name). When you open an R Markdown document (.Rmd) the program propose you automatically to install the libraries listed there. 1.1.2 Some tips R is case sensitive! Previously used command can be recalled in the console by using the up arrow on the keyboard. The working directory by default is “C:/user/.../Documents”. It can be found using the command getwd() It can be changed using the command line setwd(\"C:/Your/own/path\") In R Markdown the working directory when evaluating R code chunks is the directory of the input document by default. To access to a specific file in a sub-folder use “. /subfolder/file.ext” To access to a specific file in a up-folder use “. . /upfolder/file.ext” 1.1.3 R Commands (online resources) Many table resuming the main R commands can be found online. Here some useful links: A short list of the most useful R commands Table of Useful R commands Basic Commands to Get Started with R 1.2 R Markdown This is an R Markdown document :-) Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is a simple and easy to use plain text language allowing to combine R code, results from data analysis (including plots and tables), and comments into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or web pages). Code lines are organized into code blocks, seeking to solve specified tasks, and referred to as “code chunk”. For more details on using R Markdown see http://rmarkdown.rstudio.com. All what you have to do during the computing labs is to read each explanatory paragraph before running each individual R code chunk, one by one, and to interpret the results. Finally, to create a personal document (usually a PDF) from R Markdown, you need to Knit the document. Knitting a document simply means taking all the text and code and creating a nicely formatted document. 1.3 Data type in computational analysis 1.3.1 Variables Variables are used to store values in a computer program. Values can be numbers (real and complex), words (string), matrices, and even tables. The fundamental or atomic data in R Programming can be: integer: number without decimals numeric: number with decimals (float or double depending on the precision) character: string, label factors: a label with a limited number of categories logical: true/false Figure 1.1: Data Types in R 1.3.2 Data structure in R R’s base data structures can be organised by their dimensionality (1d, 2d, or nd) and whether they are homogeneous (all contents must be of the same type) or heterogeneous (the contents can be of different types). This gives rise to the four data structures most often used in data analysis: Figure 1.2: Data structures in R A Vector is a one-dimensional structure winch can contain object of one type only: numerical (integer and double), character, and logical. # Investigate vector&#39;s types: v1 &lt;- c(0.5, 0.7); v1; typeof(v1) ## [1] 0.5 0.7 ## [1] &quot;double&quot; v2 &lt;-c(1:10); v2; typeof(v2) ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] &quot;integer&quot; v3 &lt;- c(TRUE, FALSE); v3; typeof(v3) ## [1] TRUE FALSE ## [1] &quot;logical&quot; v4 &lt;- c(&quot;Swiss&quot;, &quot;Itay&quot;, &quot;France&quot;, &quot;Germany&quot;); v4; typeof(v4) ## [1] &quot;Swiss&quot; &quot;Itay&quot; &quot;France&quot; &quot;Germany&quot; ## [1] &quot;character&quot; #Create a sequence from 0 to 5 with a step of 0.5: v5 &lt;- seq(1, 5, by=0.5); v5; typeof(v5) ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 ## [1] &quot;double&quot; length(v5) ## [1] 9 summary(v5) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 3 4 5 #Extract the third element of the vector v5[3] ## [1] 2 #Exclude the third element from the vector and save as new vector v5[-3] ## [1] 1.0 1.5 2.5 3.0 3.5 4.0 4.5 5.0 w5&lt;-v5[-3]; w5 ## [1] 1.0 1.5 2.5 3.0 3.5 4.0 4.5 5.0 A Matrix is a two-dimensional structure winch can contain object of one type only. The function matrix() can be used to construct matrices with specific dimensions. # Matrix of elements equal to &quot;zero&quot; and dimension 2x5 m1&lt;-matrix(0,2,5); m1 #(two rows by five columns) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 # Matrix of integer elements (1 to 12, 3x4) m2&lt;-matrix(1:12, 3,4); m2 ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 # Extract the second row m2[2, ] ## [1] 2 5 8 11 # Extract the third column m2[,3] ## [1] 7 8 9 # Extract the the second element of the third column m2[2,3] ## [1] 8 A data frame allows to collect heterogeneous data. All elements must have the same length. A list is a more flexible structure since it can contain variables of different types and lengths. Nevertheless, the preferred structure for statistical analyses and computation is the data frame. It is a good practice to explore the data frame before performing further computation on the data. This can be simply accomplished by using the commands str() to explore the structure of the data and summary() to display the summary statistics and quickly summarize the data. For numerical vectors the command hist() can be used to plot the basic histogram of the given values. # Create the vectors with the variables cities &lt;- c(&quot;Berlin&quot;, &quot;New York&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;) area &lt;- c(892, 1214, 105, 2188) population &lt;- c(3.4, 8.1, 2.1, 12.9) continent &lt;- c(&quot;Europe&quot;, &quot;Norh America&quot;, &quot;Europe&quot;, &quot;Asia&quot;) # Concatenate the vectors into a new data frame df1 &lt;- data.frame(cities, area, population, continent) df1 ## cities area population continent ## 1 Berlin 892 3.4 Europe ## 2 New York 1214 8.1 Norh America ## 3 Paris 105 2.1 Europe ## 4 Tokyo 2188 12.9 Asia #Add a column (e.g., language spoken) using the command &quot;cbind&quot; df2 &lt;- cbind (df1, &quot;Language&quot; = c (&quot;German&quot;, &quot;English&quot;, &quot;Freanch&quot;, &quot;Japanese&quot;)) df2 ## cities area population continent Language ## 1 Berlin 892 3.4 Europe German ## 2 New York 1214 8.1 Norh America English ## 3 Paris 105 2.1 Europe Freanch ## 4 Tokyo 2188 12.9 Asia Japanese #Explore the data frame str(df2) # see the structure ## &#39;data.frame&#39;: 4 obs. of 5 variables: ## $ cities : chr &quot;Berlin&quot; &quot;New York&quot; &quot;Paris&quot; &quot;Tokyo&quot; ## $ area : num 892 1214 105 2188 ## $ population: num 3.4 8.1 2.1 12.9 ## $ continent : chr &quot;Europe&quot; &quot;Norh America&quot; &quot;Europe&quot; &quot;Asia&quot; ## $ Language : chr &quot;German&quot; &quot;English&quot; &quot;Freanch&quot; &quot;Japanese&quot; summary(df2) # compute basic statistics ## cities area population continent ## Length:4 Min. : 105.0 Min. : 2.100 Length:4 ## Class :character 1st Qu.: 695.2 1st Qu.: 3.075 Class :character ## Mode :character Median :1053.0 Median : 5.750 Mode :character ## Mean :1099.8 Mean : 6.625 ## 3rd Qu.:1457.5 3rd Qu.: 9.300 ## Max. :2188.0 Max. :12.900 ## Language ## Length:4 ## Class :character ## Mode :character ## ## ## # Use the symbol &quot;$&quot; to address a particular column pop&lt;-(df2$population) pop ## [1] 3.4 8.1 2.1 12.9 hist(pop) # plot the histogram "],["basic-operation-with-geodata-in-r.html", "Chapter 2 Basic operation with geodata in R 2.1 Introduction 2.2 Plotting vector dataset 2.3 Plotting raster dataset 2.4 Geodata manipulation", " Chapter 2 Basic operation with geodata in R 2.1 Introduction Geodata, or geospatial data, refers to features associated with a specific location on the Earth’s surface. Geodata can be in various forms and is often used in Geographic Information Systems (GIS) for mapping and analysis. The two key components characterizing geodata are spatial identifier and attributes. Spatial identifier specifies the location and shape of the features, with different levels of detail, while attributes describe their characteristics. The two basic formats used to represent the spatial component of geodata are: Vector: this format uses points, lines, and polygons to identify each individual features. Raster: this format uses a regular grid of pixels to represent the global geographic context. In addition, attribute tables are used to store the characteristics associated with the geospatial features. The GIS software are specifically designed to help user to edit, manage, analyze, and map geodata. To make working with geodata easier, several packages have been developed in R. These packages allow users to handle geodata directly in R computing environment, without needing separate GIS software. In this chapter, we introduce the basic functions allowing to work with geodata in the pre-processing and post-processing phase of a pipeline in geomodeling. 2.2 Plotting vector dataset The spatial component of geodata uses geometric primitives like point, line, and polygon to represent the single features. Each feature in a geodataset is associated with various attributes providing detailed quantitative and qualitative information. A single geodataset includes features of the same type, represented by using the same class of primitive. The three basic geometric vector primitives have the following characteristics: Points: defined by a single pair of coordinates (x, y) representing a specific location. Used to represent small objects like weather stations, city center, or to identify single features in a geohazard inventory (e.g., earthquake’s epicenter, landslides location, wildfires, etc.). Lines: defined by pairs of coordinates connected to each other, representing linear features such as roads or rivers network, pathways, railway, etc. Polygons: defined by a series of connected coordinates that enclose an area, representing features such as lakes, administrative units, vegetation patches, burned area, landslides footprint. 2.2.1 Package sf and vector dataset The package sf (Pebesma 2022) has been designed to work with vector data as “simple features” in R. Each feature is represented by one row in the data frame, with attributes stored as columns and spatial information stored in a special geometry column. As a toy example, we will work with the geodataset of administrative boundaries in the Canton of Vaud (Switzerland). This dataset is available in shapefile format, one of the most widely used file formats for vector geospatial data. A shapefile includes multiple files allowing to store different kind of object: *shp: the features’ geometries (i.e., the geometric vector primitive used to describe the features). *dbf: the attributes, describing the characteristics of the features (i.e., tabular information). *shx: shape index format, an index file for the geometry data. *prj: the coordinate reference systems, defining how the geometries are projected on the Earth’s surface. To read a shapefile, you only need to specify the file name with “.shp” extension. However, it is important to have all related files in the same directory. Having all these files ensures that the shapefile is read correctly and all necessary information is available for the analysis. Shapefiles can be imported and converted as sf-objects using the command st_read(). By setting the argument quiet = FALSE suppresses the output from the console when importing geodata. # Load dataset vaud &lt;- st_read(&quot;data/RGIS/Canton_de_Vaud.shp&quot;, quiet = FALSE) ## Reading layer `Canton_de_Vaud&#39; from data source ## `/Users/hliu5/Documents/AGDAbook/data/RGIS/Canton_de_Vaud.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 315 features and 4 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 2494306 ymin: 1115149 xmax: 2585462 ymax: 1203493 ## Projected CRS: CH1903+ / LV95 2.2.2 Plot vector features Basic maps can be created in sf with the command plot(). By default this creates a multi-panel plot: one plot for each variable included in the geodata (corresponding to each column). Othewise, this command have to be followed by the name variable that you wish to display. # Inspect the attribute table str(vaud) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 315 obs. of 5 variables: ## $ COMMUNE : chr &quot;Payerne&quot; &quot;Rossinière&quot; &quot;Vallorbe&quot; &quot;Puidoux&quot; ... ## $ NUMERO_COM: num 5822 5842 5764 5607 5745 ... ## $ Shape_Leng: num 30547 21405 27147 31111 28268 ... ## $ Shape_Area: num 24178457 23354685 23190781 22858943 22501915 ... ## $ geometry :sfc_POLYGON of length 315; first list element: List of 1 ## ..$ : num [1:783, 1:2] 2559434 2559258 2559286 2559263 2559337 ... ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;XY&quot; &quot;POLYGON&quot; &quot;sfg&quot; ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;COMMUNE&quot; &quot;NUMERO_COM&quot; &quot;Shape_Leng&quot; &quot;Shape_Area&quot; # Basic plot (diplay all the variables) plot(vaud) # Display each municipality using single color (attributed randomly) plot(vaud[&quot;COMMUNE&quot;]) A legend with a continuous color scale is produced by default if the object to be plotted is numeric. # Plot based on the value &quot;area&quot; plot(vaud[&quot;Shape_Area&quot;], reset = FALSE) Different operations can be performed to customize the map. For instance, we can higlight the municipality of Lausanne in red to show its geographic correspondence, keeping all the other municipalities grey. # Extract the Lausanne boundary Vaud_Lausanne = vaud[vaud$COMMUNE == &quot;Lausanne&quot;, ] # Union and merge the geometry Lausanne = st_union(Vaud_Lausanne) # plot the Lausanne municipality over a map of Canton of Vaud plot(vaud[&quot;COMMUNE&quot;],col=&quot;grey&quot;, reset = FALSE) plot(Lausanne, add = TRUE, col = &quot;red&quot;) 2.3 Plotting raster dataset Raster data are different from vector data in that they are referenced to a regular grid of regular (usually square) cells, called pixels. The spatial characteristics of a raster dataset are defined by its spatial resolution (the height and width of each cell) and its origin (typically the upper left corner of the raster grid, which is associated with a location in a coordinate reference system). Raster data is highly effective for modeling and visualizing continuous spatial phenomena such as elevation, temperature, and precipitation. Each cell in the grid captures a value representing the attribute at that specific location, allowing for smooth and detailed gradients across the study area. This format is also effective in representing categorical variables such as land cover, where each cell is associated with a class value. Common raster formats used for spatial analses include: GeoTIFF (.tif, .tiff): A widely used format that includes geographic metadata such as coordinates and projection information, making it easy to integrate with GIS applications. ESRI Grid: A proprietary format developed by ESRI for use with its software, such as ArcGIS. It supports both integer and floating-point grids. Erdas Imagine (.img): A format developed by ERDAS for its Imagine software, often used for remote sensing data and satellite imagery. It supports large files and multiple bands. NetCDF (.nc): Stands for Network Common Data Form, used for array-oriented scientific data, including GIS data. It supports multidimensional data arrays, making it suitable for complex environmental and atmospheric data. HDF (Hierarchical Data Format): Similar to NetCDF, HDF is used for managing and storing large amounts of data, especially in scientific computing. It supports various data types and is used for satellite imagery and climate data. ASCII Grid (.asc): A simple, text-based raster format where each cell value is represented by a number in a grid layout. It’s easy to read and edit with a text editor. These formats vary in terms of compression, metadata support, and suitability for different types of raster data, from simple images to complex scientific datasets. 2.3.1 Terra package and raster dataset The package terra provides a variety of specialized classes and functions for importing, processing, analyzing, and visualizing raster datasets (Hijmans 2022). It is intended to replace the raster package, which has similar data objects and the function syntax as terra package. However, the terra package contains several major improvements, including faster processing speed for large raster. # Load the libraries library(terra) library(dplyr) library(ggplot2) # Load the libraries library(terra) # Load the raster data Vaud_dem &lt;- rast(&quot;data/RGIS/DEM.tif&quot;) # Inspect the raster Vaud_dem ## class : SpatRaster ## dimensions : 3524, 3647, 1 (nrow, ncol, nlyr) ## resolution : 25, 25 (x, y) ## extent : 494300, 585475, 115150, 203250 (xmin, xmax, ymin, ymax) ## coord. ref. : CH1903 / LV03 (EPSG:21781) ## source : DEM.tif ## name : DEM ## min value : 372.0 ## max value : 3200.1 2.3.2 Plot raster features Raster objects can be imported using the function rast() and exported using writeRaster(), specifing the format argument. As a toy example, we will work with the raster *.tif representing the digital elevation model (DEM) of Canton Vaud. Similar to the sf package for ploting vector data, terra also provides plot() methods for its own classes. # Plot raster dataset plot(Vaud_dem, main=&quot;DEM&quot;) 2.4 Geodata manipulation In this section, we explore some basic manipulations with vector and raster geodata. This will be useful in the following chapters of this book. 2.4.1 Manipulate tabular datasets In a vector dataset, the characteristics associated with geospatial features are stored in attribute tables. Each feature in the vector dataset is linked to a row in the table, with its characteristics organized into columns. Columns, also known as fields, store the various attributes associated with the features. Thematic attributes stored in separate tabular dataset (such as census data, environmental monitoring stations, public health, or traffic data) can be imported and added to the attribute table of a given vector datased if the two dataset are referred to the same spatial feature. While tabular dataset can be delivered in different format (such as *.dbf, *.xlsx, *.txt), the most widely used format is *.csv (comma-separated values). As for other tabular format, *.csv is structured into rows and columns, where each column is separated by a comma. The first row often contains the column headers (field names), which describe the attribute in each column. The main advantages on this format compared with other formats are: Simplicity: easy to create, read, and edit. Lightweight: files are typically small and easy to transfer. Compatibility: supported by most data processing tools and software. In most cases, data need to be reworked before they can be visualized and analyzed in R. Common tasks include: selecting subsets of rows or columns from the attribute table; rename a field; calculate new variables from the raw data values; compute summary statistics; combine data from different sources. We explore some operations in the following chunks. # Load tabular dataset Swisscensus_2020 &lt;- read.csv(&quot;data/RGIS/census2020.csv&quot;) # Inspect the element str(Swisscensus_2020) ## &#39;data.frame&#39;: 2145 obs. of 35 variables: ## $ ID_0 : int 1 2 3 4 5 6 7 8 9 10 ... ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ p_infrastructure : num 0.131 0.302 0.178 0.133 0.199 ... ## $ p_forested : num 0.307 0.28 0.287 0.291 0.332 ... ## $ p_agriculture : num 0.502 0.401 0.526 0.554 0.464 ... ## $ p_improductible : num 0.05815 0.01133 0.01077 0.025 0.00766 ... ## $ surface_polygone : num 791 1059 743 1360 653 ... ## $ natural_growth_1000 : num -1.001 2.115 4.65 2.119 0.528 ... ## $ density : num 255 1160 755 279 581 ... ## $ dependency_ratio : num 60.1 63.4 68.6 67.4 62.9 ... ## $ migration_intern : num 11.014 -6.1 -0.537 9.534 -2.905 ... ## $ migration : num 3.5 5.61 1.07 1.32 5.02 ... ## $ p_employment : num 0.105 0.415 0.109 0.14 0.364 ... ## $ primary_sector : num 0.19388 0.06495 0 2.12 0.00187 ... ## $ secondary_sector : num 0.148 1.724 0 2.8 0.045 ... ## $ tertiary_sector : num 1.9898 7.6414 0.3155 16.1 0.0569 ... ## $ p_social_assistance : num 1.31 3.19 1.13 1.25 1.27 ... ## $ p_new_buildings : num 1.514 0.732 0.179 0.8 0.529 ... ## $ p_new_housings : num 5.048 4.552 0.179 1.333 1.059 ... ## $ p_cinema : num 0 0.0195 0 0 0 ... ## $ p_museum : num 0.000993 0 0 0 0.000264 ... ## $ p_culture_institution: num 0.01142 0.00667 0.00588 0.00816 0.00606 ... ## $ size_households : num 2.28 2.19 2.37 2.39 2.38 ... ## $ p_new_entreprise : num 0.00497 0.00407 0.00267 0.00421 0.00369 ... ## $ p_weddings : num 2.5 4.8 4.65 2.65 1.85 ... ## $ p_foreigners : num 14.4 29 17.3 15.8 17.4 ... ## $ p_individual_houses : num 69.2 54.7 71.6 69.1 74.7 ... ## $ Population : num 2014 12289 5610 3801 3795 ... ## $ p_pop_19 : num 19.8 22.3 21.9 22.1 19.3 ... ## $ p_pop_65 : num 18.5 18.6 14.1 21.5 20.4 ... ## $ lat : num 0.571 0.562 0.566 0.582 0.562 ... ## $ long : num 0.735 0.738 0.756 0.719 0.748 ... ## $ zab_2022 : num 69.6 285.9 92.4 126.6 102.8 ... ## $ net_income_h : num 57603 36521 45237 45791 48875 ... ## $ p_transport : num 5.06 7.69 5.11 4.55 4.89 ... # Create a subset (including Land Use information) Swisscensus2020_LU = subset(Swisscensus_2020, select = c(2:6)) str(Swisscensus2020_LU) ## &#39;data.frame&#39;: 2145 obs. of 5 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ p_infrastructure: num 0.131 0.302 0.178 0.133 0.199 ... ## $ p_forested : num 0.307 0.28 0.287 0.291 0.332 ... ## $ p_agriculture : num 0.502 0.401 0.526 0.554 0.464 ... ## $ p_improductible : num 0.05815 0.01133 0.01077 0.025 0.00766 ... # Rename a column for better understanding CH2020_LU &lt;- rename(Swisscensus2020_LU, Urban=p_infrastructure) head(CH2020_LU) ## ID Urban p_forested p_agriculture p_improductible ## 1 1 0.13147914 0.3072061 0.5018963 0.058154235 ## 2 2 0.30217186 0.2804533 0.4013220 0.011331445 ## 3 3 0.17765814 0.2866756 0.5262450 0.010767160 ## 4 4 0.13308823 0.2911765 0.5536765 0.025000000 ## 5 5 0.19908116 0.3323124 0.4640123 0.007656968 ## 6 6 0.08712121 0.2058081 0.6830808 0.011363636 2.4.2 Pipes: chaining of multiple operations Pipes allow for the chaining of multiple operations in a unique sequence, which makes the code easier to understand and reduces the need for nested function calls. The use of pipes in R, primarily facilitated by the magrittr package and now natively supported in base R (R version 4.1.0 and above), is a powerful way to write clear and readable code. The function pipe is represented by the symbol %&gt;%. When a pipe is placed on the right side of an object or function, the output from the function is passed as the first argument to the next function after the pipe. Below is a simple example of using the pipe operator with the function select(), used to select the the fields related to the land use in the Swiss census dataset corresponding to a population density less than 100. LU_dens100 &lt;- Swisscensus_2020 %&gt;% filter(density &lt; 100) %&gt;% select(2:6) 2.4.3 Join table A tabular dataset can be joined to the attribute table of a vector dataset by specifying the name of the columns in the two tables used for merging. To this end, we can use the function merge() included in sp package. Vaud_census_2020 &lt;- merge(x=vaud, y=Swisscensus_2020, by.x=&quot;NUMERO_COM&quot;, by.y=&quot;ID&quot;, all.x=TRUE) # Plot based on a joined attribute plot(Vaud_census_2020[&quot;p_agriculture&quot;]) 2.4.4 Mapping with ggplot2 Mapping in R can be efficiently achieved using the powerful visualization package ggplot2, especially when combined with additional packages like sf. These tools enable users to create detailed and customized maps for spatial data analysis and visualization. In particular sf facilitates the handling of complex spatial data structures, making it possible to create intricate and informative maps. In the following example we crate an aesthetic map of Canton Vaud based on the percentage of agricultural land use by municipality. Aesthetic mappings describe how the attributes of the geodata are mapped to the visual properties (aesthetics) of the plot. The command aesthetics - aes() - control the appearance of the plot elements, such as points, lines, bars, and so on. # Install viridis color scale as a package install.packages(&quot;viridis&quot;, repos=&quot;http://cran.us.r-project.org&quot;) ## ## The downloaded binary packages are in ## /var/folders/hf/q_2qq0tn1dngf4r3nzfhj3xh0000gn/T//Rtmp98d3jw/downloaded_packages library(viridis) # Use ggplot2 for mapping ggplot(Vaud_census_2020) + geom_sf(aes(fill = p_agriculture)) + scale_fill_viridis_c(option = &quot;viridis&quot;, name = &quot;Agri (%)&quot;) + theme_minimal() + labs(title = &quot;Agrucultural land use&quot;, subtitle = &quot;Canton vaud - census 2020&quot;) 2.4.5 Cropping ratser Many projects in environmental science require integrating data from various sources, such as remote sensing images (rasters) and administrative boundaries (vectors). In these scenarios, raster cropping and masking are essential for standardizing the spatial extent of the input data. These operations help to minimize memory usage and computational resources needed for subsequent analysis and are often a crucial pre-processing step before generating detailed and visually appealing maps that incorporate raster data. First, verify if the coordinate reference systems (CRS) of the two input geodatasets are the same. To do this, simply type the names of the geodatasets and check the “coord. ref.” attribute. For a detailed description of the coordinate reference system, use the crs() function. If necessary, reproject one of the spatial layers using the project() function, specifying the CRS of the other dataset to ensure their extents perfectly overlap. # verify CRS Vaud_dem ## class : SpatRaster ## dimensions : 3524, 3647, 1 (nrow, ncol, nlyr) ## resolution : 25, 25 (x, y) ## extent : 494300, 585475, 115150, 203250 (xmin, xmax, ymin, ymax) ## coord. ref. : CH1903 / LV03 (EPSG:21781) ## source : DEM.tif ## name : DEM ## min value : 372.0 ## max value : 3200.1 vaud ## Simple feature collection with 315 features and 4 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 2494306 ymin: 1115149 xmax: 2585462 ymax: 1203493 ## Projected CRS: CH1903+ / LV95 ## First 10 features: ## COMMUNE NUMERO_COM Shape_Leng Shape_Area ## 1 Payerne 5822 30546.68 24178457 ## 2 Rossinière 5842 21404.88 23354685 ## 3 Vallorbe 5764 27146.69 23190781 ## 4 Puidoux 5607 31110.58 22858943 ## 5 Baulmes 5745 28268.14 22501915 ## 6 Corbeyrier 5404 26122.21 22002176 ## 7 Vully-les-Lacs 5464 33182.64 20915807 ## 8 Bassins 5703 29224.24 20772714 ## 9 Mont-la-Ville 5491 20254.18 19791177 ## 10 Avenches 5451 30004.81 19487486 ## geometry ## 1 POLYGON ((2559434 1183900, ... ## 2 POLYGON ((2568027 1143632, ... ## 3 POLYGON ((2519472 1176171, ... ## 4 POLYGON ((2551941 1152936, ... ## 5 POLYGON ((2524665 1180813, ... ## 6 POLYGON ((2565278 1132847, ... ## 7 POLYGON ((2565363 1195281, ... ## 8 POLYGON ((2508540 1146203, ... ## 9 POLYGON ((2519784 1169416, ... ## 10 POLYGON ((2570746 1194939, ... # Project the DEM data using the CRS of the shape demVD_prj = project(Vaud_dem, crs(vaud)) # Verify the difference plot(Vaud_dem) plot(demVD_prj) # Extract the municipality of Lausanne Lausanne &lt;- filter(vaud, COMMUNE == &quot;Lausanne&quot;) # Crop the DEM to the extent of Lausanne (bounding box) DEM_Lausanne_cropped &lt;- crop(demVD_prj, Lausanne) # Mask the cropped DEM using the Lausanne polygon to get the exact shape DEM_Lausanne &lt;- mask(DEM_Lausanne_cropped, Lausanne) # Plot the DEM plot(DEM_Lausanne, main = &quot;DEM of Lausanne&quot;) # Add the Lausanne polygon outline plot(st_geometry(Lausanne), add = TRUE, border = &quot;red&quot;, lwd = 2) 2.4.6 Display categorical variables for a raster Categorical variables, also known as qualitative variables, are variables that represent distinct groups or categories. These variables are typically non-numeric and can be divided into a finite number of categories or levels. Categorical variables are often used to represent characteristics or attributes that do not have a natural ordering. Land use classes of a raster dataset are a common example of categorical variables. Each class represents a distinct type of land use labeled with names or, mostly, numeric codes. To understand the characteristics of the categorical variables, you can plot the “Land Cover” raster dataset by using its original classes. To visualize the data you need to perform few data manipulations before. library(RColorBrewer) # Convert the raster to a factor to represent categorical data # Load the raster data landCover &lt;- rast(&quot;data/RGIS/landCover.tif&quot;) # Inspect the raster landCover ## class : SpatRaster ## dimensions : 3524, 3647, 1 (nrow, ncol, nlyr) ## resolution : 25, 25 (x, y) ## extent : 494300, 585475, 115150, 203250 (xmin, xmax, ymin, ymax) ## coord. ref. : CH1903 / LV03 (EPSG:21781) ## source : landCover.tif ## name : landCover ## min value : 11 ## max value : 62 # Plot the raster (using codes) plot (landCover) # Convert the raster to a factor to represent categorical data landCover &lt;- as.factor(rast(&quot;data/RGIS/landCover.tif&quot;)) # Inspect the current levels (classes) of the raster current_levels &lt;- levels(landCover)[[1]] print(current_levels) ## ID landCover ## 1 11 11 ## 2 15 15 ## 3 21 21 ## 4 31 31 ## 5 41 41 ## 6 51 51 ## 7 62 62 # Define a vector of new class names (descriptions) new_class_names &lt;- c( &quot;11&quot; = &quot;Impermeable man-made&quot;, &quot;15&quot; = &quot;Permeable man-made&quot;, &quot;21&quot; = &quot;Herbaceous vegetation&quot;, &quot;31&quot; = &quot;Shrub vegetation&quot;, &quot;41&quot; = &quot;Forest&quot;, &quot;51&quot; = &quot;No vegetation&quot;, &quot;62&quot; = &quot;Glacier and water body&quot;) # Ensure that new class names match the existing levels current_levels$landCover &lt;- new_class_names[as.character(current_levels$ID)] # Apply the new class names to the levels of the raster levels(landCover) &lt;- current_levels # Verify the new levels print(levels(landCover)) ## [[1]] ## ID landCover ## 1 11 Impermeable man-made ## 2 15 Permeable man-made ## 3 21 Herbaceous vegetation ## 4 31 Shrub vegetation ## 5 41 Forest ## 6 51 No vegetation ## 7 62 Glacier and water body # Create a color palette with enough colors for all classes num_classes &lt;- length(new_class_names) color_palette &lt;- colorRampPalette(brewer.pal(8, &quot;Set3&quot;))(num_classes) # Plot the land cover data with the custom color palette plot(landCover, col = color_palette, main = &quot;Land Cover Classes&quot;) "],["geographically-weighted-summary-statistics-in-geosciences.html", "Chapter 3 Geographically Weighted Summary Statistics in Geosciences 3.1 GWSS for fire management 3.2 Computing lab: GWSS 3.3 Conclusions and further analyses", " Chapter 3 Geographically Weighted Summary Statistics in Geosciences Geographically Weighted Summary Statistics (GWSS) represent an advanced analytical approach in geoscience, allowing researchers to explore spatial variations in data across a geographical landscape. Unlike traditional summary statistics that provide a single, overall summary measure (like mean, median, or standard deviation) for an entire dataset, GWSS techniques calculate these measures locally, reflecting the unique characteristics and variations at different spatial locations. This method is particularly valuable in geoscience, where spatial heterogeneity often plays a critical role. 3.1 GWSS for fire management In fire risk assessment, it is essential to identify areas where fires occur most frequently and differentiate between small and large fires. This information is critical for understanding ignition factors and developing strategies to reduce forest fires, manage ignition sources, and identify high-risk areas. Although spatio-temporal inventories of forest fires are available, extracting meaningful insights about their distribution patterns remains challenging when relying solely on the examination of mapped burnt areas. To address this, Geographically Weighted Summary Statistics (GWSS) can be utilized. GWSS assumes that burned areas exhibit geographic trends and calculates local statistics to reveal these patterns more clearly. This approach provides valuable insights for effective fire management and prevention strategies. We compute here the GW local means, the GW local standard deviation and the GW localized skewness of burned areas in continental Portugal, registered in the period 1990-2013. This application is inspired by the work of (Tonini et al., 2017) 3.1.1 The overall methodology Summary statistics include a number of measures that can be used to summarize a set of observations, the most important of which are measures of central tendency (arithmetic mean, median and mode) and measures of dispersion around the mean (variance and standard deviation). In addition, measures of skewness and kurtosis are descriptors of the shape of the probability distribution function, the former indicating the asymmetry and the latter the peakedness/tailedness of the curve. For geoenvironmental processes, these global statistical descriptors may vary from one region to another, as their values may be affected by local environmental and socio-economic factors. In this case, an appropriately localized calibration can provide a better description of the observed values. One way to achieve this goal is to weight the above statistical measures for a given quantitative variable based on their geographical location. We introduce here the method proposed by (Brunsdon et al., 2002) and implemented in the function GWSS presented in the R package GWmodel (Brunsdon, 2019; Lu et al., 2014). The evaluation of geographically weighted summary statistics is obtained by computing a summary for a small area around each geolocalized punctual observation, by using the kernel density estimation technique (KDE) (Brunsdon, 1995). KDE is estimated at each point, taking into account the influence of the points falling within an area, with increasing weight towards the center, corresponding to the point location. A surface summary statistic is thus obtained. 3.1.2 Forest fires dataset Forest fires inventories indicating the location, the starting date and other related variables, such as the cause of ignition and the size of the area burned, are broadly available with a different degree of accuracy in different countries. In the present study, we consider the Portuguese National Mapping Burnt Areas dataset , freely available from the website of the Institute for the Conservation of Nature and Forests, ICNF (https://www.icnf.pt/florestas/gfr/gfrgestaoinformacao/dfciinformacaocartografica). This is a long spatio-temporal dataset (from 1975) resulting from the processing of satellite images acquired once a year at the end of the summer season. Row data consist of records of observed fire scars. The burned areas were estimated by using image classification techniques, then compared with ground data to resolve the discrepancies. Polygons have been converted into point shapefile, where each point represent the centroid of the burned areas, while the size of the burned areas and the starting date of the fires events are given as attributes. In this work, for consistency reasons, we consider only fires occurred between 1990 and 2013 and with a burned area above 5 hectares. . Figure 3.1: Total annual number of forest fire events, expressed in thousands of square metres 3.2 Computing lab: GWSS 3.2.1 Load the libraries First you have to load the following libraries: splancs: for display and analysis of spatial point pattern data GWmodel: techniques from a particular branch of spatial statistics, termed geographically-weighted (GW) models sf: support for simple features, a standardized way to encode spatial vector data ggplot2: a system for ‘declaratively’ creating graphics sp: classes and methods for spatial data library(splancs) library(GWmodel) library(sf) library(ggplot2) library(sp) (.packages()) ## [1] &quot;ggplot2&quot; &quot;sf&quot; &quot;GWmodel&quot; &quot;Rcpp&quot; &quot;robustbase&quot; ## [6] &quot;splancs&quot; &quot;sp&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [11] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 3.2.2 Import the forest fire dataset In this section you will load the geodata representing the forest fires inventories for events occurred in the continental Portuguese area in the period 1990-2013. You will also load the boundaries of the study area. You will start by exploring the datasets using mainly visual tools (plotting and histogram). # Import Portugal boundary Portugal &lt;- st_read(&quot;data/GWSS/Area_Portugal.shp&quot;) # Import the Portuguese forest fires dataset for the entire Portuguese area. FFPorto&lt;-st_read(&quot;data/GWSS/FF_Portugal.shp&quot;) You can explore the dataset by using different tools for exploratory data analyses. You will start by visualizing the databases. In the GIS environment, this correspond to the attribute table of a vector punctual feature. Than you can plot the histogram of events distribution based on the variable “Area_ha” (the size in hectares of the burned area). Since this is a power low distribution, for a better understanding it is recommended to transform the data using a logarithmic scale. Using Log10 you can easily evaluate the frequency distribution of the burned areas. # Show the attribute table (first 10 rows) FFPorto # Open the attribute table in a new tab View(FFPorto) # Summary statistics of all of the attributes associated with this dataset summary(FFPorto$Area_ha) # Display the histogram of burned area distribution hist(FFPorto$Area_ha) hist(log10(FFPorto$Area_ha)) 3.2.3 Forest fires spatial distribution For a better understanding of the phenomenon, you can group the events according to the size of the burned area. Based on the frequency distribution of the burned areas, the following three classes can be defined: Small fires: less than 15 ha Medium fires: between 15 ha and 100 ha Large fires: bigger than 100 ha Plotting the forest fires events using different colors, based on the size of the burned areas, can simplify the understanding of their pattern distribution, knowing that fires of different size have normally different drivers. 3.2.4 Compute the geographically weighted statistics From the exploratory data analysis performed above, it seems that a simple plotting of the forest fires events based on their spatial distribution, even if classified based on their size, can not really help to understand their behaviors. This is because we face to a huge number of events and the variable that we are using to characterize them (i.e., the size of the burned area) is very heterogeneous. To this aim, we can compute basic and robust GWSS and plot the data accordingly. GWSS includes geographically weighted means, standard deviations and the skweness. As you can see from the R Documentation - command: help(gwss) - same data manipulations are necessary to transform the forest fires dataset into a compatible data frame format. # Transform inputs data into a spatial points data frame FFdf&lt;-data.frame(X=FFPorto$X, Y=FFPorto$Y, Area=FFPorto$Area_ha) FFspdf&lt;-SpatialPointsDataFrame(FFdf[,1:2], FFdf) str(FFspdf) GWSS parameters: We summarize the data based on the size of the burned area (vars). We use here an adaptive kernel where the bandwidth (bw) corresponds to the number (100 in this case) of nearest neighbors (i.e. adaptive distance). We keep the default values for the other parameters. # Run gwss: this operation can take several minutes...be patient! # While waiting, you can look at gwss R Documentation: help(gwss) FFgwss &lt;- gwss(FFspdf,vars=(&quot;Area&quot;),adaptive=TRUE, bw=100) 3.2.5 Look at the results The resulting object (FFgwss) has a number of components. The most important one is the spatial data frame containing the results of local summary statistics for each data point location, stored in FFgwss$SDF (that is a spatial data frame). # display the first 6 rows head(FFgwss$SDF) # Inspect the resulting object FFgwss 3.2.6 GWSS maps To produce a map of the local geographically weighted summary statistic of your choice, firstly we need to enter a small R function definition. This is just a short R program to draw a map: you can think of it as a command that tells R how to draw a map (see Geographically Weighted Summary Statistics in Geosciences (https://rpubs.com/chrisbrunsdon/99667) for more details). The advantage of defining a function is that the entire map can now be drawn using a single command for each variable, rather than having to repeat those steps each time. To define the intervals for the classification, we use Jenks natural breaks classification method (style=\"fisher\"). library(RColorBrewer) #a useful tool for designing map color palettes. library(classInt) #to define class intervals # The function definition to draw the map: quick.map &lt;- function(spdf,var,legend.title,main.title) { x &lt;- spdf@data[,var] int &lt;- classIntervals(x, n=5, style=&quot;fisher&quot;) cut.vals&lt;-int$brks x.cut &lt;- cut(x,cut.vals) cut.levels &lt;- levels(x.cut) cut.band &lt;- match(x.cut,cut.levels) colors &lt;- rev(brewer.pal(length(cut.levels), &quot;RdYlGn&quot;)) par(mar=c(1,1,1,1)) plot(Portugal$geometry,col=&#39;lightgrey&#39;) title(main.title) plot(spdf,add=TRUE,col=colors[cut.band],pch=16, cex=0.5) legend(&quot;bottomright&quot;,cut.levels,col=colors,pch=16,bty=&quot;n&quot;,title=legend.title) } # Call the function to display the maps of the Local Mean (LM), Local Standard Deviation(LS), and Local Skweness (LSKe) par(mfrow=c(1,3)) quick.map(FFgwss$SDF, &quot;Area_LM&quot;, &quot;Area (ha)&quot;, &quot;GWL Means&quot;) quick.map(FFgwss$SDF, &quot;Area_LSD&quot;, &quot;Area (ha)&quot;, &quot;GWL Standard Deviation&quot;) quick.map(FFgwss$SDF, &quot;Area_LSKe&quot;, &quot;Area (ha)&quot;, &quot;GWL Skewness&quot;) 3.3 Conclusions and further analyses This practical computer lab allowed you to familiarize with GWSS, by the proposed application about geographically weighted summary statistics. This method allowed us to explore how the average burned area vary locally through Continental Portugal in the period 1990-2013. The global Geographically Weighted (GW) means informs us about the local average value of the burned area, based of the neighboring events occurred in a given period. Similarly, you may compute the GW standard deviation to see the extent to which the size of the burned area spread around this mean. Finally you can compute the GW skewness to measure the symmetry of distribution: a positively skewed distribution means that there is a higher number of data points with low values, with mean value lower that the median; and the contrary for a negatively skewed distribution. To ensure that everything is perfectly clear, we propose you to answer the following questions. You can find the answers in the reference paper (Tonini et al., 2017). What is the pattern distribution of the GW-means for burned area in Portugal during the investigated periods? Does the GW-standard deviation follows the same pattern? How can you interpret the two pattern in terms of burned area and their characterization? GW-skewness has positive values everywhere: what does it means? What do these values suggest to be the distribution of the burned areas, in terms of their size, around the local means? Which can be other applications of GWSS for geo-environmental data? In other words, can you imagine other geo-environmental dataset that can be analysed using GWSS? You can finally play with the code and try to run it using a different numbers of nearest neighbors (bw=x) and compare the results. References Brunsdon, C. (1995). Estimating probability surfaces for geographical point data: An adaptive kernel algorithm. Computers &amp; Geosciences, 21(7), 877–894. https://doi.org/10.1016/0098-3004(95)00020-9 Brunsdon, C. (2019, July). RPubs - GWSS - (7th channel network conference). https://rpubs.com/chrisbrunsdon/503649 Brunsdon, C., Fotheringham, A. S., &amp; Charlton, M. (2002). Geographically weighted summary statistics - a framework for localised exploratory data analysis. Computers, Environment and Urban Systems, 26(6), 501–524. https://doi.org/10.1016/s0198-9715(01)00009-6 Lu, B., Harris, P., Charlton, M., &amp; Brunsdon, C. (2014). The GWmodel r package: Further topics for exploring spatial heterogeneity using geographically weighted models. Geo-Spatial Information Science, 17(2), 85–101. https://doi.org/10.1080/10095020.2014.917453 Tonini, M., Pereira, M. G., Parente, J., &amp; Vega Orozco, C. (2017). Evolution of forest fires in portugal: From spatio-temporal point events to smoothed density maps. Natural Hazards, 85(3), 1489–1510. https://doi.org/10.1007/s11069-016-2637-x "],["spatio-temporal-cluster-analysis-of-geoenvironmental-processes.html", "Chapter 4 Spatio-Temporal Cluster Analysis of Geoenvironmental Processes 4.1 ST clustering for fire management 4.2 Clustering methods 4.3 Computing lab: spatio-temporal clustering 4.4 Space-time K-function 4.5 Conclusions and further analyses", " Chapter 4 Spatio-Temporal Cluster Analysis of Geoenvironmental Processes Spatio-temporal cluster analysis (ST clustering) is a powerful tool used to identify patterns and relationships in data that vary across both space and time. In the context of geoenvironmental processes, this type of analysis helps in understanding how environmental phenomena—such as wildfires, landslides, or flood events—are distributed geographically and how they evolve over time. By detecting clusters, researchers can uncover hotspots of activity, assess trends, and identify potential triggers or influencing factors, leading to more informed decision-making and targeted interventions in environmental management. 4.1 ST clustering for fire management The configuration of forest fires across space and time presents a complex pattern which significantly affects the forest environment and adjacent human developments. Statistical techniques designed for spatio-temporal random points can be utilized to identify a structure, recognize hot-spots and vulnerable areas, and address policy makers to prevention and forecasting measures. In this practical computer lab we consider the same case study as in the “Geographically Weighted Summary Statistics” lab. The main objective is to reveal if space and time act independently or whether neighboring events are also closer in time, interacting to generate spatio-temporal clusters. The attribute that we will consider to achieve this goal is the starting date of fires events. To account for the different geographical distribution of fires in Portugal, events occurred in the Norther and Southern area will be modeled separately. For more details about the input dataset, please refer to the GWSS lab documentation. 4.2 Clustering methods To detect spatio-temporal clusters of forest fires, we will use the following statistical methods: The Ripley’s K- function to test the space-time interaction and the spatial attraction/independency between fires of different size. The kernel density estimator allowing elaborating smoothed density surfaces representing fires over-densities. We provide below a short description for both these methods. More details can be found in Tonini et al. (2017). 4.2.1 Ripley’s K-function The Ripley’s K-function allows inferring about the spatial randomness of mapped punctual events. It is largely applied in environmental studies to analyse the pattern distribution of a spatial point process. The original spatial univariate K-function \\(K(s)\\) is defined as the ratio between the expected number \\(E\\) of point events falling at a certain distance \\(r\\) from an arbitrary event and the intensity \\(\\lambda\\) of the spatial point process, this last corresponding to the average number of points per unit area. Under complete spatial randomness, which assumes the independence among the events, \\(K(s)\\) is equal to the area of the circle around the target event for each distance’s value. It follows that events are spatially clustered within the range of distances at which \\(K(s)\\) assumes vales higher than this area, while they are spatially dispersed for lower values. The temporal K-function \\(K(t)\\) is defined in the same way as for the spatial case, with the time-based intensity and the time length replacing the spatial parameters. The space-time K-function, \\(K(s,t)\\) can be considered as a bivariate function where space and time represent the two variables of the equation. It is defined as the number of further events occurring within a distance \\(r\\) and time \\(t\\) from an arbitrary event. 4.2.1.1 Spatio-temporal interaction If there is no space-time interaction, \\(K(s,t)\\) is equal to the product of the purely spatial and purely temporal K-function. Inversely, if space and time interact generating clusters, the difference between these two values is positive [\\(D(s,t)=K(s,t)-K(s)*K(t)\\)]. Thus, we can use the perspective 3D-plot of the function \\(D(s,t)\\) to obtain a first diagnostic of space-time clustering: positive values indicate an interaction between these two variables at a well-detectable spatio-temporal scale. 4.2.2 Kernel density estimator The Kernel Density Estimator (KDE) is a non-parametric descriptor tool widely applied in GIS-science to elaborate smoothed density surfaces from spatial variables. A kernel function \\(K\\) allows weighing up the contribution of each event, based on the relative distance of neighboring to the target. The parameter \\(h\\), called bandwidth, controls the smoothness of the estimated kernel density. Finally, the kernel density function \\(f_h(x)\\) is estimated by summing all the kernel functions \\(K\\) computed at each point location \\(x\\) and dividing the result by the total number of events (\\(n\\)): \\[f_h(x) = \\frac{1}{nh}\\sum_{i=j}^{n}K(\\frac{x-x_i}{h})\\] The time extension of the kernel density estimator (Nakaya &amp; Yano (2010)) allows to compute the three-dimensional kernel density estimator which includes the spatio-temporal dimensions. In the present case study we apply a quadratic weighting kernel function, which is an approximation to the Gaussian kernel. Regarding the bandwidth’s value, we propose to consider the results of the spatio-temporal K-function as an indicator. Indeed, the distance values showing a maximum cluster behavior over the displayed perspective \\(D(s,t)\\) plot can be attributed to the \\(h\\)-value, minimizing the problem of under- or over-smoothing due to an arbitrary choice of the bandwidth. 4.3 Computing lab: spatio-temporal clustering 4.3.1 Load the libraries Fist you have to load the following libraries: splancs: for display and analysis of spatial point pattern data sf: Support for simple features, a standardized way to encode spatial vector data ggplot2: A system for ‘declaratively’ creating graphics sp: Classes and methods for spatial data spatstat: comprehensive open-source toolbox for analyzing Spatial Point Patterns library(splancs) library(spatstat) library(sf) library(ggplot2) library(sp) (.packages()) ## [1] &quot;ggplot2&quot; &quot;sf&quot; &quot;spatstat&quot; &quot;spatstat.linnet&quot; ## [5] &quot;spatstat.model&quot; &quot;rpart&quot; &quot;spatstat.explore&quot; &quot;nlme&quot; ## [9] &quot;spatstat.random&quot; &quot;spatstat.geom&quot; &quot;spatstat.univar&quot; &quot;spatstat.data&quot; ## [13] &quot;splancs&quot; &quot;sp&quot; &quot;stats&quot; &quot;graphics&quot; ## [17] &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; ## [21] &quot;base&quot; 4.3.2 Import the forest fire dataset In this section you will load the geodata representing the dataset of the Forest Fires (FF) occurred in the continental Portuguese area in the period 1990-2013. You will also load the boundaries of the study area. You will start by exploring the datasets using mainly visual tools (plotting and histogram). # Import Portugal boundary Portugal &lt;- st_read(&quot;data/KDE/Portugal.shp&quot;) # entire area PortN&lt;- st_read(&quot;data/KDE/Porto_North.shp&quot;) # northern area PortS&lt;- st_read(&quot;data/KDE/Porto_South.shp&quot;) # southern # Import the forest fires dataset FF&lt;-st_read(&quot;data/KDE/ForestFires.shp&quot;) # entire area FFN&lt;-st_read(&quot;data/KDE/FF_North.shp&quot;) # Northern area FFS&lt;-st_read(&quot;data/KDE/FF_South.shp&quot;) # Southern area # Import the shapefile of the Tagus river river&lt;-st_read(&quot;data/KDE/Rio_Tajo.shp&quot;) summary(FF$Area_ha) # summary statistics hist(FF$Area_ha) hist(log10(FF$Area_ha)) # see the lab GWSS for more details For a better understanding of the phenomenon, we can group the events according to the size of the burnt area and to their incidence in the northern and in the southern part of continental Portugal. 4.3.3 FF-subsets based on the burned areas’ size Remember that fires of different size can have been induced by different drivers. Thus, in the following, we will investigate all the global cluster behavior of forest fires in Portugal considering the three subset separately. As you have seen in the lab GWSS, based on the frequency distribution of the burned areas, the following three classes can be defined: Small fires: less than 15 ha Medium fires: between 15 ha and 100 ha Large fires: bigger than 100 ha SF=(subset(FF, Area_ha &lt;=15)) #create a sub-set including only small fires. summary(SF) ## Year Area_ha X Y ## Min. :1990 Min. : 5.000 Min. : 82761 Min. : 7772 ## 1st Qu.:1997 1st Qu.: 6.624 1st Qu.:182972 1st Qu.:403849 ## Median :2001 Median : 8.569 Median :213159 Median :472892 ## Mean :2002 Mean : 9.041 Mean :219789 Mean :440692 ## 3rd Qu.:2008 3rd Qu.:11.219 3rd Qu.:259054 3rd Qu.:518384 ## Max. :2013 Max. :15.000 Max. :361492 Max. :573192 ## geometry ## POINT :10902 ## epsg:NA : 0 ## +proj=tmer...: 0 ## ## ## # This is to save the plot pSF &lt;- ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=SF, size=0.5, col=&quot;yellow&quot;)+ ggtitle(&quot;Small fires&quot;) + coord_sf() MF=(subset(FF, Area_ha &gt;15 &amp; Area_ha &lt;=100)) #create a sub-set including only medium fires. summary(MF) ## Year Area_ha X Y ## Min. :1990 Min. :15.00 Min. : 82127 Min. : 7084 ## 1st Qu.:1997 1st Qu.:21.18 1st Qu.:186031 1st Qu.:403864 ## Median :2001 Median :30.89 Median :219459 Median :468503 ## Mean :2002 Mean :38.55 Mean :223915 Mean :439442 ## 3rd Qu.:2007 3rd Qu.:51.06 3rd Qu.:264349 3rd Qu.:518248 ## Max. :2013 Max. :99.98 Max. :359535 Max. :572488 ## geometry ## POINT :12070 ## epsg:NA : 0 ## +proj=tmer...: 0 ## ## ## # This is to save the plot pMF &lt;- ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=MF, size=0.5, col=&quot;orange&quot;)+ ggtitle(&quot;Midium fires&quot;) + coord_sf() LF=(subset(FF, Area_ha &gt;100)) #create a sub-set including only large fires. summary (LF) ## Year Area_ha X Y ## Min. :1990 Min. : 100.0 Min. : 83472 Min. : 8840 ## 1st Qu.:1996 1st Qu.: 139.6 1st Qu.:193831 1st Qu.:383164 ## Median :2002 Median : 220.7 Median :228709 Median :445272 ## Mean :2001 Mean : 548.3 Mean :229278 Mean :423441 ## 3rd Qu.:2006 3rd Qu.: 447.9 3rd Qu.:266146 3rd Qu.:500321 ## Max. :2013 Max. :66070.6 Max. :357933 Max. :571984 ## geometry ## POINT :4301 ## epsg:NA : 0 ## +proj=tmer...: 0 ## ## ## # This is to save the plot pLF &lt;- ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=LF, size=0.5, col=&quot;red&quot;)+ ggtitle(&quot;Large fires&quot;) + coord_sf() # Arrange the three spatial maps side by side install.packages(&#39;patchwork&#39;, repos = &quot;http://cran.us.r-project.org&quot;) ## ## The downloaded binary packages are in ## /var/folders/hf/q_2qq0tn1dngf4r3nzfhj3xh0000gn/T//RtmpXq3RfW/downloaded_packages library(patchwork) # Allow to combine separate ggplots into the same graphic pSF+pMF+pLF 4.3.4 FF-subsets based on their geographical distribution In continental Portugal, the northen half of the country (above the Tagus River) is characterized by the predominance of forest and semi-natural areas, and by the development of the main cities with their sub-urban ares intermingled with wild land, which makes the Northern area highly prone to forest fires. On the other hand, the southern half of the country is dominated by agricultural areas with mixed and broad-leaved forest concentrated near the south-west coast, which makes this are less affected by forest fires. For this reason we will consider these two areas separately. # Plot the map with all the objects ggplot ()+ geom_sf(data=Portugal)+ geom_sf(data=river, col=&quot;blue&quot;, size=2) + geom_sf(data=FFN, size=0.3, col=&quot;red&quot;) + geom_sf(data=FFS, size=0.3, col=&quot;orange&quot;) + ggtitle(&quot;Forest foirest in the northern and souther area&quot;) + theme(plot.title=element_text(hjust=0.5)) + coord_sf() 4.4 Space-time K-function 4.4.1 Extract time and PTS object The function stkhat, included in the library spancs, allows to compute the space-time K-function. As you can see from the R Documentation (command: help(stkhat)), same data manipulations are necessary to transform the input data in a compatible data frame format. Namely the user needs to specify: pts: the input forest fires dataset, with the coordinates to geolocalize each event. times: a vector of times, defined by the starting date of ignition. poly: a polygon of class matrix enclosing the input dataset (pts) s and tm: a vector of spatial (s) and a vector of temporal (tm) distances for the analysis. # Extract &quot;pts&quot; (divided by 1000 to compute in Km) FFN_pts &lt;- as.points(FFN$X/1000, FFN$Y/1000) FFN_times&lt;-FFN$Year # extract &quot;times&quot; # Extract the coordinates (in Km): PTN_xy&lt;-st_coordinates(PortN$geometry/1000) # Define the matrix with the set of bounding points (&quot;poly&quot;) enclosing the input dataset: FFN_poly&lt;-PTN_xy[, -c(3,4)] 4.4.2 Compute the space-time K-function We compute the space-time K-function for forest fires in the northern area. Since the computation can take a long time (about 20 mints), we propose you to load directly the output R object provided (STK_North_10y). The general code is also provided, but preceded by the hashtag, so it is not treated as a command: you have to remove # if you wish to evaluate the code. We consider here a subset of forest fires event occurred in the period 2001-2010. The parameters s and t are defined here as follows: (s) each kilometer distance up to ten kilometers; (tm) each year up to five years. NB: If you wish to run the code, remove # to make it work # Open stkhat documentation help(stkhat) library(readr) STK_North_10y &lt;- readRDS(&quot;data/KDE/STK_North_10y.RData&quot;) str(STK_North_10y) ## List of 5 ## $ s : num [1:11] 0 1 2 3 4 5 6 7 8 9 ... ## $ t : num [1:6] 0 1 2 3 4 5 ## $ ks : num [1:11] 1.61e-04 9.03 2.98e+01 6.18e+01 1.05e+02 ... ## $ kt : num [1:6] 0.814 2.29 3.596 4.827 6.085 ... ## $ kst: num [1:11, 1:6] 1.45e-03 1.06e+01 3.75e+01 7.47e+01 1.22e+02 ... 4.4.3 Assess the space-time clustering behavior In the following section you will explore and plot the values of the three components produced as outputs of the function stkhat: the spatial K-function (ks), the temporal K-function (kt); the space-time K-function (kst). then, you will plot the perspective 3D-plot of \\(D(s,t)\\) to evaluate the space-time clustering behavior of forest fires in the present case study. The multifaceted shape of this function, along with the spatial and the temporal dimension, can help to identify peaks of clustering. The corresponding values can be attributed to the bandwidth of the kernel density estimators allowing to elaborate smoothed density maps in the last step of this lab. 4.4.3.1 Plot the stkhat outputs Plot of the purely spatial and the purely temporal K function. # Plot of the purely spatial K function plot(STK_North_10y$s, STK_North_10y$ks, type=&quot;l&quot;, xlab=&quot;distance&quot;, ylab=&quot;Estimated Ks&quot;, main=&quot;Spatial K function&quot;) lines(STK_North_10y$s, pi*STK_North_10y$s^2, type=&quot;l&quot;, col=&quot;red&quot;) # Plot of the purely temporal K-function plot(STK_North_10y$t, STK_North_10y$kt, type=&quot;l&quot;, xlab=&quot;time&quot;, ylab=&quot;Estimated Kt&quot;, main=&quot;Temporal K function&quot;) lines(STK_North_10y$t, 2*STK_North_10y$t, type=&quot;l&quot;, col=&quot;red&quot;) Plot the space-time D-plot # Define the function: D(s,t)=K(s,t)-[K(s)*K(t)] Dplot &lt;- function (stkhat, Dzero = FALSE, main=TRUE) { oprod &lt;- outer(stkhat$ks, stkhat$kt) st.D &lt;- stkhat$kst - oprod persp(stkhat$s, stkhat$t, st.D, xlab = &quot;Distance (Km)&quot;, ylab = &quot;Time (years)&quot;, zlab = &quot;D(s,t)&quot;, expand = 0.5, ticktype = &quot;detailed&quot;, theta = -45, shade = 0.75, cex = 0.7, ltheta=120, col=&quot;cyan1&quot;, font.lab=2) } Dplot(STK_North_10y) title(&quot;Dplot Nothern Fires&quot;) 4.4.3.2 Run the space-time kernel density estimator The multifaceted shape the D-Plot identify peaks of clustering a time value of 3 year. In space, events are clustered at every distance, so in this case we use the maximum values (10 km). these two values are attributed to the bandwidth of the kernel density estimators allowing to elaborate smoothed density maps. # Open the help to analyse the parameter of this kernel function: help(kernel3d) # Run the function KDE_FFN&lt;-kernel3d(FFN_pts, FFN_times, seq(80, 362, 1), seq(180, 580, 1), seq(1990,2013,1), 10, 3) summary(KDE_FFN$v) hist(log10(KDE_FFN$v)) min(KDE_FFN$v[KDE_FFN$v&gt;0]) #check the lower non-zero value ## [1] 2.893333e-12 # Create quantile clssification Q&lt;-quantile(KDE_FFN$v, seq(0.5,1,0.05)) Q ## 50% 55% 60% 65% 70% 75% ## 0.0002194741 0.0097073381 0.0263666531 0.0478856382 0.0748979448 0.1093260662 ## 80% 85% 90% 95% 100% ## 0.1560709189 0.2210177893 0.3232627322 0.5162620818 2.1627084390 # Create a blue/red palette pal&lt;-colorRampPalette(c(&quot;grey&quot;,&quot;blue&quot;,&quot;green&quot;, &quot;yellow&quot;,&quot;orange&quot;, &quot;red&quot; )) colsR&lt;-pal(length(Q)-1) # Display classes pie(Q, clockwise=TRUE, labels=round(Q, digits=2), border=&quot;white&quot;, col=colsR) # Plot KDE maps for selected years oldpar&lt;-par(mfrow=c(5,5), mar=c(1,1,1,1)) for (i in 1:24){ (image(seq(80, 362, 1), seq(180, 580, 1), KDE_FFN$v[,,i], asp=1, xlab=&quot;&quot;, ylab=&quot;&quot;, main=1989+i, breaks=Q, col=colsR)) } 4.5 Conclusions and further analyses This practical computing lab allowed you to asses the global cluster behavior of hazardous events, achieved by using the Ripley’s k-function. In addition, we learned how smoothed density maps can be elaborated from punctual events, namely using the kernel density estimator. Both spatial and the temporal dimension have been considered in this case. You cluld explore the density distribution of forest fires events through Continental Portugal in the period 1990-2013. In the northern half of the country, hot spots are present almost on each investigated years, with a higher concentration in the northern areas. To ensure that everything is perfectly clear, we propose you to do this lab again, by using this time another subset chosen among the forest fires in the Southern area, the small, medium or the large forest fires dataset. For whatever dataset you are going to use, try to answer to the following questions. The reading of the paper Tonini et al. (2017), which inspired this lab, can help you in in this task. At which spatial and temporal distance you can observe a peak of clustering? Describe which is spatio-temporal density distribution of forest fires events through the study area. References Nakaya, T., &amp; Yano, K. (2010). Visualising crime clusters in a space-time cube: An exploratory data-analysis approach using space-time kernel density estimation and scan statistics. Transactions in GIS, 14(3), 223–239. https://doi.org/10.1111/j.1467-9671.2010.01194.x Tonini, M., Pereira, M. G., Parente, J., &amp; Vega Orozco, C. (2017). Evolution of forest fires in portugal: From spatio-temporal point events to smoothed density maps. Natural Hazards, 85(3), 1489–1510. https://doi.org/10.1007/s11069-016-2637-x "],["predictive-mapping-of-natural-hazards-using-random-forest.html", "Chapter 5 Predictive Mapping of Natural Hazards Using Random Forest 5.1 RF for landslides susceptibility mapping 5.2 Computing lab: Random Forest 5.3 Extract values 5.4 Run Random Forest 5.5 Susceptibility mapping 5.6 Conclusions and further analyses 5.7 Further reading on this topic", " Chapter 5 Predictive Mapping of Natural Hazards Using Random Forest Random Forest (RF) is a robust and widely-used machine learning algorithm particularly suited for predictive mapping in the context of natural hazards and susceptibility assessments. It operates by constructing multiple decision trees during training, and then aggregating their predictions to improve accuracy and generalizability (Breiman, 2001). For the sake of clarity, we define “susceptibility of an area” as the potential to experience a particular hazard in the future, based only on the intrinsic local properties of the territory, assessed in terms of relative spatial likelihood. Machine Learning (ML) based approaches lend themselves particularly well to this purpose. Essentially ML includes algorithms capable of learning from and making predictions on data by modelling the hidden/non-linear relationships between a set of input variables (predictor variables) and output observations. In natural hazard and susceptibility mapping, RF can analyze complex relationships between environmental variables such as topography, soil type, vegetation cover, and climate data to predict the likelihood of hazardous events, like landslides, floods, or earthquakes, across a landscape. Its ability to handle large datasets, manage variable interactions, and provide importance rankings for predictor variables makes it an invaluable tool for generating reliable susceptibility maps, which are crucial for disaster risk management and land-use planning. 5.1 RF for landslides susceptibility mapping In this application, we explore the capabilities RF to elaborate landslides susceptibility mapping in Canton Vaud, Switzerland. Landslides are one of the major hazard occurring around the world. In Switzerland, landslides cause damages to infrastructures and sometimes threaten human lives, especially shallow landslides. Such slope movements are mainly triggered by intense rainfalls and generally very rapid and hardly predictable. In this computing lab we introduce a data-driven methodology based on RF to elaborate the landslides susceptibility map of canton of Vaud, in Switzerland. RF is applied to a set of independent variables (i.e., the predictors) and dependent variables (the inventoried landslides and an equal number of locations for absences). The overall methodology is described in the following graphic (). Figure 5.1: Basic elements of the generic methodology 5.2 Computing lab: Random Forest 5.2.1 Load the libraries To perform the analysis, you have first to install the following libraries: terra: Methods for spatial data analysis with vector (points, lines, polygons) and raster (grid) data. sp: Classes and methods for spatial data. readr: The goal of ‘readr’ is to provide a fast and friendly way to read rectangular data (like ‘csv’, ‘tsv’, and ‘fwf’). randomForest: Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) doi:10.1023/A:1010933404324. dplyr: It is the next iteration of plyr, focused on tools for working with data frames (hence the d in the name). pROC: Allowing to compute, analyze ROC curves, and plotROC: to display ROC curve ggplot2: Is a system for declaratively creating graphics. List of the loaded libraries ## [1] &quot;plotROC&quot; &quot;ggplot2&quot; &quot;pROC&quot; &quot;dplyr&quot; &quot;randomForest&quot; ## [6] &quot;readr&quot; &quot;terra&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [11] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 5.2.2 Import and process geodata Import landslides punctual dataset presences and absences (LS_pa) and predictor variables (in raster format). This help the exploratory data analyses step and to understand the input data structure. 5.2.3 Landslides dataset The landslide inventory has been provided by the environmental office of the Canton Vaud. Only shallow landslides are used for susceptibility modelling. One pixel per landslide-area (namely the one located at the highest elevation) has been extracted. Since the landslide scarp is located in the upper part of the polygon, it makes sense to consider the highest pixel to characterize each single event. Our model includes the implementation of the landslide pseudo-absences, which are the areas where the hazardous events did not took place (i.e. landslide location is known and the mapped footprint areas are available, but the non-landslide areas have to be defined). Indeed, to assure a good generalization of the model and to avoid the overestimation of the absence, pseudo-absences need to be generated in all the cases where they are not explicitly expressed. In this case study, an equal number of point as for presences has been randomly generated in the study area, except within landslides polygons, lakes and glaciers (that is what is called “validity domain”, where events could potentially occur). # Import the boundary of Canton Vaud Vaud &lt;- vect(&quot;data/RF/Vaud_CH.shp&quot;) Lake &lt;- vect(&quot;data/RF/Lakes_VD.shp&quot;) # Import the landslides dataset (dependent variable) LS_pa &lt;- read.csv(&quot;data/RF/LS_pa.csv&quot;) # Convert the numeric values (0/1) as factor ##(i.e. categorical value) LS_pa$LS&lt;-as.factor(LS_pa$LS) LS_vect&lt;-vect(LS_pa, geom=c(&quot;X&quot;, &quot;Y&quot;),crs=crs(Vaud)) # Display the structure (str) and result summaries (summary) str(LS_vect) summary(LS_vect) # Plot the events plot(Vaud) plot(Lake, col=&quot;lightblue&quot;, add=TRUE) plot(LS_vect, col=LS_pa$LS, pch=20, cex=0.5, add=TRUE) 5.2.4 Predictor variables Selecting predictive variables is a key stage of landslide susceptibility modelling when using a data-driven approach. There is no consensus about the number of variables and which variables should be used. In the present exercise e will use the following: DEM (digital elevation model): provided by the Swiss Federal Offce of Topography. The elevation is not a direct conditioning factor for landslide; however, it can reflect differences in vegetation characteristics and soil Slope: is one of the most explicating factor in landslide susceptibility modelling. \\[Slope = arctan(\\sqrt{(dz/dx)^2 + (dz/dy)^2)} * (\\pi/2)\\] Curvature: curvature is widely used in landslide susceptibility modelling. It allows assessing the water flow acceleration and sediment transport process (profile curvature) and the water flow propensity to converge and diverge (plan curvature). They were derived from DEM using the curvature tool in ArcGIS. TWI (topographical water index): topography plays a key role in the spatial distribution of soil hydrological conditions. Defining \\(\\alpha\\) as the upslope contributing area describing the propensity of a cell to receive water, and \\(\\beta\\) as the slope angle, TWI (compute by the formula below), reflects the propensity of a cell to evacuate water: \\[TWI=ln(\\alpha/tan(\\beta))\\] Distance to roads: roads build in mountainous areas often cut the slope, weakening the cohesion of the soil. Moreover, roads surfaces are highly impermeable. This raster has been elaborated using the euclidean distance tool in ArcGIS, from the swissTLMRegio map where roads are represented by lines. Land Cover: developed by the Swiss administration and based on aerial photographs and control points. It includes 27 categories distributed in the following 6 domains: human modified surfaces, herbaceous vegetation, shrubs vegetation, tree vegetation, surfaces without vegetation, water surfaces (glaciers included). Geology: The use of the lithology increase the performance of the susceptibility landslide models. We use here the map elaborated by the Canton Vaud, defining the geotypes and reclassified in 10 classes in order to differentiate sedimentary rocks. Than the predictor variables have to be aggregated into a single object, storing multiple raster. We use here the generic function c to combine the single raster into a multiple raster object. ## Import raster (independent variables) 25 meter resolution landCover&lt;-as.factor(rast(&quot;data/RF/landCover.tif&quot;)) geology&lt;-as.factor(rast(&quot;data/RF/Geological_classes.tif&quot;)) planCurv&lt;-rast(&quot;data/RF/plan_curvature.tif&quot;)/100 profCurv&lt;-rast(&quot;data/RF/profil_curvature.tif&quot;)/100 # this because ArcGIS multiply curvature values by 100 TWI &lt;- rast(&quot;data/RF/TWI.tif&quot;) Slope &lt;- rast(&quot;data/RF/Slope.tif&quot;) dem &lt;- rast(&quot;data/RF/DEM.tif&quot;) dist &lt;- rast(&quot;data/RF/dist_roads.tif&quot;) # Combine raster features&lt;-c(dist, dem, landCover, TWI, planCurv, profCurv, Slope, geology) # renames features as in LS names(features)&lt;-c(&quot;distRoad&quot;, &quot;DEM&quot;, &quot;landCover&quot;, &quot;TWI&quot;, &quot;planCurv&quot;, &quot;profCurv&quot;, &quot;slope&quot;, &quot;geology&quot;) # mask to DEM extension features &lt;- terra::mask(features, dem) plot(features) 5.2.5 The use of categorical variables in Machine Learning The majority of ML algorithms (e.g., support vector machines, artificial neural network, deep learning) makes predictions on the base of the proximity between the values of the predictors, computed in terms of euclidean distance. This means that these algorithms can not handle directly categorical values (i.e., qualitative descriptors). Thus, in most of the cases, categorical variables need to be transformed into a numerical format. One of the advantage of using Random Forest (as implemented in R) is that it can handle directly categorical variables, since the algorithm operate by constructing a multitude of decision trees at training time and the best split is chosen just by counting the proportion of each class observation. To understand the characteristics of the categorical variables, you can plot the tow raster Land Cover and Geology by using their original classes and look at the attribute table to analyse the corresponding definitions. plot(geology) plot(landCover) (#fig:cat_class)Categorical variables 5.3 Extract values In this step, you will extract the values of the predictors at each location in the landslides (presences and absences) dataset. The final output represents the input dataset with dependent (LS = landslides) and independent (raster features) variables. # Extract values from the raster dataset (features) LS_input &lt;-extract(features, LS_vect, method=&quot;simple&quot;, xy=TRUE) LS_input$LS &lt;- as.factor(LS_vect$LS) # add LS str(LS_input) # explore the dataset # remove extra column (ID) LS_input &lt;- LS_input[,2:ncol(LS_input)] LS_input&lt;-na.omit(LS_input) # Explore the newly created input dataset head(LS_input) str(LS_input) 5.3.1 Split the input dataset into training (80%) and testing (20%) A well-established procedure in ML is to split the input dataset into training, validation, and testing. The training dataset is needed to calibrate the parameters of the model, which will be used to get predictions on new data. The purpose of the validation dataset is to optimize the hyperparameter of the model (training phase). NB: in RF this subset is represented by the Out-Of-Bag (OOB)! To provide an unbiased evaluation of the final model and to assess its performance, results are then predicted over unused observations (prediction phase), defined as the testing dataset. # Shuffle the rows set.seed(123) # to ensure reproducibility LS_input_sh&lt;-LS_input [sample(nrow(LS_input), nrow(LS_input)), ] # Split the input dataset into training (80%) and testing (20%) n &lt;- nrow (LS_input_sh) set.seed(123) n_train &lt;- round(0.80 * n) train_indices &lt;- sample(1:n, n_train) # Create indices LS_train &lt;- LS_input_sh[train_indices, ] LS_test &lt;- LS_input_sh[-train_indices, ] # Count the number of elements in the two subset: training and testing count(LS_train) count(LS_test) 5.4 Run Random Forest Computationally, a subset of the training dataset is generated by bootstrapping (i.e. random sampling with replacement). For each subset a decision tree is grown and, at each split, the algorithm randomly selects a number of variables (mtry) and it computes the Gini index to identify the best one. The process stops when each node contains less than a fixed number of data points. The fundamental hyperparameters that needs to be defined in RF are mtry and the total number of trees (ntrees). The prediction error on the training dataset is finally assessed by evaluating predictions on those observations that were not used in the subset, defined as “out-of-bag” (OOB). This values is used the optimize the values of the hyperparameters, by a trial and error process (that is, trying to minimize the OOB estimate of error rate). # Set the seed of R‘s random number generator, ## this is useful for creating simulations that can be reproduced. set.seed(123) # Run RF model RF_LS&lt;-randomForest(y=LS_train$LS, x=LS_train[1:8],data=LS_train, ntree=500, mtry=3,importance=TRUE) 5.4.1 RF main outputs Printing the results of RF allows you to gain insight into the outputs of the implemented model, namely the following: a summary of the model hyperparameters, the OOB estimate of error rate, the confusion matrix (in this case a 2x2 matrix used for evaluating the performance of the classification model: 1==presence vs 0==absence). The plot of the error rate is useful to estimate the decreasing values on the OOB and on the predictions (1==presence vs 0==absence) over increasing number of trees. # Print the model setting print(RF_LS) ## ## Call: ## randomForest(x = LS_train[1:8], y = LS_train$LS, ntree = 500, mtry = 3, importance = TRUE, data = LS_train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 16% ## Confusion matrix: ## 0 1 class.error ## 0 1691 397 0.1901341 ## 1 267 1795 0.1294859 # Show the predicted probability values RF.predict &lt;- predict(RF_LS,type=&quot;prob&quot;) head(RF.predict) # 0 = absence ; 1 = presence ## 0 1 ## 2063 0.28795812 0.71204188 ## 1750 0.02259887 0.97740113 ## 688 0.76506024 0.23493976 ## 3301 1.00000000 0.00000000 ## 4401 0.98514851 0.01485149 ## 1166 0.05113636 0.94886364 # Plot the OOB error rate plot(RF_LS) legend(x=&quot;topright&quot;, legend=c(&quot;perd 0&quot;, &quot;pred 1&quot;, &quot;OOB error&quot;), col=c(&quot;red&quot;, &quot;green&quot;, &quot;black&quot;), lty=1:2, cex=0.8) 5.4.2 Model evaluation The prediction capability of the implemented RF model can be evaluated by predicting the results over previously unseen data, that is the testing dataset. The Area Under the “Receiver Operating Characteristic (ROC)” Curve (AUC) represents the evaluation score used here as indicator of the goodness of the model in classifying areas more susceptible to landslides. ROC curve is a graphical technique based on the plot of the percentage of correct classification (the true positives rate) against the false positives rate (occurring when an outcome is incorrectly predicted as belonging to the class “1” when it actually belongs to the class “0”), evaluates for many thresholds. The AUC value lies between 0.5, denoting a bad classifier, and 1, denoting an excellent classifier, which, on the other hand, can indicate overfitting. # Make predictions on the testing dataset RFpred_test &lt;- predict(object = RF_LS, newdata = LS_test, type=&quot;prob&quot;) # Make predictions on the validation dataset (taining using the Out-of-bag) RFpred_oob &lt;- predict(object = RF_LS, newdata = LS_train, type=&quot;prob&quot;, OOB=TRUE) roc_test &lt;- roc(LS_test$LS, RFpred_test[,2]) roc_oob &lt;- roc(LS_train$LS, RFpred_oob[,2]) plot.new() plot(1-roc_test$specificities, roc_test$sensitivities, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &quot;False positive rate&quot;, ylab = &quot;True positive rate&quot;) lines(1-roc_oob$specificities, roc_oob$sensitivities, type = &#39;l&#39;, col = &#39;red&#39;) # Print AUC values roc_test roc_oob 5.5 Susceptibility mapping You have now all the elements necessary to elaborate the final landslide susceptibility map. This will be achieved by making predictions (of presence only) based on the values of the predictor variables, which are stored into the multiple raster named features, created above. # Convert the input multiple raster to data frame features_df&lt;-as.data.frame(features, xy=TRUE, na.rm=TRUE) # Predict results of RF (probability of fires presence: index = 2) to the feature space ## this operation can take several minuts to run! scp_map&lt;-predict(object = RF_LS, newdata = features_df, type=&quot;prob&quot;, index=2) scp_df=as.data.frame(scp_map) # convert to data frame # get coordinates scp_df$X=features_df$x scp_df$Y=features_df$y # Convert to raster the probability to get a landslide ## 3=X, 4=Y, 2=probability of presence (1) scp_rast=rast(scp_df[,c(3,4,2)],type=&quot;xyz&quot;) summary(scp_rast) plot(scp_rast) # Save all outputs ## this operation can take several minuts to run! save.image(file=&quot;LSM_RF.RData&quot;) # Export susceptibility map as raster writeRaster(scp_rast,&quot;Susceptibility_LSmap.tif&quot;,overwrite=T) 5.5.1 Class intervals for decision maker What can you say by looking at this map? Actually a risk heat map is a data visualization tool for communicating the level for a specific risk to occur. These maps helps authorities to identify and prioritize the risks associated with a given hazard. Normally an authority (i.e., a decision maker) prioritize its efforts based on the available resources it has. So, it can be more useful to detect the areas with the highest probability of burning based on certain intervals (i.e., breaks). The authority can thus concentrate its resources for preventive actions on a given threshold (such as 5%, 10%, or 20%) of the area with the highest probability of burning, instead of concentrate on the areas with a “stochastic” output probability value of 0.8 (for example). Susceptibility maps based on equal intervals, five classes (each 20%) library(&quot;RColorBrewer&quot;) plot(scp_rast, xlab = &quot;East [m]&quot;, ylab = &quot;North [m]&quot;, main = &quot;Landslides susceptibility map&quot;, col = rev(c(&#39;#a50026&#39;,&#39;#d73027&#39;,&#39;#f46d43&#39;,&#39;#fdae61&#39;,&#39;#fee08b&#39;,&#39;#d9ef8b&#39;))) Breaks chosen based on the summary statics: these values corresponds to 0-25%-50%-75%-100% of the p-value distribution. brk&lt;-c(0, 0.03, 0.14, 0.43, 1) plot(scp_rast, xlab = &quot;East [m]&quot;, ylab = &quot;North [m]&quot;, main = &quot;Landslides susceptibility map&quot;, col = rev(c(&quot;brown&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;grey&quot;)), breaks=brk) Breaks chosen based on well-established percentile classes # Output predicted values are transformed to a vector pred.vect &lt;- as.vector(scp_map[, 2]) # The function &quot;quantile&quot; is used to fix classes qtl.pred &lt;- quantile(pred.vect,probs=c(0.25,0.5,0.75,0.85,0.95), na.rm = TRUE) qtl.pred ## 25% 50% 75% 85% 95% ## 0.026 0.138 0.424 0.606 0.824 # and then extract the corresponding values qtl.int&lt;- c(0,0.03,0.14,0.42,0.6,0.82,1) plot(scp_rast, xlab = &quot;East [m]&quot;, ylab = &quot;North [m]&quot;, main = &quot;Landslides susceptibility map&quot;, col = rev(c(&quot;brown&quot;, &quot;red&quot;, &quot;orange&quot;,&quot;yellow&quot;, &quot;green&quot;, &quot;grey&quot;)), breaks=qtl.int) 5.6 Conclusions and further analyses This exercise allowed you to familiarize with Random Forest, by the proposed application about landslides susceptibility mapping and variables importance assessment. To ensure that everything is perfectly clear, we propose you to answer the following questions. Why is it important to implement the pseudo-absence, other that the presences (i.e., the observations) in a data-driven modelization? What is the difference between a numerical and a categorical variable? Give some examples of both types. Why RF can handle directly native categorical variables? Is it the same for other algorithms (like logistic regression or neural network)? Which is the values of the OOB estimate error rate of your model? Which parameters you can change to try to reduce it? Be brave and do it (i.e., change the values for ntree and mtry, than analyse which values for the AUC you obtain and which model perform better. 5.7 Further reading on this topic The research framework that inspired this computing lab refers to a pioneering study in susceptibility mapping for wildfire events by (Tonini et al., 2020) and further developed for the assessment of variable importance by (Trucchia et al., 2022). Analyses have been adapted to the case study of landslides. References Breiman, L. (2001). Random forest. Machine Learning, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324 Tonini, M., D’Andrea, M., Biondi, G., Degli Esposti, S., Trucchia, A., &amp; Fiorucci, P. (2020). A machine learning-based approach for wildfire susceptibility mapping. The case study of the liguria region in italy. Geosciences, 10(3), 105. https://doi.org/10.3390/geosciences10030105 Trucchia, A., Izadgoshasb, H., Isnardi, S., Fiorucci, P., &amp; Tonini, M. (2022). Machine-learning applications in geosciences: Comparison of different algorithms and vegetation classes’ importance ranking in wildfire susceptibility. Geosciences, 12(11), 424. https://doi.org/10.3390/geosciences12110424 "],["interpretability-explainability-with-random-forest.html", "Chapter 6 Interpretability &amp; Explainability with Random Forest 6.1 Aim of the present lab 6.2 Features importance score", " Chapter 6 Interpretability &amp; Explainability with Random Forest The distinction between interpretability and explainability lies in their focus and depth. We can say that interpretability focuses on understanding the inner workings of the models, while explainability focuses on explaining the decisions made.  Model complexity: when dealing with intricate models like Random Forest (with tens of variables and thousands of trees), up to deep neural networks, interpretability becomes challenging due to their complexity and the interplay among their components. In such scenarios, explainability proves to be a more practical approach, as it focuses on clarifying decisions rather than delving into the complexities of the model. Communication: in terms of audience and purpose, interpretability primarily concerns machine learning specialists , whereas explainability targets end users seeking to grasp model decisions. Consequently, explainability necessitates a more straightforward and intuitive communication of information. While Random Forest models are powerful and often yield high accuracy, interpretability can be challenging due to their complex structure and the high number of tress. However, the following techniques can enhance the interpretability and explainability of Random Forest models. A surrogate model, such as a single decision tree, can approximate the predictions of a more complex model like a Random Forest, which is composed of thousands of decision trees. The surrogate model is more interpretable and helps in understanding the general rules that the Random Forest model follows. For explainability, examining feature importance scores, which measure the contribution of each variable to the model’s predictions, allows us to identify the most influential variables in the model’s decisions. In addition, partial dependence plots enable us to visualize how changes in a variable influence the model’s predictions, making this tool useful for interpreting the global effects of predictors across the entire dataset. 6.1 Aim of the present lab In this computing lab you will work with the outputs of Random Forest resulting from the lab on landslides susceptibility map (LSM_RF). Firstly, we will explore the relative importance of the predictor variables (feature importance scores) , and their relative probability of prediction success (partial dependence plots). In the second part, we will apply a local version of Random Forest (named “Geographical Random Forest”) to analyse the spatial heterogeneity of the local variable importance. 6.1.1 Re-load libraries and workspace If you have quit the workspace where you have run the RF model for landslide susceptibility map you need to load it again in this new project. Loading the workspace refers to the action of restoring the saved state of R environment. When you save your workspace in R, it typically includes all the objects (such as variables, functions, data frames, etc.) that are currently present in your R session. Loading the workspace means to restore this saved state, bringing back all the previously saved objects into your current R session. ## [1] &quot;RColorBrewer&quot; &quot;tidyr&quot; &quot;randomForest&quot; &quot;classInt&quot; &quot;plotROC&quot; ## [6] &quot;ggplot2&quot; &quot;pROC&quot; &quot;dplyr&quot; &quot;readr&quot; &quot;foreign&quot; ## [11] &quot;terra&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; ## [16] &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 6.2 Features importance score Although machine learning algorithms are often considered as a black box, with RF is possible to plot a sample tree (selected randomly) to analyse its structure and investigate how decisions have been made. In addition RF provides two scores allowing to assess the importance of each variables in the model: the mean decrease in accuracy (MDA), and the mean decrease in Gini index. Higher values indicate the most important variables. library(&quot;party&quot;) x &lt;- ctree(LS~., data=LS_train) plot(x, type=&quot;simple&quot;) # Display the plot with the relative importance of each variable importance(RF_LS) varImpPlot(RF_LS) 6.2.1 Partial dependence plot In addition, the Partial Dependence Plot (PDP) allows us to estimate, for each single variable, the relative probability of prediction success over different ranges of values. PDP provides a graphical depiction of the marginal effect of each variable on the class probability over different ranges of continuous or discrete values. Positive values are associated with the probability of occurrence of the phenomena (i.e., landslides presence), while negative vales indicate its absence. # Slope partialPlot(RF_LS, LS_train, x.var = slope, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;Slope [Â°]&quot;, main = &quot;&quot;, ylab = &quot;PDP&quot;) # Elevation partialPlot(RF_LS, LS_train ,x.var = DEM, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;Elevation [m]&quot;, main = &quot;&quot;,ylab = &quot;PDP&quot;) # Profile curvature partialPlot(RF_LS, LS_train, x.var = profCurv, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;Profile curvature [1/m]&quot;, main = &quot;&quot;, ylab = &quot;PDP&quot;, xlim = c(-0.1,0.1)) # Plan Curvature partialPlot(RF_LS, LS_train, x.var = planCurv, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;Plan curvature [1/m]&quot;, main = &quot;&quot;, ylab = &quot;PDP&quot;, xlim = c(-0.1,0.1)) # Distance to road partialPlot(RF_LS, LS_train, x.var = distRoad, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;Distance to road [m]&quot;, main = &quot;&quot;, ylab = &quot;PDP&quot;) # Topographic wetness index partialPlot(RF_LS, LS_train, x.var = TWI, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;TWI [-]&quot;, main = &quot;&quot;, ylab = &quot;PDP&quot;) # Geology partialPlot(RF_LS, LS_train, x.var = geology, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;Geology&quot;, main = &quot;&quot;, ylab = &quot;PDP&quot;) # Land cover partialPlot(RF_LS, LS_train, x.var = landCover, rug = TRUE, which.class = RF_LS$classes[2],xlab= &quot;Land Cover&quot;, main = &quot;&quot;, ylab = &quot;PDP&quot;) "],["local-random-forest.html", "Chapter 7 Local Random Forest 7.1 Computing lab: GRF 7.2 Conclusions and further analyses 7.3 Further reading on this topic", " Chapter 7 Local Random Forest Standard machine learning algorithms like Random Forest (RF) lack spatial calibration, hindering capturing the spatial heterogeneity in the relationship between a dependent and a set of independent variables. To account for spatial heterogeneity (i.e. non-stationarity) when modeling landslides spatial patterns as function of geographical features, in the present work we explore the local feature importance of geographical independent predisposing variables on the spatial distribution of landslides in canton Vaud (Switzerland). We adopted Geographically Random Forest (GRF), a spatial analysis method using a local version of RF algorithm (georganos_forest_2022?) . This is achieved by fitting a sub-model for each observation in space, taking into account the neighboring observations. GRF can model the non-stationarity coupled with a non-linear model (RF), which, compared to liner models, tends not to overfit due to its bootstrapping nature. In addition RF is suited for datasets with numerous predictor variables. Essentially, GRF was designed to be a bridge between machine learning and geographical models, combining inferential and explanatory power. 7.1 Computing lab: GRF We will use the last development of Geographical Random Forest (GRF) [4]. This function have been implemented for regression problem, so we need to transform our binary response variable (i.e., presence==1 / absence==0) as a numeric value which can assume a range of values from zero to one. ## &#39;data.frame&#39;: 5188 obs. of 11 variables: ## $ distRoad : num 160.1 55.9 106.1 70.7 125 ... ## $ DEM : num 443 451 460 458 463 ... ## $ landCover: Factor w/ 7 levels &quot;11&quot;,&quot;15&quot;,&quot;21&quot;,..: 3 1 2 2 3 1 3 3 3 1 ... ## $ TWI : num 7.4 8.72 9.08 8.55 8.33 ... ## $ planCurv : num -0.000143 0.003633 0.005981 0.006165 0.000217 ... ## $ profCurv : num 0.013297 -0.006447 0.000381 -0.001675 -0.005383 ... ## $ slope : num 20.86 2.91 2.04 3.46 4.32 ... ## $ geology : Factor w/ 11 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;7&quot;,..: 8 8 8 8 8 8 8 8 8 8 ... ## $ x : num 567887 568087 567912 567962 567812 ... ## $ y : num 2e+05 2e+05 2e+05 2e+05 2e+05 ... ## $ LS : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## &#39;data.frame&#39;: 4150 obs. of 12 variables: ## $ distRoad : num 182 25 90.1 212.1 134.6 ... ## $ DEM : num 917 719 662 498 810 ... ## $ landCover: Factor w/ 7 levels &quot;11&quot;,&quot;15&quot;,&quot;21&quot;,..: 5 5 1 3 3 3 3 2 5 3 ... ## $ TWI : num 8.15 8.47 8.73 15.52 8.98 ... ## $ planCurv : num 0.000789 0.003464 0.000962 -0.00004 0.000359 ... ## $ profCurv : num 7.89e-04 -1.17e-02 -7.98e-04 -3.99e-05 -4.42e-04 ... ## $ slope : num 28.31 33.15 2.88 1.94 4.49 ... ## $ geology : Factor w/ 11 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;7&quot;,..: 7 4 8 2 2 8 8 2 2 8 ... ## $ x : num 569837 561612 540062 504862 548312 ... ## $ y : num 133062 139887 154237 140712 169212 ... ## $ LS : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 1 2 2 2 1 1 ... ## $ LSregr : num 1 1 1 0 0 1 1 1 0 0 ... ## &#39;data.frame&#39;: 1038 obs. of 12 variables: ## $ distRoad : num 90.1 956.9 55.9 70.7 25 ... ## $ DEM : num 490 1299 850 480 1338 ... ## $ landCover: Factor w/ 7 levels &quot;11&quot;,&quot;15&quot;,&quot;21&quot;,..: 3 5 5 1 3 3 2 3 1 4 ... ## $ TWI : num 10.14 9.19 9.74 10.78 10.51 ... ## $ planCurv : num 0.005263 0.002523 -0.005231 -0.000883 0.001165 ... ## $ profCurv : num -0.010737 -0.003077 0.008209 0.000877 0.000685 ... ## $ slope : num 12.51 1.82 18.31 5.2 12.48 ... ## $ geology : Factor w/ 11 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;7&quot;,..: 8 3 8 2 2 8 2 2 8 5 ... ## $ x : num 529962 501762 552912 545262 572112 ... ## $ y : num 160712 149912 148212 189712 135962 ... ## $ LS : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 2 1 2 1 2 1 ... ## $ LSregr : num 1 0 1 0 1 0 1 0 1 0 ... Coords&lt;-LS_train[,9:10] # define coordinates # Run GRF set.seed(123) # initialize gwRF_LS&lt;-grf(LSregr~distRoad+DEM+landCover+TWI+planCurv+profCurv+slope+geology, LS_train, bw=40, mtry=3, kernel=&quot;adaptive&quot;,coords=Coords) ## Ranger result ## ## Call: ## ranger(LSregr ~ distRoad + DEM + landCover + TWI + planCurv + profCurv + slope + geology, data = LS_train, num.trees = 500, mtry = 3, importance = &quot;impurity&quot;, num.threads = NULL) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 4150 ## Number of independent variables: 8 ## Mtry: 3 ## Target node size: 5 ## Variable importance mode: impurity ## Splitrule: variance ## OOB prediction error (MSE): 0.117128 ## R squared (OOB): 0.5315825 ## distRoad DEM landCover TWI planCurv profCurv slope geology ## 181.93889 113.22987 33.98818 101.59075 83.15042 123.31246 302.83349 39.09378 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.00000 -0.03162 0.00000 -0.01882 0.03284 1.00000 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.3981333 -0.0030000 0.0000000 -0.0003047 0.0042917 0.2957333 ## Min Max Mean StD ## distRoad 0 4.846175 0.6640442 0.8686210 ## DEM 0 4.442619 0.7486061 0.7680691 ## landCover 0 4.301103 0.1936414 0.3141822 ## TWI 0 4.284581 0.6287907 0.6790622 ## planCurv 0 3.126327 0.4565076 0.4888202 ## profCurv 0 4.048531 0.5832407 0.6300842 ## slope 0 4.913916 0.9080469 0.9518115 ## geology 0 3.669751 0.2213336 0.4038733 saveRDS(gwRF_LS, &quot;gwRF_LS.rds&quot;) 7.1.1 Feature importance 7.1.1.1 Global variable importance plot Based on the results of the GRF, we can plot of the variable importance ranking for illustrative purposes. Values came from “Global ML Model Summary” –&gt; “Importance” # Create a data frame with variable names and importance values variable_importance &lt;- data.frame ( Variable = c(&quot;distRoad&quot;, &quot;DEM&quot;, &quot;landCover&quot;, &quot;TWI&quot;, &quot;planCurv&quot;, &quot;profCurv&quot;, &quot;slope&quot;, &quot;geology&quot;), Importance = c(181.18490, 114.32444, 34.23643, 101.51863, 84.81667, 125.93651, 297.74411, 39.22721 ) # Importance - Global ML ) # Assign different colors to the top three important variables variable_importance$Color &lt;- ifelse(variable_importance$Importance &gt;= sort(variable_importance$Importance, decreasing = TRUE)[3], &quot;orange&quot;, &quot;skyblue&quot;) # Create a bar plot for variable importance with different colors for the top three variables ggplot(data = variable_importance, aes(x = Variable, y = Importance, fill = Color)) + geom_bar(stat = &quot;identity&quot;) + scale_fill_identity() + labs(title = &quot;Variable Importance Plot&quot;, x = &quot;Variable&quot;, y = &quot;Importance&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for better readability 7.1.1.2 Local feature importance We can plot the local feature importance (FI) scores for the two variables that are globally most important, slope and distrance to road with the output values mapped over the geographic space. # Create a data frame with the values of the local variables importance and the coordinates for each location gwRF_LS_var&lt;-gwRF_LS$Local.Variable.Importance gwRF_LS_var_XY&lt;-cbind(gwRF_LS_var,LS_train$x,LS_train$y ) # add coordinates colnames(gwRF_LS_var_XY)[9]&lt;- &quot;X&quot; #rename column X-coordinate colnames(gwRF_LS_var_XY)[10]&lt;- &quot;Y&quot; #rename column Y-coordinate str(gwRF_LS_var_XY) ## &#39;data.frame&#39;: 4150 obs. of 10 variables: ## $ distRoad : num 0.53312 0.00351 0.31458 0.07548 2.46608 ... ## $ DEM : num 1.11112 0.00548 0.13488 0.79269 0.66155 ... ## $ landCover: num 0.1857 0.0072 0.0856 0.6757 0.0124 ... ## $ TWI : num 0.2571 0.0425 1.1416 0.2105 0.392 ... ## $ planCurv : num 0.20406 0.00112 0.12462 0.84817 1.17041 ... ## $ profCurv : num 0.1011 0.0032 0.2125 1.6097 2.5097 ... ## $ slope : num 0.3679 0.0269 0.6325 0.6656 0.9508 ... ## $ geology : num 0.078629 0.000606 0.09991 0.572837 0.01066 ... ## $ X : num 569837 561612 540062 504862 548312 ... ## $ Y : num 133062 139887 154237 140712 169212 ... library(sf) #for spatial data operations # Convert vector to sf (simple feature) Vaud&lt;-vect(&quot;data/RF/Vaud_CH.shp&quot;) Vaud_sf&lt;-st_as_sf(Vaud) # Output predicted values are transformed to a vector pred.vect &lt;- as.vector(gwRF_LS_var_XY$slope) library(classInt) #for classification brk&lt;-(classIntervals(pred.vect, n=5, style = &quot;fisher&quot;)) brkInt&lt;-round(brk$brks, digits=2) print(brkInt) ## [1] 0.00 0.39 1.00 1.81 2.99 4.91 #natural breaks (fisher) ggplot() + geom_point(data = gwRF_LS_var_XY, aes(x = X, y = Y, colour = slope), size = 1) + scale_color_gradientn(colors = c(&quot;white&quot;, &quot;orange&quot;,&quot;red&quot;, &quot;blue&quot;), breaks = c(0.00, 0.40, 1.15, 2.11, 3.23, 4.91), labels=c(0.00, 0.40, 1.15, 2.11, 3.23, 4.91)) + labs( x = &quot;X Coordinate&quot;, y = &quot;Y Coordinate&quot;)+ ggtitle(&quot;Local average FI - Slope&quot;)+ geom_sf(data = Vaud_sf, fill = &quot;transparent&quot;, color = &quot;black&quot;, size=2) #overlap borders # Output predicted values are transformed to a vector pred.vect &lt;- as.vector(gwRF_LS$Local.Variable.Importance$distRoad) brk&lt;-(classIntervals(pred.vect, n=5, style = &quot;fisher&quot;)) brkInt&lt;-round(brk$brks, digits=2) print(brkInt) # print breaks ## [1] 0.00 0.36 0.98 2.04 3.33 4.85 #natural breaks (fisher) ggplot() + geom_point(data = gwRF_LS_var_XY, aes(x = X, y = Y, colour = distRoad), size = 1) + scale_color_gradientn(colors = c(&quot;white&quot;, &quot;orange&quot;,&quot;red&quot;, &quot;blue&quot;), breaks = c(0.00, 0.37, 1.02, 2.17, 3.58, 4.85), labels=c(0.00, 0.37, 1.02, 2.17, 3.58, 4.85)) + labs( x = &quot;X Coordinate&quot;, y = &quot;Y Coordinate&quot;)+ ggtitle(&quot;Local average FI - Distance to roads&quot;)+ geom_sf(data = Vaud_sf, fill = &quot;transparent&quot;, color = &quot;black&quot;, size=2) #overlap borders 7.1.2 Local R squared The Local R-squared value represents the strength of the correlations of the local model on the features and ranges from 0 to 1. Rsq&lt;-gwRF_LS$LGofFit$LM_Rsq100 Rsq_XY&lt;-as.data.frame(cbind(Rsq,LS_train$x,LS_train$y)) # add coordinates colnames(Rsq_XY)[2]&lt;- &quot;X&quot; colnames(Rsq_XY)[3]&lt;- &quot;Y&quot; str(Rsq_XY) ## &#39;data.frame&#39;: 4150 obs. of 3 variables: ## $ Rsq: num -0.2341 -0.0148 -0.2604 0.2742 0.5882 ... ## $ X : num 569837 561612 540062 504862 548312 ... ## $ Y : num 133062 139887 154237 140712 169212 ... ggplot () + geom_point(data = Rsq_XY, aes(x = X, y = Y, colour = Rsq), size = 1)+ scale_color_gradientn(colors = c(&quot;white&quot;, &quot;yellow&quot;,&quot;red&quot;, &quot;blue&quot;))+ labs(title = &quot;Rsq&quot;, x = &quot;X Coordinate&quot;, y = &quot;Y Coordinate&quot;)+ ggtitle(&quot;Local R2&quot;)+ geom_sf(data = Vaud_sf, fill = &quot;transparent&quot;, color = &quot;black&quot;, size=2) #overlap borders 7.2 Conclusions and further analyses In the present exercise GRF has been used as a purely exploratory tool to estimate the spatial variation of the relationship between landslides in Canton Vaud (Switzerland) and the influencing factors. It allowed to elaborate maps delineating the local average importance of the most highly correlated features and to visualize the local fitting performance (R2 local value) into a map. To ensure that everything is perfectly clear, we propose you to answer the following questions: Among the following algorithms evaluate them in therms of their interpretability and explainability: Support Vector Machines , linear regression, Deep Learning Models, Decision Trees, K-Nearest Neighbors, Neural Networks, Random Forests, logistic regression. Which are the three most important variables of your model (based on the MDA)? What is the slope value (or range of values) that gives the highest probability of landslides occurrence? And for the geology, which are the most important classes? Evaluate the spatial variation of the relationship between landslides and slope / distance to roads in your study area by visually inspecting the local average importance of these features. You can replicate this code (some chiuncks of it) to evaluate the local average importance of the third most important variable, as well as to map the local mean squared error. 7.3 Further reading on this topic "],["segmentation-analysis-with-som-on-census-data.html", "Chapter 8 Segmentation Analysis with SOM on Census Data 8.1 Geo-demographic context in Switzerland 8.2 Self Organizing Map 8.3 Computing lab: SOM 8.4 Hierarchical clustering 8.5 Clusters characterization 8.6 Conclusions 8.7 Further analyses 8.8 References", " Chapter 8 Segmentation Analysis with SOM on Census Data The increasing availability of multivariate datatets stimulates researches to develop new techniques that differ from those utilized in earlier scientific paradigms. In the field of social and economic science, geo-demographic segmentation defines a technique used to classify a population based on input data describing administrative units and people living there (see Spielman &amp; Thill, 2008). Small areas can thus be classified into discrete categories using demographic input data, by means of data reduction techniques (such as the Principal component analysis). The main limitation of these approaches is the problem of communicating the multidimensional complexity characterizing the different output classes. Machine learning techniques can help in this context, as they are designed to extract useful formation and insight from the interaction of the variables describing the complex structure of a given phenomenon. In the present computing lab we introduce an unsupervised learning procedure based on Self-Organizing Map (SOM) (see Kohonen, 1982), allowing detecting clusters and characterizing the pattern of population dynamic in Switzerland in the recent period. This lab is inspired by the work of Marj Tonini et al. (2023). 8.1 Geo-demographic context in Switzerland Switzerland has the highest life expectancy in the world. It counts about 8,5 million inhabitants (official census data 2020), twice as much as at the beginning of the 20th century, mainly because of the high level of immigration. The number of foreigners that currently reside in the country corresponds to about one quarter of the total population. Most of the population (85%) lives in cities. Population aging increased over the course of the 20th century, with one in five person of retirement age today. In the present study, we developed a machine learning based approach to understand and describe the population patterns in different geographic areas. 8.1.1 Swiss census data Input data come from the national population census, provided by the Swiss Federal Statistical Office, and including information on the socio-economic status of the population and surveyed land use and land cover. This information has been aggregated to the municipality level for the purpose of the present investigation. Census data 2020 have been considered to analyse the current pattern, while the former census surveys can be considered to assess transitions and evaluate population dynamics. 8.2 Self Organizing Map We use SOM, an unsupervised competitive learning neural network allowing representing a high-dimensional feature space (defined by the multivariate input dataset), as two-dimensional discretized pattern (the SOM grid of neurons). In SOM the proximity of the units in the output grid reflects the similarity of the corresponding input observations, which guarantees the preservation of the topological structure of the input data. Compared to other approaches, SOM is very efficient for data visualization. Indeed it provides additional outputs such as the heatmaps, representing the distribution of each input feature across the SOM grid, extremely useful to visually explore the relationship between the input variables. 8.3 Computing lab: SOM 8.3.1 Import the Swiss census dataset Fist, you have to import the Swiss census dataset for the year 2020, referred to the municipality administrative units. As you can see from the description of the selected variables , some of them can be discarded from the analysis. So, in the following step, we extract a subset of the most meaningful variables for the purpose of the present study. knitr::include_graphics(c(&quot;images/Variables1.jpg&quot;, &quot;images/Variables2.jpg&quot;)) Figure 8.1: Variables description Figure 8.2: Variables description 8.3.1.1 Inspect the data Plotting the histogram of the variables distribution allows to detect outliers, see if they have comparable range of values, and evaluate if a data transformation is necessary. # Plot variables&#39; frequency distribution for(i in 3:ncol(subset2020)) { hist((subset2020[ , i]), main=colnames(subset2020[i])) } 8.3.1.2 Data transformation Data transformation seeks the make the variables range comparable. This process ensure the data entries to look similar across all fields and records, making information easier to find, group and analyze. With Min-Max Scaling, the data values are scaled in a range between 0 and 1. As a consequence, the effect of outliers suppresses to a certain extent. Moreover, it helps us to have a smaller value of the standard deviation of the data scaled. # Custom function to implement min max scaling minMax &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) } # Apply max-min to the data dfnorm2020&lt;- as.data.frame(lapply((subset2020[ , -c(1,2)]), minMax)) summary(dfnorm2020) # Plot the histograms of the transformed data for(i in 1:ncol(dfnorm2020)) { hist((dfnorm2020[ , i]), main=colnames(dfnorm2020[i])) } 8.3.2 Load the libraries To perform the analyses and visualize the results you have to load the following libraries: kohonen: Supervised and Unsupervised Self-Organizing Maps (SOM) aweSOM: offers a set of tools to explore and analyze datasets with SOM ggplot2: create Elegant Data Visualizations Using the Grammar of Graphics colorpatch: rendering methods (ggplot extensions) library(kohonen) library(aweSOM) library(ggplot2) library(colorpatch) (.packages()) 8.3.3 Run SOM The main idea with SOM is to map the input high-dimensional feature space in a lower-dimensional output space organized on a grid made up of regular units (i.e. the neurons). At the end of the process, each observation from the input space (\\(X_{k,i}\\)) will be associated (i.e., mapped) to a unit in the SOM grid. Depending on the size of the grid, one unit can include several input observations. The first step consists in defining the size and geometry of the SOM grid. In this case we define a rectangular grid of 18 by 13 units, allowing to allocate, on average, about 15 input-items per units, and whose geometry reproduces the shape of the study area. (NB: several parameters and configurations of the SOM grid can be implemented and compared, seeking to minimize the Quantization Error, QE) Finally, you can run the SOM model. Note that rlen indicates the number of times the complete data set will be presented to the network, while for the other parameters we will keep the default values. The overall computation include the following steps: 1) The data frame has to be transformed to a matrix mxM2020&lt;-as.matrix(dfnorm2020) # max-min 2) Create the SOM-grid Before running SOM, you have to create the grid of output units. # Gird size 18x13 units som_grid &lt;- somgrid(xdim = 18, ydim=13) 3) Run-SOM The general R function is used for creating simulations of random objects that can be reproduced. # Use max-min data transformation set.seed(123) # Run SOM SOM2020M&lt;- som(mxM2020, grid=som_grid, rlen=1000) print(SOM2020M) ## SOM of size 18x13 with a rectangular topology. ## Training data included. 4) SOM-model evaluation Finally, you can optimize the size of the grid by inspecting several quality measure and changing the parameters accordingly. In particular we will explore the following: Changes: shows the mean distance to the closest codebook vector during the training. Quantization error: average squared distance between the data points and the map’s codebook to which they are mapped. Lower is better. Percentage of explained variance: similar to other clustering methods, the share of total variance that is explained by the clustering (equal to 1 minus the ratio of quantization error to total variance). Higher is better. # Evaluate rlen plot(SOM2020M, type=&quot;changes&quot;) # Evaluate the results QEM&lt;-somQuality(SOM2020M, dfnorm2020) ## Quality measures: QEM$err.quant # Quantization error ## [1] 0.04534672 QEM$err.varratio # % explained variance ## [1] 82.06 8.3.4 SOM’s main outputs The main graphical outputs of SOM are the node counts, the neighborhood distances, and the heatmaps. Node counts map informs about the number of input vectors falling inside each output unit. Neighbourhood distance plot shows the distance between each unit and its neighborhoods. Heatmaps show the distribution of each input variable, associated to each input vectors, across the SOM grid. # Create a color palette coolBlueHotRed &lt;- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]} par(mfrow = c(1, 2)) # Plot node counts map plot(SOM2020M, type=&quot;count&quot;, main=&quot;Node Counts&quot;, palette.name=coolBlueHotRed) # Plot SOM neighbourhood distance plot(SOM2020M, type=&quot;dist.neighbours&quot;, main = &quot;SOM neighbour distances&quot;) Visualized side by side, heatmaps provide a useful tool to explore the correlation between the input variables . # Plot heatmaps for selected variables for (i in 1:18) { plot(SOM2020M, type = &quot;property&quot;, property = getCodes(SOM2020M)[,i], main=colnames(getCodes(SOM2020M))[i], palette.name=coolBlueHotRed) } knitr::include_graphics(&quot;images/Heatmaps.jpg&quot;) Figure 8.3: Heatmaps 8.4 Hierarchical clustering Hierarchical clustering can finally be performed to isolate groups of input vectors characterized by a similar distribution of the input variables. This second step takes the SOM-units (i.e., the CodeBook vectors) as input and group them into the desired number of clusters (we choose 7 in this case). This allows to characterize main typologies of geo-demographic groups. Apply hierarchical clustering: CB2020M &lt;- getCodes(SOM2020M) # Extract codebook vectors cls2020M &lt;- cutree(hclust(dist(CB2020M)), 7) Display the SOM-grid with different colors for each cluster: # Colour palette definition, will use later too. # If you change the number of hierarchical clustering, remember change the number of color. map_palette &lt;- c(&quot;steelblue3&quot;,&quot;darkgoldenrod4&quot;,&quot;darkolivegreen&quot;, &quot;springgreen3&quot;, &quot;darkorange&quot;, &quot;firebrick&quot;, &quot;darkolivegreen1&quot; ) plot(SOM2020M, type=&quot;mapping&quot;, pchs=&quot;&quot;, bgcol = map_palette[cls2020M], main = &quot;Clusters&quot;) Assign the cluster number (based on hierarchical clustering) to each unit: clasgn &lt;- cls2020M[SOM2020M$unit.classif] dfnorm2020$hc&lt;-clasgn clsnorm&lt;-cbind(dfnorm2020, subset2020$BFS_nummer) # Export the table with clusters write.table(clsnorm, file=&quot;hcM_7cls.csv&quot;, sep=&quot;,&quot;) Map the resulting clusters to the geographical space You can import the final table with the values of the population census variable in a GIS and join the values to the administrative limits at municipality levels in Switzerland. You need only two columns: the code identifying each municipality (“BFS_nummer”) and the cluster number (hc). Open the table including the resulting clusters with a dedicated program (like Excel) to reorganize the header, as a new column for unique identifier has been created automatically (you can name it “ID”), and headers has to be shifted of one place on the right. If you want to try the primary and automatic process in Rstudio, you can conduct the below codes and see the map. library(st) library(sf) library(ggplot2) library(ggspatial) library(dplyr) # Load the shapefile data CH_outline &lt;- st_read(&quot;data/SOM/Municiplities.shp&quot;) # Quick check the shapefile data ggplot() + geom_sf(data = CH_outline, size = 1.5, color = &quot;black&quot;, fill = &quot;cyan1&quot;) + ggtitle(&quot;Swiss municiplities&quot;) + coord_sf() # Rename the column colnames(clsnorm)[20] &lt;- &quot;BFS_NUMMER&quot; # Merge the dataframe and shapefile together by the column with the same name z = merge(CH_outline, clsnorm, by = &quot;BFS_NUMMER&quot;) # Rename the column with clustering results z$cluster &lt;- as.numeric(z$hc) # Mapping the clusters via ggplot like QGIS/ArcGIS Pro/Arcmap, similar package &#39;tmap&#39; CH_cluster &lt;- ggplot(data = z) + # Original data geom_sf(aes(fill = as.factor(cluster))) + # Mapping the clusters annotation_scale(location = &quot;bl&quot;, width_hint = 0.3, pad_x = unit(0.2, &quot;cm&quot;), pad_y = unit(0.1, &quot;cm&quot;)) + # Mapping scales, which are calculated by project coordinates annotation_north_arrow(location = &quot;tr&quot;, which_north = &quot;true&quot;, pad_x = unit(0.0, &quot;cm&quot;), pad_y = unit(0.1, &quot;cm&quot;), style = north_arrow_fancy_orienteering) + # Mapping north arrow scale_colour_manual(values = map_palette) + # Set up the our color palette theme_minimal() + # Background color, if use, it means without color labs(title = &quot;Spatial pattern distribution of Swiss poputaion census&quot;) + # Insert title of the map scale_fill_discrete(name = &quot;Clusters&quot;, labels = c(&quot;1: Unproductive&quot;, &quot;2: Forest/Peri-urban&quot;, &quot;3: Peri-urban/Rural&quot;, &quot;4: Rural&quot;,&quot;5: Suburban&quot;,&quot;6: Urban centre&quot;,&quot;7: Peri-urban fringe&quot;)) # Legend labels guides(fill=guide_legend(title=&quot;Clusters&quot;)) # Legend name # Save the map as jpg ggsave(&quot;CH_cluster.jpg&quot;) CH_cluster 8.5 Clusters characterization To interpret the final main clusters in therms of their geo-demographic characteristics, you can evaluate the distribution of each variable within the clusters by using box plots (also known as whisker plot). Box-plot is a standardized way to display a dataset based on the five-number summary statistics: the minimum, the maximum, the sample median, and the first and third quartiles (i.e., the median of the lower half (25%) and the median of the upper half (75%) of the dataset). # Creates an empty list object that will be filled by the loop cls20M &lt;- list() # Split the single clusters for(i in 1:7) { cls20M[[i]]&lt;-subset(clsnorm, clsnorm$hc==i) } clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(1:18)) # Box-plots for the single clusters for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), mar=c(8,3,3,1), cex.axis=0.5, las=2)} To better investigate the values assumed by each class of variables withing the different clusters, you can group them by category. # Box plot by categories: &quot;Physical space&quot; clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(1:5)) par(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7) for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), las=2)} Figure 8.4: Physical space # Box plot by categories: &quot;Demographics&quot; clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(6:13)) par(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7) for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), las=2)} Figure 8.5: Demographics # Box plot by categories: &quot;Socio-economics&quot; clsvar20M &lt;- lapply(cls20M, &quot;[&quot;, c(14:18)) par(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7) for (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(&quot;Cluster&quot;, i), las=2)} Figure 8.6: Socio-economics 8.6 Conclusions Results of the present study reveal the main patterns of the population in Switzerland based on the surveyed land-use, socio-economic, and demographic indicators. To characterize the final main clusters, the distribution of each variable within them have been assessed by using box plots. We could thus identify main clusters including the most developed and active cities with higher income, the peri-urban areas mostly devoted to the agricultural activity, or the areas with higher levels of migration. SOM heatmaps allow to display the pattern distribution of each input variable over the SOM-grid and how values change in space. Visualized side by side, heatmaps show a picture of the different areas and their characteristics. In this way it is possible to explore the level of complementarity that links one or more variables among them. In conclusion, in the present study we proposed a performant data-driven approach based on unsupervised learning allowing to extract useful information from a huge volume of multivariate population census data. This approach led to represent and interpret the main patterns characterizing the dynamic of population in Switzerland in the recent period. 8.7 Further analyses To ensure that everything is perfectly clear, we propose you to answer the following questions and to discuss your answers with the other participants to the course or directly with the teacher. Change the size of the gridmap and check if you get better results for SOM. N.B. Better results are achieved when the quantisation error decreases, the explained variance increases, and there are no empty observations revealed by the Node Counts map. Focusing on Cluster 6: which variable characterize it the most? Based on these variables, which class of land use can be associated to this cluster? And in the case of Cluster 4? Describe the distribution of the clusters in the geographical space. In more details, describe to which kind of land use the different clusters can be referred and specify why. From the visual inspection of the heatmaps, describe the correlation you can observe between the following variables: a) “p_transport” and “p_infrastructure”; b) “p_agriculture” and “p_improductible”. 8.8 References References Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43(1), 59–69. https://doi.org/10.1007/BF00337288 Marj Tonini, Axelle Bersier, Jingyan Yu, &amp; Francois Bavaud. (2023, September). An unsupervised learning approach to explore geodemographic clusters in switzerland. ECTQG 2023 Proceedings. 23rd european colloquium on theoretical and quantitative geography. https://ucpages.uc.pt/site/assets/files/1249198/ectqg_2023_proceedings_final.pdf Spielman, S. E., &amp; Thill, J.-C. (2008). Social area analysis, data mining, and GIS. Computers, Environment and Urban Systems, 32(2), 110–122. https://doi.org/10.1016/j.compenvurbsys.2007.11.004 "],["dbscan-for-3d-fetures-detection-in-point-clouds.html", "Chapter 9 DBSCAN for 3D fetures detection in point clouds 9.1 The overall methodology 9.2 Computing lab: DBSCAN 9.3 Conclusions and further analyses 9.4 Further reading on this topic", " Chapter 9 DBSCAN for 3D fetures detection in point clouds Terrestrial laser scanning (TLS) has been one of the most successful methods for 3D data collection in the last few years. Sequential acquisition can be used to detect and quantify surface changes. The three main challenges arising from TLS data collection are: 1) the large number of points acquired is computationally intense and datasets have to be filtered depending on the aim of the investigation; 2) data collection methods suffer from perspective effects, which can lead to either zones of occlusion (shadow effect) or spatially-variable point densities; 3) 3D point clouds are normally interpolated to digital elevation models either as regular grids or triangulated irregular networks. Thus, for change detection proposes (i.e. determination of topographic change, including erosion and deposition), it is appropriate to develop methods based upon the direct analysis of the point clouds using semi-automatic approaches allowing to detect and extract individual features. In this practical computing lab we introduce a semi-automated method developed for isolating and identifying features of topographic change (i.e., apparent changes caused by surface displacements and indicating erosion or deposition) directly from point cloud data using a Density-Based Spatial Clustering of Applications with Noise (DBSCAN). This methodology was developed in Micheletti et al. (2017) for a very active rock glacier front located in the Swiss Alps: the Tsarmine rock glacier. Figure 3.1: The Tsarmine rock glacier, Hérens Valley, in the Western Swiss Alps. Source: Micheletti et al, 2016 9.1 The overall methodology Stepwise analysis: Point clouds were generated using a TLS on a number of dates. Point clouds were co-registered using stable zones within the surveyed area. Using a threshold value, only the points where there may have been some topographical change were selected. The final dataset was treated with DBSCAN, aiming at grouping cluster-points into single features and filtering out noise points, found in low-density regions. The present lab deals with steps 2 and 3: 3D features detection using DBSCAN. The detected features are finally labeled as clusters and visualized in a 3D map. Material loss or gain can eventually be analysed in a GIS environment according to the elevation assignment of the change and the volume of change computed for each cluster by a triangulation process (not performed in this lab). 9.1.1 DBSCAN: 3-D density based clustering algorithm DBSCAN allows identification of spatial clusters of arbitrary shape on the base of the local density of points. Points that are close together are grouped into the same cluster, while isolated points are labelled as noise. Two parameters are required to perform this classification: the minimum number of points necessary to form a cluster (\\(MinPts\\)), and the neighborhood size epsilon (\\(eps\\)). The algorithm explores each point in the dataset, counting the number of the neighboring points falling within a circle (for the 2D model) or a sphere (for the 3D model) of radius equal to \\(eps\\) (a). If this number is equal to or greater than \\(MinPts\\), points are labelled as belonging to the same cluster (b). If this number is lower than \\(MinPts\\), points are classified as noise. The central point of each cluster is called “core-point”. Since some points can be density-reachable by more than one core-point, they can belong to more than one cluster. In this case clusters are blended together to form a unique feature of arbitrary shape (c). Figure 9.1: Parameters in DBSCAN to form a cluster 9.1.2 Field campaign An ultra-long range LiDAR RIEGL VZ-6000 scanner was employed to acquire sequential 3D datasets of the rock glacier front. TLS scans were performed on different dates over two consecutive summers: a first survey was carried out on the 23th of September 2014, and the last one on the 22th of September 2015. DBSCAN requires as input a 3-dimensional dataset, which in our case consists on the points clouds (X,Y,Z) plus the displacement distance. For the two co-registered datasets (2014 and 2015), we set the first as the target and the more recent as the reference. For each point in the target cloud the corresponding nearest point in the reference cloud was identified and the distance between them evaluated using the software Cloud Compare. Co-registration errors, estimated as noise and not real material loss/gain signals, has to be removed from the analysis. Here, we noted from field observations that the size of displaced boulders is typically &gt; 0.30 m and we use this as a change criteria to remove the noise. 9.2 Computing lab: DBSCAN 9.2.1 Load the libraries Fist you have to load the following libraries: dbscan: a fast implementation of several density-based algorithms of the DBSCAN family. rgl: provides medium to high level functions for 3D interactive graphics plot3Drgl: plot 3D graphs in rgl window. classInt: selected methods to choose class intervals for mapping puposes. RColorBrewer: Provides color schemes for mapping. library(&quot;dbscan&quot;) library(&quot;rgl&quot;) library(&quot;plot3Drgl&quot;) library(&quot;classInt&quot;) library(&quot;RColorBrewer&quot;) (.packages()) ## [1] &quot;RColorBrewer&quot; &quot;classInt&quot; &quot;plot3Drgl&quot; &quot;plot3D&quot; &quot;rgl&quot; ## [6] &quot;dbscan&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; ## [11] &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; 9.2.2 Import and visualize the point clouds dataset We provide the dataset corresponding to one-year displacement distance, masked over the the rock glacier front. The noise-points are removed using a threshold of 30 cm. Finally we will plot the 3D-points cloud filtered dataset using the displacement distance as attribute to display the map. # Import point cloud dataset (ptc) # 1 year displacement TLS campaigns; data masked over the active front only. ptc &lt;- read.table(&quot;data/DBSCAN/TsarmineRG_230914_frontonly_XYZ_dist_ref_220915.txt&quot;, header=FALSE, sep=&quot;&quot;) # Add names to columns: X,Y, Z coordinates and the displacement distance &quot;d&quot;. colnames(ptc)&lt;-c(&quot;Y&quot;,&quot;X&quot;,&quot;Z&quot;,&quot;d&quot;) # rename columns # Inspect the dataset: str(ptc) summary(ptc$d) hist(log10(ptc$d)) # Create a subset: removing noise-points (d&gt;30cm). ptc30 &lt;- subset(ptc, d&gt;=0.3) # Inspect the subset: str(ptc30) summary(ptc30$d) # Plot-3D: display 3D plot (X,Y,Z) with class intervals based on the distance d. # Create a class interval (int) based on natural breaks: int &lt;- classIntervals(ptc30$d, n=5, style=&quot;fisher&quot;) cut.vals&lt;-int$brks d.cut &lt;- cut(ptc30$d,cut.vals) cut.levels &lt;- levels(d.cut) cut.band &lt;- match(d.cut,cut.levels) cls &lt;- rev(brewer.pal(length(cut.levels), &quot;RdYlGn&quot;)) # Display the 3D-plot: scatter3D(ptc30$X, ptc30$Y, ptc30$Z, colvar =ptc30$d, breaks=cut.vals, cex=0.5) plotrgl() #animated 3D-plot 9.2.3 Run DBSCAN The two parameters \\(eps\\) and \\(MinPts\\) greatly affect the output cluster detection leading to the identification of a large number of small clusters (for small values) or a small number of large features of arbitrary shape (for large values). Once fixed the \\(MinPts\\) parameter, a suitable value for the \\(eps\\) neighborhood size can be estimated using k-nearest neighbors (k-NN) distance graph, imposing k as equal to \\(MinPts\\) and plotting the distance to the nearest neighbors. The optimal \\(eps\\)-value should coincide with the stronger curvature of the curve. In the following, the minimum number of points allowing to form a cluster is fixed first, and then the plot of the k-NN distance is used to find a suitable value for the \\(eps\\) neighborhood size. # Plot the k-NN distance graph (with k=10) kNNdistplot(ptc30[,-4], k = 10) Once the best \\(MinPts\\) and \\(eps\\) parameter have been selected, the DBSCAN function can be run. # Eps=1m (~3xsigma) ; MinPts=10 cl30&lt;-dbscan(ptc30, 1, minPts=10) # Inspect the results: str(cl30) summary(cl30$cluster) sum(cl30$cluster==0) # Plot only the detected clusters. # Join the data: cl30&lt;-cbind(ptc30, cluster=cl30$cluster) # Save as data frame and remove the noise (label==0) cl30_df&lt;-as.data.frame(subset(cl30, cl30$cluster&gt;=1)) # Simple plot: plot(cl30_df$X, cl30_df$Y, col=cl30_df$cluster, pch=20) # Animated 3D-plot: plot3d(cl30_df, col=cl30_df$cluster, pch=20) Finally you can export the result as a *.txt file and import it in a GIS system for further analyses, such as to determine the volumes of loss or gain of material. write.table(cl30_df, file=&quot;cls30_1_10.txt&quot;, sep=&quot;\\t&quot;) 9.3 Conclusions and further analyses The proposed method allowed to detection of features of changes on a rock glacier front located in the Swiss Alps. Single cluster features of erosion and deposition/front movements were extracted directly from point clouds, detected by DBSCAN without the necessity of interpolate the 3-D original data. To ensure that everything is perfectly clear, we propose you to do this lab again, by changing the values of MinPts and eps and discuss the compare the resulting extracted features. 9.4 Further reading on this topic References Micheletti, N., Tonini, M., &amp; Lane, S. N. (2017). Geomorphological activity at a rock glacier front detected with a 3D density-based clustering algorithm. Geomorphology, 278, 287–297. https://doi.org/10.1016/j.geomorph.2016.11.016 "],["references-1.html", "References", " References Breiman, L. (2001). Random forest. Machine Learning, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324 Brunsdon, C. (1995). Estimating probability surfaces for geographical point data: An adaptive kernel algorithm. Computers &amp; Geosciences, 21(7), 877–894. https://doi.org/10.1016/0098-3004(95)00020-9 Brunsdon, C. (2019, July). RPubs - GWSS - (7th channel network conference). https://rpubs.com/chrisbrunsdon/503649 Brunsdon, C., Fotheringham, A. S., &amp; Charlton, M. (2002). Geographically weighted summary statistics - a framework for localised exploratory data analysis. Computers, Environment and Urban Systems, 26(6), 501–524. https://doi.org/10.1016/s0198-9715(01)00009-6 Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43(1), 59–69. https://doi.org/10.1007/BF00337288 Lu, B., Harris, P., Charlton, M., &amp; Brunsdon, C. (2014). The GWmodel r package: Further topics for exploring spatial heterogeneity using geographically weighted models. Geo-Spatial Information Science, 17(2), 85–101. https://doi.org/10.1080/10095020.2014.917453 Marj Tonini, Axelle Bersier, Jingyan Yu, &amp; Francois Bavaud. (2023, September). An unsupervised learning approach to explore geodemographic clusters in switzerland. ECTQG 2023 Proceedings. 23rd european colloquium on theoretical and quantitative geography. https://ucpages.uc.pt/site/assets/files/1249198/ectqg_2023_proceedings_final.pdf Micheletti, N., Tonini, M., &amp; Lane, S. N. (2017). Geomorphological activity at a rock glacier front detected with a 3D density-based clustering algorithm. Geomorphology, 278, 287–297. https://doi.org/10.1016/j.geomorph.2016.11.016 Nakaya, T., &amp; Yano, K. (2010). Visualising crime clusters in a space-time cube: An exploratory data-analysis approach using space-time kernel density estimation and scan statistics. Transactions in GIS, 14(3), 223–239. https://doi.org/10.1111/j.1467-9671.2010.01194.x Naur, Peter. (1974). Concise survey of computer methods. New York : Petrocelli Books. https://archive.org/details/concisesurveyofc0000naur Spielman, S. E., &amp; Thill, J.-C. (2008). Social area analysis, data mining, and GIS. Computers, Environment and Urban Systems, 32(2), 110–122. https://doi.org/10.1016/j.compenvurbsys.2007.11.004 Tonini, M., D’Andrea, M., Biondi, G., Degli Esposti, S., Trucchia, A., &amp; Fiorucci, P. (2020). A machine learning-based approach for wildfire susceptibility mapping. The case study of the liguria region in italy. Geosciences, 10(3), 105. https://doi.org/10.3390/geosciences10030105 Tonini, M., Pereira, M. G., Parente, J., &amp; Vega Orozco, C. (2017). Evolution of forest fires in portugal: From spatio-temporal point events to smoothed density maps. Natural Hazards, 85(3), 1489–1510. https://doi.org/10.1007/s11069-016-2637-x Trucchia, A., Izadgoshasb, H., Isnardi, S., Fiorucci, P., &amp; Tonini, M. (2022). Machine-learning applications in geosciences: Comparison of different algorithms and vegetation classes’ importance ranking in wildfire susceptibility. Geosciences, 12(11), 424. https://doi.org/10.3390/geosciences12110424 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
